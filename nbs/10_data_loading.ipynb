{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "> Functions and classes for loading and parsing datasets for LLaVA-style training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Union, Tuple\n",
    "from dataclasses import dataclass\n",
    "import PIL.Image\n",
    "from functools import partial\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.block import DataBlock, TransformBlock, CategoryBlock # Added CategoryBlock just in case\n",
    "from fastai.data.transforms import parent_label, GrandparentSplitter, RandomSplitter, IntToFloatTensor\n",
    "\n",
    "from Adaptive_Patching_VIT_fastai.utils import load_config\n",
    "# Import necessary items from preprocessing notebook\n",
    "from Adaptive_Patching_VIT_fastai.data.preprocessing import (\n",
    "    tokenizer, \n",
    "    LLaVATextTokenizer, \n",
    "    clip_normalize, \n",
    "    format_plain_template, # Needed if not using LLaVATextTokenizer directly\n",
    "    LLaVABatchTransform # Import the custom batch transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Data Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need functions to parse the JSONL files commonly used in LLaVA datasets. Each line typically contains:\n",
    "- `id`: A unique identifier for the sample.\n",
    "- `image`: The filename of the image (often relative to an `image_folder`).\n",
    "- `conversations`: A list of dictionaries, where each dictionary has `from` ('human' or 'gpt') and `value` (the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class LLaVASample:\n",
    "    \"\"\"Represents a single sample from a LLaVA-style dataset.\n",
    "\n",
    "    Attributes:\n",
    "        sample_id: Unique identifier for the sample.\n",
    "        image_path: Absolute path to the image file.\n",
    "        conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
    "        data_source: Optional field indicating the source dataset.\n",
    "    \"\"\"\n",
    "    sample_id: str\n",
    "    image_path: Path\n",
    "    conversations: List[Dict[str, str]]\n",
    "    data_source: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "#| export\n",
       "@dataclass\n",
       "class LLaVASample:\n",
       "    \"\"\"Represents a single sample from a LLaVA-style dataset.\\n\\n    Attributes:\\n        sample_id: Unique identifier for the sample.\\n        image_path: Absolute path to the image file.\\n        conversations: List of conversation turns (dictionaries with 'from' and 'value').\\n        data_source: Optional field indicating the source dataset.\\n    \"\"\"\n",
       "    sample_id: str\n",
       "    image_path: Path\n",
       "    conversations: List[Dict[str, str]]\n",
       "    data_source: str | None = None\n",
       "```"
      ],
      "text/plain": [
       "<showdoc.show_doc at 0x7f1e42e4bb80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLaVASample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_llava_jsonl(jsonl_path: Union[str, Path], image_folder: Union[str, Path]) -> List[LLaVASample]:\n",
    "    \"\"\"Parses a LLaVA-style JSONL file and resolves image paths.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path: Path to the JSONL file.\n",
    "        image_folder: Path to the directory containing the images referenced in the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the JSONL file does not exist.\n",
    "        json.JSONDecodeError: If a line in the file is not valid JSON.\n",
    "    \"\"\"\n",
    "    jsonl_path = Path(jsonl_path)\n",
    "    image_folder = Path(image_folder)\n",
    "\n",
    "    if not jsonl_path.is_file():\n",
    "        raise FileNotFoundError(f\"JSONL file not found: {jsonl_path}\")\n",
    "\n",
    "    samples = []\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise json.JSONDecodeError(f\"Error decoding JSON on line {i+1} in {jsonl_path}: {e.msg}\", e.doc, e.pos)\n",
    "\n",
    "            # Check for required keys\n",
    "            if not all(k in data for k in ['id', 'image', 'conversations']):\n",
    "                print(f\"Warning: Skipping line {i+1} due to missing keys ('id', 'image', or 'conversations') in {jsonl_path}.\\n\",\n",
    "                      f\"Data: {data}\")\n",
    "                continue\n",
    "\n",
    "            sample_id = data['id']\n",
    "            # Construct the full image path. Assumes 'image' key holds a relative path or filename.\n",
    "            # Handle potential nested structure like {'bytes': ..., 'path': ...} if found in parquet conversions\n",
    "            image_ref = data['image']\n",
    "            if isinstance(image_ref, dict) and 'path' in image_ref:\n",
    "                # Handle cases where image info is nested (e.g., from parquet processing)\n",
    "                image_filename = image_ref['path']\n",
    "            elif isinstance(image_ref, str):\n",
    "                image_filename = image_ref\n",
    "            else:\n",
    "                 print(f\"Warning: Skipping line {i+1} due to unexpected image field format in {jsonl_path}.\\n\",\n",
    "                       f\"Expected string or dict with 'path', got: {type(image_ref)}\")\n",
    "                 continue\n",
    "\n",
    "            # Resolve the image path: assume image_filename is relative to image_folder\n",
    "            # Path.name is used to handle cases where an absolute path might be mistakenly stored\n",
    "            # but we want it relative to the designated image_folder.\n",
    "            image_path = image_folder / Path(image_filename).name\n",
    "\n",
    "            conversations = data['conversations']\n",
    "            data_source = data.get('data_source') # Optional field\n",
    "\n",
    "            # Basic validation for conversations format\n",
    "            if not isinstance(conversations, list) or not all(isinstance(turn, dict) and 'from' in turn and 'value' in turn for turn in conversations):\n",
    "                 print(f\"Warning: Skipping line {i+1} due to invalid 'conversations' format in {jsonl_path}.\")\n",
    "                 continue\n",
    "\n",
    "            # Check if image file actually exists (optional, can slow down parsing)\n",
    "            # if not image_path.is_file():\n",
    "            #     print(f\"Warning: Image file not found for sample {sample_id} at {image_path}, skipping.\")\n",
    "            #     continue\n",
    "\n",
    "            samples.append(LLaVASample(\n",
    "                sample_id=str(sample_id),\n",
    "                image_path=image_path,\n",
    "                conversations=conversations,\n",
    "                data_source=data_source\n",
    "            ))\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "#| export\n",
       "def parse_llava_jsonl(jsonl_path: str | Path, image_folder: str | Path) -> List[LLaVASample]:\n",
       "    \"\"\"Parses a LLaVA-style JSONL file and resolves image paths.\\n\\n    Args:\\n        jsonl_path: Path to the JSONL file.\\n        image_folder: Path to the directory containing the images referenced in the JSONL file.\\n\\n    Returns:\\n        A list of LLaVASample objects.\\n\\n    Raises:\\n        FileNotFoundError: If the JSONL file does not exist.\\n        json.JSONDecodeError: If a line in the file is not valid JSON.\\n    \"\"\"\n",
       "    jsonl_path = Path(jsonl_path)\n",
       "    image_folder = Path(image_folder)\n",
       "\n",
       "    if not jsonl_path.is_file():\n",
       "        raise FileNotFoundError(f\"JSONL file not found: {jsonl_path}\")\n",
       "\n",
       "    samples = []\n",
       "    with open(jsonl_path, 'r') as f:\n",
       "        for i, line in enumerate(f):\n",
       "            try:\n",
       "                data = json.loads(line)\n",
       "            except json.JSONDecodeError as e:\n",
       "                raise json.JSONDecodeError(f\"Error decoding JSON on line {i+1} in {jsonl_path}: {e.msg}\", e.doc, e.pos)\n",
       "\n",
       "            # Check for required keys\n",
       "            if not all(k in data for k in ['id', 'image', 'conversations']):\n",
       "                print(f\"Warning: Skipping line {i+1} due to missing keys ('id', 'image', or 'conversations') in {jsonl_path}.\\n\",\n",
       "                      f\"Data: {data}\")\n",
       "                continue\n",
       "\n",
       "            sample_id = data['id']\n",
       "            # Construct the full image path. Assumes 'image' key holds a relative path or filename.\n",
       "            # Handle potential nested structure like {'bytes': ..., 'path': ...} if found in parquet conversions\n",
       "            image_ref = data['image']\n",
       "            if isinstance(image_ref, dict) and 'path' in image_ref:\n",
       "                # Handle cases where image info is nested (e.g., from parquet processing)\n",
       "                image_filename = image_ref['path']\n",
       "            elif isinstance(image_ref, str):\n",
       "                image_filename = image_ref\n",
       "            else:\n",
       "                 print(f\"Warning: Skipping line {i+1} due to unexpected image field format in {jsonl_path}.\\n\",\n",
       "                       f\"Expected string or dict with 'path', got: {type(image_ref)}\")\n",
       "                 continue\n",
       "            \n",
       "            # Resolve the image path: assume image_filename is relative to image_folder\n",
       "            # Path.name is used to handle cases where an absolute path might be mistakenly stored\n",
       "            # but we want it relative to the designated image_folder.\n",
       "            image_path = image_folder / Path(image_filename).name \n",
       "\n",
       "            conversations = data['conversations']\n",
       "            data_source = data.get('data_source') # Optional field\n",
       "\n",
       "            # Basic validation for conversations format\n",
       "            if not isinstance(conversations, list) or not all(isinstance(turn, dict) and 'from' in turn and 'value' in turn for turn in conversations):\n",
       "                 print(f\"Warning: Skipping line {i+1} due to invalid 'conversations' format in {jsonl_path}.\")\n",
       "                 continue\n",
       "            \n",
       "            # Check if image file actually exists (optional, can slow down parsing)\n",
       "            # if not image_path.is_file():\n",
       "            #     print(f\"Warning: Image file not found for sample {sample_id} at {image_path}, skipping.\")\n",
       "            #     continue\n",
       "\n",
       "            samples.append(LLaVASample(\n",
       "                sample_id=str(sample_id),\n",
       "                image_path=image_path,\n",
       "                conversations=conversations,\n",
       "                data_source=data_source\n",
       "            ))\n",
       "\n",
       "    return samples\n",
       "```"
      ],
      "text/plain": [
       "<showdoc.show_doc at 0x7f1e42e4b280>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(parse_llava_jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy directory: dummy_data/images\n",
      "Note: Could not create dummy image files (PIL might not be fully installed or usable): name 'Image' is not defined\n",
      "Created dummy jsonl file: dummy_data/dummy_llava_data.jsonl\n",
      "Warning: Skipping line 3 due to missing keys ('id', 'image', or 'conversations') in dummy_data/dummy_llava_data.jsonl.\n",
      "Data: {'id': 'sample3_missing_keys', 'conversations': []}\n",
      "Warning: Skipping line 4 due to invalid 'conversations' format in dummy_data/dummy_llava_data.jsonl.\n",
      "Successfully parsed 3 samples:\n",
      "LLaVASample(sample_id='sample1', image_path=PosixPath('dummy_data/images/img1.jpg'), conversations=[{'from': 'human', 'value': '<image>\\nDescribe this.'}, {'from': 'gpt', 'value': 'It is red.'}], data_source=None)\n",
      "LLaVASample(sample_id='sample2', image_path=PosixPath('dummy_data/images/img2.png'), conversations=[{'from': 'human', 'value': '<image>\\nWhat color?'}, {'from': 'gpt', 'value': 'Green.'}], data_source=None)\n",
      "LLaVASample(sample_id='sample5_missing_img_file', image_path=PosixPath('dummy_data/images/nonexistent.jpg'), conversations=[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': '...'}], data_source=None)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Create dummy data for testing\n",
    "dummy_data_dir = Path('./dummy_data')\n",
    "dummy_img_dir = dummy_data_dir / 'images'\n",
    "dummy_jsonl_path = dummy_data_dir / 'dummy_llava_data.jsonl'\n",
    "\n",
    "dummy_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created dummy directory: {dummy_img_dir}\")\n",
    "\n",
    "# Create dummy image files (attempt, may fail if PIL issues persist)\n",
    "try:\n",
    "    PIL.Image.new('RGB', (60, 30), color = 'red').save(dummy_img_dir / 'img1.jpg')\n",
    "    PIL.Image.new('RGB', (60, 30), color = 'green').save(dummy_img_dir / 'img2.png')\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not create dummy image files (PIL might not be fully installed or usable): {e}\")\n",
    "\n",
    "# Create dummy jsonl content\n",
    "dummy_jsonl_content = [\n",
    "    {\"id\": \"sample1\", \"image\": \"img1.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nDescribe this.\"}, {\"from\": \"gpt\", \"value\": \"It is red.\"}]}, \n",
    "    {\"id\": \"sample2\", \"image\": \"img2.png\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nWhat color?\"}, {\"from\": \"gpt\", \"value\": \"Green.\"}]},\n",
    "    {\"id\": \"sample3_missing_keys\", \"conversations\": []}, # Missing image/id\n",
    "    {\"id\": \"sample4_bad_conv\", \"image\": \"img1.jpg\", \"conversations\": \"not a list\"}, # Bad conversation format\n",
    "    {\"id\": \"sample5_missing_img_file\", \"image\": \"nonexistent.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\"}, {\"from\": \"gpt\", \"value\": \"...\"}]},\n",
    "]\n",
    "\n",
    "with open(dummy_jsonl_path, 'w') as f:\n",
    "    for item in dummy_jsonl_content:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "print(f\"Created dummy jsonl file: {dummy_jsonl_path}\")\n",
    "\n",
    "# Test parsing\n",
    "try:\n",
    "    parsed_samples = parse_llava_jsonl(dummy_jsonl_path, dummy_img_dir)\n",
    "    print(f\"Successfully parsed {len(parsed_samples)} samples:\")\n",
    "    for sample in parsed_samples:\n",
    "        print(sample)\n",
    "\n",
    "    # Basic checks (adjust expected length based on warnings/skips)\n",
    "    # Check based on the print output, 3 samples are expected now (1, 2, 5)\n",
    "    assert len(parsed_samples) == 3 \n",
    "    assert parsed_samples[0].sample_id == 'sample1'\n",
    "    # Resolve paths for comparison\n",
    "    assert parsed_samples[0].image_path.resolve() == (dummy_img_dir / 'img1.jpg').resolve()\n",
    "    assert parsed_samples[1].sample_id == 'sample2'\n",
    "    assert parsed_samples[1].image_path.resolve() == (dummy_img_dir / 'img2.png').resolve()\n",
    "    assert parsed_samples[0].conversations[0]['from'] == 'human'\n",
    "    assert parsed_samples[2].sample_id == 'sample5_missing_img_file'\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON Parsing Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Clean up dummy data (optional)\n",
    "# import shutil\n",
    "# if dummy_data_dir.exists():\n",
    "#    shutil.rmtree(dummy_data_dir)\n",
    "#    print(f\"Cleaned up dummy data directory: {dummy_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Image Loading and Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This involves defining the `ImageBlock` and basic `item_tfms` for loading and resizing images. Normalization stats were defined in `11_data_preprocessing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define the ImageBlock using PILImage for loading images\n",
    "image_block = ImageBlock(cls=PILImage)\n",
    "\n",
    "# Define basic item transforms for image processing\n",
    "# 1. Resize images to 336x336, padding if necessary\n",
    "#    method='pad': Pads the image to the target size.\n",
    "#    pad_mode='const': Uses a constant value for padding.\n",
    "#    pad_value=0: Uses black for padding (common for vision models).\n",
    "# 2. Convert the image to a PyTorch tensor.\n",
    "# Note: Normalization will be applied in batch_tfms using clip_normalize.\n",
    "basic_image_item_tfms = [\n",
    "    Resize(336, method='pad', pad_mode=PadMode.Constant, pad_value=0),\n",
    "    ToTensor(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Define Custom Dataset/DataBlock (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the fastai `DataBlock` for Stage 1 projector pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_llava_items(config: dict, stage: int = 1) -> List[LLaVASample]:\n",
    "    \"\"\"Loads LLaVA samples for a specific stage based on config.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "        stage: The training stage (1 or 2).\n",
    "\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If stage is not 1 or 2.\n",
    "        KeyError: If required path configurations are missing.\n",
    "    \"\"\"\n",
    "    data_base_path = Path(config['paths']['data_base'])\n",
    "    if stage == 1:\n",
    "        jsonl_rel_path = config['paths']['stage1_data']\n",
    "        images_rel_path = config['paths']['stage1_images']\n",
    "    elif stage == 2:\n",
    "        jsonl_rel_path = config['paths']['stage2_data']\n",
    "        # Stage 2 images might be relative to base path or absolute paths might be in JSONL\n",
    "        images_rel_path = config['paths'].get('stage2_images', '.') # Assume relative to base if specified\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid stage specified: {stage}. Must be 1 or 2.\")\n",
    "\n",
    "    jsonl_path = data_base_path / jsonl_rel_path\n",
    "    image_folder = data_base_path / images_rel_path\n",
    "\n",
    "    print(f\"Loading Stage {stage} items from: {jsonl_path}\")\n",
    "    print(f\"Assuming images relative to: {image_folder}\")\n",
    "\n",
    "    # Check if paths exist\n",
    "    if not jsonl_path.exists():\n",
    "        raise FileNotFoundError(f\"Stage {stage} JSONL file not found: {jsonl_path}\")\n",
    "    if not image_folder.is_dir() and stage == 1: # Only strictly check image folder for stage 1\n",
    "        print(f\"Warning: Stage {stage} image folder not found or not a directory: {image_folder}\")\n",
    "        # Proceed cautiously, parsing might still work if JSONL contains absolute paths\n",
    "        # or if image existence check is disabled in parse_llava_jsonl\n",
    "\n",
    "    samples = parse_llava_jsonl(jsonl_path, image_folder)\n",
    "    print(f\"Found {len(samples)} samples for Stage {stage}.\")\n",
    "    return samples\n",
    "\n",
    "def get_image_path(sample: LLaVASample) -> Path:\n",
    "    \"\"\"Extracts the image path from a LLaVASample.\"\"\"\n",
    "    return sample.image_path\n",
    "\n",
    "def get_conversations(sample: LLaVASample) -> List[Dict[str, str]]:\n",
    "    \"\"\"Extracts the conversations from a LLaVASample.\"\"\"\n",
    "    return sample.conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 2\n",
      "LLaVADataBlockStage1 defined.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Define the DataBlock for Stage 1 (Projector Pre-training)\n",
    "if tokenizer:\n",
    "    # Instantiate the custom batch transform (needs tokenizer)\n",
    "    llava_batch_tfm = LLaVABatchTransform(tokenizer=tokenizer)\n",
    "    \n",
    "    LLaVADataBlockStage1 = DataBlock(\n",
    "        blocks=(ImageBlock(cls=PILImage), TransformBlock), # Input: Image, Target: Processed Conversations (token IDs)\n",
    "        get_items=partial(get_llava_items, stage=1), # Use partial to pass stage=1 to the item getter\n",
    "        get_x=get_image_path, # Function to get image path from sample\n",
    "        get_y=get_conversations, # Function to get conversations for text processing\n",
    "        splitter=RandomSplitter(valid_pct=0.01, seed=42), # Small validation set for monitoring\n",
    "        item_tfms=[\n",
    "            *basic_image_item_tfms, # Apply resize/ToTensor to images (x)\n",
    "            LLaVATextTokenizer(tokenizer, template_formatter=format_plain_template) # Apply template+tokenization to conversations (y)\n",
    "        ],\n",
    "        # batch_tfms are applied after items are collated into a batch\n",
    "        batch_tfms=[ \n",
    "            IntToFloatTensor(div_mask=torch.BoolTensor([True,False])), # Convert image tensor to float (but not text IDs)\n",
    "            clip_normalize,   # Apply normalization (to image tensor)\n",
    "            llava_batch_tfm   # Apply custom batch padding, masking, etc.\n",
    "        ]\n",
    "    )\n",
    "    print(\"LLaVADataBlockStage1 defined.\")\n",
    "else:\n",
    "    LLaVADataBlockStage1 = None\n",
    "    print(\"Tokenizer not available, LLaVADataBlockStage1 not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (DataBlock Definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting DataBlock summary...\n",
      "DataBlock Summary:\n",
      "  Setting up Pipeline: partial -> get_image_path\n",
      "  Setting up Pipeline: partial -> get_conversations -> LLaVATextTokenizer\n",
      "  \n",
      "  Building validation fold (1%)\n",
      "    Setting up Pipeline: partial -> get_image_path\n",
      "    Setting up Pipeline: partial -> get_conversations -> LLaVATextTokenizer\n",
      "    \n",
      "\n",
      "Skipping summary: FileNotFoundError: [Errno 2] No such file or directory: '/path/to/your/datasets/llava_pretrain/llava_pretrain.jsonl'\n",
      "Please ensure 'paths.data_base', 'paths.stage1_data', and 'paths.stage1_images' are correctly set in configs/config.yaml and point to existing data.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "if LLaVADataBlockStage1:\n",
    "    try:\n",
    "        # Load config to pass to get_items via the DataBlock\n",
    "        config = load_config('configs/config.yaml') \n",
    "        print(\"Attempting DataBlock summary...\")\n",
    "        # This will call get_items(config) internally\n",
    "        # It requires valid paths in config.yaml and actual data to succeed fully.\n",
    "        LLaVADataBlockStage1.summary(config=config, bs=4)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nSkipping summary: FileNotFoundError: {e}\")\n",
    "        print(\"Please ensure 'paths.data_base', 'paths.stage1_data', and 'paths.stage1_images' are correctly set in configs/config.yaml and point to existing data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nSkipping summary: Exception occurred during gathering samples: {e}\")\n",
    "        print(\"Check paths in config.yaml and ensure data files are accessible and correctly formatted.\")\n",
    "else:\n",
    "    print(\"LLaVADataBlockStage1 not defined, cannot show summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6: Create DataLoaders (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how to create the `DataLoaders` object from the `DataBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stage1_dataloaders(config: dict, dblock: DataBlock = LLaVADataBlockStage1) -> DataLoaders:\n",
    "    \"\"\"Creates fastai DataLoaders for Stage 1 training.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "        dblock: The configured DataBlock for Stage 1 (defaults to LLaVADataBlockStage1).\n",
    "\n",
    "    Returns:\n",
    "        A fastai DataLoaders object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the DataBlock is not defined.\n",
    "        FileNotFoundError: If data paths are invalid during DataBlock processing.\n",
    "    \"\"\"\n",
    "    if dblock is None:\n",
    "        raise ValueError(\"Stage 1 DataBlock is not defined. Ensure tokenizer loaded correctly.\")\n",
    "\n",
    "    batch_size = config.get('data', {}).get('batch_size_per_device_stage1', 8)\n",
    "    num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "\n",
    "    print(f\"Creating Stage 1 DataLoaders with batch size: {batch_size}, num_workers: {num_workers}\")\n",
    "\n",
    "    # The DataBlock's get_items function needs the config,\n",
    "    # we pass it here when calling dataloaders()\n",
    "    try:\n",
    "        # Pass config explicitly to dataloaders, which passes it down to get_items\n",
    "        dls = dblock.dataloaders(source=None, # Source is determined by get_items\n",
    "                                 config=config, # Pass config to be used by get_items\n",
    "                                 bs=batch_size,\n",
    "                                 num_workers=num_workers)\n",
    "        print(\"DataLoaders created successfully.\")\n",
    "        return dls\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error creating DataLoaders: {e}\")\n",
    "        print(\"Please ensure data paths in config.yaml are correct and data exists.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An unexpected error occurred during DataLoaders creation: {e}\")\n",
    "        traceback.print_exc() # Print full traceback for debugging\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from configs/config.yaml\n",
      "Creating Stage 1 DataLoaders with batch size: 8, num_workers: 4\n",
      "Loading Stage 1 items from: /path/to/your/datasets/llava_pretrain/llava_pretrain.jsonl\n",
      "Assuming images relative to: /path/to/your/datasets/llava_pretrain/images\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/your/datasets/llava_pretrain/llava_pretrain.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;01m:\n\u001b[1;32m      3\u001b[0m     config \u001b[38;5;241m=\u001b[39m load_config(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigs/config.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"Loaded config from \u001b[39m\u001b[38;5;132;01m{\u001b[39;01mconfig_path\u001b[38;5;132;01m}\u001b[39;01m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     dls \u001b[38;5;241m=\u001b[39m \u001b[43mget_stage1_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoaders created. Testing show_batch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Now that batch transforms are complete, show_batch should work if data exists\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# dls.show_batch(max_n=4, figsize=(12, 8))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"Testing one_batch...\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m, in \u001b[0;36mget_stage1_dataloaders\u001b[0;34m(config, dblock)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# we pass it here when calling dataloaders()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;01m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Pass config explicitly to dataloaders, which passes it down to get_items\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     dls \u001b[38;5;241m=\u001b[39m \u001b[43mdblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43mNone\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43m# Source is determined by get_items\u001b[39;49m\u001b[43m\u001b[49m\n\u001b[1;32m     26\u001b[0m                                  \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43m# Pass config to be used by get_items\u001b[39;49m\u001b[43m\u001b[49m\n\u001b[1;32m     27\u001b[0m                                  \u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m                                  \u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoaders created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/adaptive-patching/lib/python3.10/site-packages/fastai/data/block.py:158\u001b[0m, in \u001b[0;36mDataBlock.dataloaders\u001b[0;34m(self, source, path, verbose, bs, num_workers, pin_memory, timeout, batch_size, shuffle, drop_last, shuffle_train, n, device, dl_type, dl_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;01m source \u001b[38;5;129;01mis\u001b[39;01m \u001b[38;5;129;01mnot\u001b[39;01m \u001b[38;5;129;01mNone\u001b[39;01m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m=\u001b[39m source\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;129;01mor\u001b[39;01m bs\n\u001b[1;32m    157\u001b[0m dsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets(source, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m**\u001b[39mkwargs)\n\u001b[0;32m--> 158\u001b[0m dls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dls(dsets, path, \u001b[38;5;28;01mTrue\u001b[39;01m, batch_size, num_workers, pin_memory, timeout,\n\u001b[1;32m    159\u001b[0m                 shuffle, drop_last, shuffle_train, n, device, dl_type, dl_kwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dls_args \u001b[38;5;241m=\u001b[39m (path, \u001b[38;5;28;01mTrue\u001b[39;01m, batch_size, num_workers, pin_memory, timeout,\n\u001b[1;32m    161\u001b[0m                    shuffle, drop_last, shuffle_train, n, device, dl_type, dl_kwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/adaptive-patching/lib/python3.10/site-packages/fastai/data/block.py:147\u001b[0m, in \u001b[0;36mDataBlock.datasets\u001b[0;34m(self, source, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m=\u001b[39m source\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;01m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items \u001b[38;5;129;01mis\u001b[39;01m \u001b[38;5;129;01mNone\u001b[39;01m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_items(source, \u001b[38;5;241m**\u001b[39mkwargs)\n\u001b[0;32m--> 147\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items)\n\u001b[1;32m    148\u001b[0m pv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'Splitting items into {splits}\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;01m \u001b[38;5;28;01mnot\u001b[39;01m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inp:\n",
      "File \u001b[0;32m/opt/conda/envs/adaptive-patching/lib/python3.10/site-packages/fastcore/basics.py:486\u001b[0m, in \u001b[0;36mReindexSplitter.__call__\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;01m \u001b[38;5;21;01m__call__\u001b[39;01m(\u001b[38;5;28mself\u001b[39m, items):\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;01m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplits \u001b[38;5;129;01mis\u001b[39;01m \u001b[38;5;129;01mNone\u001b[39;01m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit(items)\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;01m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplits\n",
      "File \u001b[0;32m/opt/conda/envs/adaptive-patching/lib/python3.10/site-packages/fastcore/basics.py:479\u001b[0m, in \u001b[0;36mRandomSplitter.split\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b;32m\"Split `items` between train/val with `valid_pct` randomly.\"\u001b[0m\n\u001b[1;32m    478\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed\n\u001b[0;32m--> 479\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(items))\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;01m start \u001b[38;5;129;01mis\u001b[39;01m \u001b[38;5;129;01mnot\u001b[39;01m \u001b[38;5;129;01mNone\u001b[39;01m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m    481\u001b[0m cut \u001b[38;5;241m=\u001b[39m int(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_pct \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(items))\n",
      "File \u001b[0;32m/opt/conda/envs/adaptive-patching/lib/python3.10/site-packages/fastai/data/block.py:121\u001b[0m, in \u001b[0;36mDataBlock.get_items\u001b[0;34m(self, source, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b;32m\"Pass `source` to `get_items` unless it's None, then pass `self.source`\"\u001b[0m\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;01m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_items(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource \u001b[38;5;129;01mif\u001b[39;01m source \u001b[38;5;129;01mis\u001b[39;01m \u001b[38;5;129;01mNone\u001b[39;01m \u001b[38;5;129;01melse\u001b[39;01m source, \u001b[38;5;241m**\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m, in \u001b[0;36mget_llava_items\u001b[0;34m(config, stage)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# or if image existence check is disabled in parse_llava_jsonl\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;01m\u001b[38;5;28mlen\u001b[39m(samples)\u001b[38;5;132;01m}\u001b[39;01m\u001b[38;5;124m samples for Stage \u001b[39m\u001b[38;5;132;01m{\u001b[39;01mstage\u001b[38;5;132;01m}\u001b[39;01m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mparse_llava_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsonl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;01m samples\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mparse_llava_jsonl\u001b[0;34m(jsonl_path, image_folder)\u001b[0m\n\u001b[1;32m     14\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m Path(image_folder)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;01m \u001b[38;5;129;01mnot\u001b[39;01m jsonl_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;01m \u001b[43mFileNotFoundError\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJSONL file not found: \u001b[39;49m\u001b[38;5;132;01;43m{\u001b[39;01;49m\u001b[43mjsonl_path\u001b[49m\u001b[38;5;132;01;43m}\u001b[39;01;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;01m \u001b[38;5;28mopen\u001b[39m(jsonl_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;01m f:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/datasets/llava_pretrain/llava_pretrain.jsonl'"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Example usage: Create DataLoaders (requires config.yaml and actual data)\n",
    "if LLaVADataBlockStage1:\n",
    "    try:\n",
    "        config_path = 'configs/config.yaml'\n",
    "        config = load_config(config_path)\n",
    "        print(f\"Loaded config from {config_path}\")\n",
    "        dls = get_stage1_dataloaders(config)\n",
    "\n",
    "        print(f\"DataLoaders created. Testing show_batch...\")\n",
    "        # Now that batch transforms are complete, show_batch should work if data exists\n",
    "        # dls.show_batch(max_n=4, figsize=(12, 8))\n",
    "        print(\"Testing one_batch...\")\n",
    "        # b = dls.one_batch()\n",
    "        # print(\"one_batch() output keys:\", list(b.keys()) if isinstance(b, dict) else type(b))\n",
    "        # print(\"Pixel values shape:\", b.get('pixel_values', torch.Tensor()).shape)\n",
    "        # print(\"Input IDs shape:\", b.get('input_ids', torch.Tensor()).shape)\n",
    "        # print(\"Labels shape:\", b.get('labels', torch.Tensor()).shape)\n",
    "        \n",
    "        print(\"\\nNote: Full testing requires valid data paths and files.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nError creating DataLoaders: FileNotFoundError - {e}\")\n",
    "        print(\"Please ensure 'paths.data_base', 'paths.stage1_data', and 'paths.stage1_images' are correctly set in configs/config.yaml and point to existing data.\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"\\nAn unexpected error occurred during DataLoaders creation/testing: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"LLaVADataBlockStage1 not defined, cannot create DataLoaders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Update Data Handling for Stage 2 (Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will contain functions to create DataLoaders specifically for Stage 2 instruction tuning, using the appropriate chat template and label masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "# Placeholder for get_stage2_dataloaders function\n",
    "def get_stage2_dataloaders(config: dict):\n",
    "    \"\"\"Creates fastai DataLoaders for Stage 2 training (Placeholder).\"\"\"\n",
    "    # Similar to stage 1 but uses different template/masking\n",
    "    # Define llava_datablock_stage2 or custom dataset logic\n",
    "    print(\"get_stage2_dataloaders - Placeholder: Not implemented yet.\")\n",
    "    # Example structure:\n",
    "    # from .preprocessing import format_v1_template # Need v1 formatter\n",
    "    # llava_tokenizer_tfm_stage2 = LLaVATextTokenizer(tokenizer, template_formatter=format_v1_template)\n",
    "    # llava_batch_tfm_stage2 = LLaVABatchTransform(tokenizer=tokenizer, template='v1') # Need to adapt batch tfm for v1 masking\n",
    "    # LLaVADataBlockStage2 = DataBlock(\n",
    "    #     blocks=(ImageBlock(cls=PILImage), TransformBlock),\n",
    "    #     get_items=partial(get_llava_items, stage=2),\n",
    "    #     get_x=get_image_path,\n",
    "    #     get_y=get_conversations,\n",
    "    #     splitter=RandomSplitter(valid_pct=0.01, seed=42),\n",
    "    #     item_tfms=[\n",
    "    #         *basic_image_item_tfms,\n",
    "    #         llava_tokenizer_tfm_stage2\n",
    "    #     ],\n",
    "    #     batch_tfms=[\n",
    "    #         IntToFloatTensor(div_mask=torch.BoolTensor([True,False])),\n",
    "    #         clip_normalize,\n",
    "    #         llava_batch_tfm_stage2\n",
    "    #     ]\n",
    "    # )\n",
    "    # batch_size = config.get('data', {}).get('batch_size_per_device_stage2', 4)\n",
    "    # num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "    # dls = LLaVADataBlockStage2.dataloaders(config=config, bs=batch_size, num_workers=num_workers)\n",
    "    # return dls\n",
    "    raise NotImplementedError(\"Stage 2 DataLoaders are not yet implemented.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.4: Implement Custom Evaluation Set Handling (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "# Placeholder for loading custom eval set\n",
    "def get_custom_eval_dataloaders(config: dict):\n",
    "    \"\"\"Creates fastai DataLoaders for the custom evaluation set (Placeholder).\"\"\"\n",
    "    print(\"get_custom_eval_dataloaders - Placeholder: Not implemented yet.\")\n",
    "    # Similar logic to get_stage1/2_dataloaders but points to custom eval data paths\n",
    "    # Might use stage 2 templates/transforms or custom ones.\n",
    "    raise NotImplementedError(\"Custom Eval DataLoaders are not yet implemented.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "metadata": {
   "path": "nbs/10_data_loading.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "Adaptive_Patching_VIT_fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}