{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "> Functions and classes for loading and parsing datasets for LLaVA-style training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n",
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Union, Tuple\n",
    "from dataclasses import dataclass\n",
    "import PIL.Image\n",
    "from functools import partial\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.block import DataBlock, TransformBlock, CategoryBlock # Added CategoryBlock just in case\n",
    "from fastai.data.transforms import parent_label, GrandparentSplitter, RandomSplitter, IntToFloatTensor\n",
    "\n",
    "from llava.utils import load_config\n",
    "# Import necessary items from preprocessing notebook\n",
    "from llava.data.preprocessing import (\n",
    "    tokenizer, \n",
    "    LLaVATextTokenizer, \n",
    "    clip_normalize, \n",
    "    format_plain_template, # Needed if not using LLaVATextTokenizer directly\n",
    "    LLaVABatchTransform # Import the custom batch transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Data Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need functions to parse the JSONL files commonly used in LLaVA datasets. Each line typically contains:\n",
    "- `id`: A unique identifier for the sample.\n",
    "- `image`: The filename of the image (often relative to an `image_folder`).\n",
    "- `conversations`: A list of dictionaries, where each dictionary has `from` ('human' or 'gpt') and `value` (the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class LLaVASample:\n",
    "    \"\"\"Represents a single sample from a LLaVA-style dataset.\n",
    "\n",
    "    Attributes:\n",
    "        sample_id: Unique identifier for the sample.\n",
    "        image_path: Absolute path to the image file.\n",
    "        conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
    "        data_source: Optional field indicating the source dataset.\n",
    "    \"\"\"\n",
    "    sample_id: str\n",
    "    image_path: Path\n",
    "    conversations: List[Dict[str, str]]\n",
    "    data_source: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LLaVASample\n",
       "\n",
       ">      LLaVASample (sample_id:str, image_path:pathlib.Path,\n",
       ">                   conversations:List[Dict[str,str]],\n",
       ">                   data_source:str|None=None)\n",
       "\n",
       "*Represents a single sample from a LLaVA-style dataset.\n",
       "\n",
       "Attributes:\n",
       "    sample_id: Unique identifier for the sample.\n",
       "    image_path: Absolute path to the image file.\n",
       "    conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
       "    data_source: Optional field indicating the source dataset.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LLaVASample\n",
       "\n",
       ">      LLaVASample (sample_id:str, image_path:pathlib.Path,\n",
       ">                   conversations:List[Dict[str,str]],\n",
       ">                   data_source:str|None=None)\n",
       "\n",
       "*Represents a single sample from a LLaVA-style dataset.\n",
       "\n",
       "Attributes:\n",
       "    sample_id: Unique identifier for the sample.\n",
       "    image_path: Absolute path to the image file.\n",
       "    conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
       "    data_source: Optional field indicating the source dataset.*"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLaVASample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_llava_json(json_path: Union[str, Path], image_folder: Union[str, Path]) -> list:\n",
    "    \"\"\"Parses a LLaVA-style JSON file (containing a list of objects) and resolves image paths.\n",
    "       Note: This function assumes the JSON file contains a single JSON array, not JSON Lines.\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the JSON file (expected to contain a list).\n",
    "        image_folder: Path to the directory containing the images referenced in the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the JSON file does not exist.\n",
    "        json.JSONDecodeError: If the file content is not a valid JSON list.\n",
    "        TypeError: If the parsed JSON is not a list.\n",
    "    \"\"\"\n",
    "    json_path = Path(json_path)\n",
    "    image_folder = Path(image_folder)\n",
    "\n",
    "    if not json_path.is_file():\n",
    "        raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n",
    "\n",
    "    samples = []\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            full_data_list = json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Add context to the original error\n",
    "        raise json.JSONDecodeError(f\"Error decoding JSON file {json_path}: {e.msg}\", e.doc, e.pos) from e\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading file {json_path}: {e}\") from e\n",
    "\n",
    "\n",
    "    if not isinstance(full_data_list, list):\n",
    "        raise TypeError(f\"Expected JSON file to contain a list of objects, but got type {type(full_data_list)} in {json_path}\")\n",
    "\n",
    "    for i, data in enumerate(full_data_list):\n",
    "        try:\n",
    "            # Check for required keys\n",
    "            if not isinstance(data, dict) or not all(k in data for k in ['id', 'image', 'conversations']):\n",
    "                print(f\"Warning: Skipping item index {i} due to missing keys ('id', 'image', or 'conversations') or incorrect format in {json_path}. Data: {data}\")\n",
    "                continue\n",
    "\n",
    "            sample_id = data['id']\n",
    "            # Construct the full image path\n",
    "            image_ref = data['image']\n",
    "            if isinstance(image_ref, dict) and 'path' in image_ref:\n",
    "                image_filename = image_ref['path']\n",
    "            elif isinstance(image_ref, str):\n",
    "                image_filename = image_ref\n",
    "            else:\n",
    "                 print(f\"Warning: Skipping item index {i} due to unexpected image field format in {json_path}. Expected string or dict with 'path', got: {type(image_ref)}\")\n",
    "                 continue\n",
    "\n",
    "            # Resolve the image path relative to image_folder\n",
    "            image_path = image_folder / Path(image_filename).name\n",
    "\n",
    "            conversations = data['conversations']\n",
    "            data_source = data.get('data_source')\n",
    "\n",
    "            # Basic validation for conversations format\n",
    "            if not isinstance(conversations, list) or not all(isinstance(turn, dict) and 'from' in turn and 'value' in turn for turn in conversations):\n",
    "                 print(f\"Warning: Skipping item index {i} due to invalid 'conversations' format in {json_path}.\")\n",
    "                 continue\n",
    "\n",
    "            # Optional: Check if image exists (can be slow)\n",
    "            # if not image_path.is_file():\n",
    "            #     print(f\"Warning: Image file not found for sample {sample_id} at {image_path}, skipping item index {i}.\")\n",
    "            #     continue\n",
    "\n",
    "            samples.append(LLaVASample(\n",
    "                sample_id=str(sample_id),\n",
    "                image_path=image_path,\n",
    "                conversations=conversations,\n",
    "                data_source=data_source\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item index {i} in {json_path}: {e}. Data: {data}\")\n",
    "            # Optionally re-raise or just continue\n",
    "            continue\n",
    "\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### parse_llava_json\n",
       "\n",
       ">      parse_llava_json (json_path:Union[str,pathlib.Path],\n",
       ">                        image_folder:Union[str,pathlib.Path])\n",
       "\n",
       "*Parses a LLaVA-style JSON file (containing a list of objects) and resolves image paths.\n",
       "   Note: This function assumes the JSON file contains a single JSON array, not JSON Lines.\n",
       "\n",
       "Args:\n",
       "    json_path: Path to the JSON file (expected to contain a list).\n",
       "    image_folder: Path to the directory containing the images referenced in the JSON file.\n",
       "\n",
       "Returns:\n",
       "    A list of LLaVASample objects.\n",
       "\n",
       "Raises:\n",
       "    FileNotFoundError: If the JSON file does not exist.\n",
       "    json.JSONDecodeError: If the file content is not a valid JSON list.\n",
       "    TypeError: If the parsed JSON is not a list.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### parse_llava_json\n",
       "\n",
       ">      parse_llava_json (json_path:Union[str,pathlib.Path],\n",
       ">                        image_folder:Union[str,pathlib.Path])\n",
       "\n",
       "*Parses a LLaVA-style JSON file (containing a list of objects) and resolves image paths.\n",
       "   Note: This function assumes the JSON file contains a single JSON array, not JSON Lines.\n",
       "\n",
       "Args:\n",
       "    json_path: Path to the JSON file (expected to contain a list).\n",
       "    image_folder: Path to the directory containing the images referenced in the JSON file.\n",
       "\n",
       "Returns:\n",
       "    A list of LLaVASample objects.\n",
       "\n",
       "Raises:\n",
       "    FileNotFoundError: If the JSON file does not exist.\n",
       "    json.JSONDecodeError: If the file content is not a valid JSON list.\n",
       "    TypeError: If the parsed JSON is not a list.*"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(parse_llava_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy directory: dummy_data/images\n",
      "Created dummy jsonl file: dummy_data/dummy_llava_data.jsonl\n",
      "JSON Parsing Error: Error decoding JSON file dummy_data/dummy_llava_data.jsonl: Extra data: line 2 column 1 (char 153)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Create dummy data for testing\n",
    "dummy_data_dir = Path('./dummy_data')\n",
    "dummy_img_dir = dummy_data_dir / 'images'\n",
    "dummy_jsonl_path = dummy_data_dir / 'dummy_llava_data.jsonl'\n",
    "\n",
    "dummy_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created dummy directory: {dummy_img_dir}\")\n",
    "\n",
    "# Create dummy image files (attempt, may fail if PIL issues persist)\n",
    "try:\n",
    "    PIL.Image.new('RGB', (60, 30), color = 'red').save(dummy_img_dir / 'img1.jpg')\n",
    "    PIL.Image.new('RGB', (60, 30), color = 'green').save(dummy_img_dir / 'img2.png')\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not create dummy image files (PIL might not be fully installed or usable): {e}\")\n",
    "\n",
    "# Create dummy jsonl content\n",
    "dummy_jsonl_content = [\n",
    "    {\"id\": \"sample1\", \"image\": \"img1.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nDescribe this.\"}, {\"from\": \"gpt\", \"value\": \"It is red.\"}]}, \n",
    "    {\"id\": \"sample2\", \"image\": \"img2.png\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nWhat color?\"}, {\"from\": \"gpt\", \"value\": \"Green.\"}]},\n",
    "    {\"id\": \"sample3_missing_keys\", \"conversations\": []}, # Missing image/id\n",
    "    {\"id\": \"sample4_bad_conv\", \"image\": \"img1.jpg\", \"conversations\": \"not a list\"}, # Bad conversation format\n",
    "    {\"id\": \"sample5_missing_img_file\", \"image\": \"nonexistent.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\"}, {\"from\": \"gpt\", \"value\": \"...\"}]},\n",
    "]\n",
    "\n",
    "with open(dummy_jsonl_path, 'w') as f:\n",
    "    for item in dummy_jsonl_content:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "print(f\"Created dummy jsonl file: {dummy_jsonl_path}\")\n",
    "\n",
    "# Test parsing\n",
    "try:\n",
    "    parsed_samples = parse_llava_json(dummy_jsonl_path, dummy_img_dir)\n",
    "    print(f\"Successfully parsed {len(parsed_samples)} samples:\")\n",
    "    for sample in parsed_samples:\n",
    "        print(sample)\n",
    "\n",
    "    # Basic checks (adjust expected length based on warnings/skips)\n",
    "    # Check based on the print output, 3 samples are expected now (1, 2, 5)\n",
    "    assert len(parsed_samples) == 3 \n",
    "    assert parsed_samples[0].sample_id == 'sample1'\n",
    "    # Resolve paths for comparison\n",
    "    assert parsed_samples[0].image_path.resolve() == (dummy_img_dir / 'img1.jpg').resolve()\n",
    "    assert parsed_samples[1].sample_id == 'sample2'\n",
    "    assert parsed_samples[1].image_path.resolve() == (dummy_img_dir / 'img2.png').resolve()\n",
    "    assert parsed_samples[0].conversations[0]['from'] == 'human'\n",
    "    assert parsed_samples[2].sample_id == 'sample5_missing_img_file'\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON Parsing Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Clean up dummy data (optional)\n",
    "# import shutil\n",
    "# if dummy_data_dir.exists():\n",
    "#    shutil.rmtree(dummy_data_dir)\n",
    "#    print(f\"Cleaned up dummy data directory: {dummy_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Image Loading and Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This involves defining the `ImageBlock` and basic `item_tfms` for loading and resizing images. Normalization stats were defined in `11_data_preprocessing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define the ImageBlock using PILImage for loading images\n",
    "image_block = ImageBlock(cls=PILImage)\n",
    "\n",
    "# Define basic item transforms for image processing\n",
    "# 1. Resize images to 336x336, padding if necessary\n",
    "#    method='pad': Pads the image to the target size.\n",
    "#    pad_mode='const': Uses a constant value for padding.\n",
    "#    pad_value=0: Uses black for padding (common for vision models).\n",
    "# 2. Convert the image to a PyTorch tensor.\n",
    "# Note: Normalization will be applied in batch_tfms using clip_normalize.\n",
    "basic_image_item_tfms = [\n",
    "    Resize(336),\n",
    "    ToTensor(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Define Custom Dataset/DataBlock (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the fastai `DataBlock` for Stage 1 projector pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_llava_items(config_source: dict, stage: int = 1) -> list:\n",
    "    \"\"\"Loads LLaVA samples for a specific stage based on config passed as source.\n",
    "\n",
    "    Args:\n",
    "        config_source: The main configuration dictionary (passed as the first argument).\n",
    "        stage: The training stage (1 or 2).\n",
    "\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "    # ... (rest of docstring/implementation) ...\n",
    "    \"\"\"\n",
    "    config = config_source\n",
    "    if not isinstance(config, dict):\n",
    "         raise TypeError(f\"Expected configuration dictionary as the first argument, but got type {type(config_source)}\")\n",
    "\n",
    "    data_base_path = Path(config['paths']['data_base'])\n",
    "    if stage == 1:\n",
    "        json_rel_path = config['paths']['stage1_data'] # Renamed variable for clarity\n",
    "        images_rel_path = config['paths']['stage1_images']\n",
    "    elif stage == 2:\n",
    "        json_rel_path = config['paths']['stage2_data']\n",
    "        images_rel_path = config['paths'].get('stage2_images', '.')\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid stage specified: {stage}. Must be 1 or 2.\")\n",
    "\n",
    "    json_path = data_base_path / json_rel_path # Path to the JSON file\n",
    "    image_folder = data_base_path / images_rel_path\n",
    "\n",
    "    print(f\"Loading Stage {stage} items from: {json_path}\")\n",
    "    print(f\"Assuming images relative to: {image_folder}\")\n",
    "\n",
    "    if not json_path.exists():\n",
    "        raise FileNotFoundError(f\"Stage {stage} JSON file not found: {json_path}\") # Updated error message\n",
    "    if not image_folder.is_dir() and stage == 1:\n",
    "        print(f\"Warning: Stage {stage} image folder not found or not a directory: {image_folder}\")\n",
    "\n",
    "    # Use the updated parsing function\n",
    "    samples = parse_llava_json(json_path, image_folder) # <--- Use parse_llava_json\n",
    "    print(f\"Found {len(samples)} samples for Stage {stage}.\")\n",
    "    return samples\n",
    "\n",
    "# Keep get_image_path and get_conversations as they are\n",
    "def get_image_path(sample: LLaVASample) -> Path:\n",
    "    \"\"\"Extracts the image path from a LLaVASample.\"\"\"\n",
    "    return sample.image_path\n",
    "\n",
    "def get_conversations(sample: LLaVASample) -> list:\n",
    "    \"\"\"Extracts the conversations from a LLaVASample.\"\"\"\n",
    "    return sample.conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0\n",
      "LLaVADataBlockStage1 defined.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Define the DataBlock for Stage 1 (Projector Pre-training)\n",
    "if tokenizer:\n",
    "    # Instantiate the custom batch transform (needs tokenizer)\n",
    "    llava_batch_tfm = LLaVABatchTransform(tokenizer=tokenizer, normalize_tfm=clip_normalize)\n",
    "\n",
    "    LLaVADataBlockStage1 = DataBlock(\n",
    "        blocks=(ImageBlock(cls=PILImage), TransformBlock),\n",
    "        # get_items expects config as first arg (source), use partial only for stage\n",
    "        get_items=partial(get_llava_items, stage=1),\n",
    "        get_x=get_image_path,\n",
    "        get_y=get_conversations,\n",
    "        splitter=RandomSplitter(valid_pct=0.01, seed=42),\n",
    "        item_tfms=[\n",
    "            *basic_image_item_tfms,\n",
    "            LLaVATextTokenizer(tokenizer, template_formatter=format_plain_template)\n",
    "        ],\n",
    "        batch_tfms=[\n",
    "            llava_batch_tfm\n",
    "        ]\n",
    "    )\n",
    "    print(\"LLaVADataBlockStage1 defined.\")\n",
    "else:\n",
    "    LLaVADataBlockStage1 = None\n",
    "    print(\"Tokenizer not available, LLaVADataBlockStage1 not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (DataBlock Definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting DataBlock summary...\n",
      "Setting-up type transforms pipelines\n",
      "Collecting items from {'project_name': 'llava', 'version': 1.0, 'paths': {'data_base': '/workspace/llava/data/', 'output_dir': '/workspace/llava/output/', 'stage1_data': 'llava_pretrain/llava_pretrain.jsonl', 'stage1_images': 'llava_pretrain/images', 'stage2_data': 'llava_instruct_150k/llava_v1_5_mix665k.jsonl', 'stage2_images': '/', 'vqav2_test': 'vqav2/test2015', 'vqav2_test_annotations': 'vqav2/v2_mscoco_test2015_annotations.json', 'textvqa_val': 'textvqa/TextVQA_0.5.1_val.json', 'textvqa_images': 'textvqa/train_images', 'stage1_projector_weights': 'stage1_projector.pth', 'stage2_model_weights': 'stage2_full_model'}, 'model': {'llm_name_or_path': 'lmsys/vicuna-7b-v1.5', 'vision_encoder_name_or_path': 'openai/clip-vit-large-patch14-336', 'vision_feature_layer': -2, 'image_token': '<image>', 'image_token_index_marker': -200, 'projector': {'type': 'mlp_2x', 'input_dim': 1024, 'output_dim': 4096}, 'peft': {'use_lora': True, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'v_proj']}, 'adaptive_patcher': {'enabled': False, 'strategy': None}, 'use_activation_checkpointing': True}, 'data': {'image_size': 336, 'image_aspect_ratio_padding': 'pad', 'image_mean': [0.48145466, 0.4578275, 0.40821073], 'image_std': [0.26862954, 0.26130258, 0.27577711], 'tokenizer_padding_side': 'right', 'tokenizer_model_max_length': 2048, 'stage1_text_template': 'plain', 'stage2_text_template': 'v1', 'batch_size_per_device_stage1': 8, 'batch_size_per_device_stage2': 4, 'num_workers': 4}, 'training': {'seed': 42, 'num_epochs_stage1': 1, 'num_epochs_stage2': 3, 'learning_rate_stage1': 0.0001, 'learning_rate_stage2': 2e-05, 'weight_decay': 0.0, 'optimizer': 'AdamW', 'scheduler': 'cosine', 'warmup_ratio': 0.03, 'gradient_accumulation_steps': 4, 'use_mixed_precision': True, 'freeze_vision_encoder_stage1': True, 'freeze_llm_stage1': True, 'freeze_vision_encoder_stage2': True, 'save_strategy': 'epoch', 'save_total_limit': 1}, 'evaluation': {'eval_batch_size_per_device': 4}, 'logging': {'wandb': {'enabled': True, 'project': 'adaptive_patching_vit', 'entity': 'your_wandb_entity', 'log_model': False}}, 'environment': {'use_deepspeed': False}}\n",
      "Loading Stage 1 items from: /workspace/llava/data/llava_pretrain/llava_pretrain.jsonl\n",
      "Assuming images relative to: /workspace/llava/data/llava_pretrain/images\n",
      "Found 595375 samples for Stage 1.\n",
      "Found 595375 items\n",
      "2 datasets of sizes 589422,5953\n",
      "Setting up Pipeline: get_image_path -> PILBase.create\n",
      "Setting up Pipeline: get_conversations\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: get_image_path -> PILBase.create\n",
      "    starting from\n",
      "      LLaVASample(sample_id='GCC_train_000187464', image_path=Path('/workspace/llava/data/llava_pretrain/images/GCC_train_000187464.jpg'), conversations=[{'from': 'human', 'value': 'Provide a brief description of the given image.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of an isolated laurel wreath with an emotionless text face'}], data_source=None)\n",
      "    applying get_image_path gives\n",
      "      /workspace/llava/data/llava_pretrain/images/GCC_train_000187464.jpg\n",
      "    applying PILBase.create gives\n",
      "      PILImage mode=RGB size=224x224\n",
      "  Pipeline: get_conversations\n",
      "    starting from\n",
      "      LLaVASample(sample_id='GCC_train_000187464', image_path=Path('/workspace/llava/data/llava_pretrain/images/GCC_train_000187464.jpg'), conversations=[{'from': 'human', 'value': 'Provide a brief description of the given image.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of an isolated laurel wreath with an emotionless text face'}], data_source=None)\n",
      "    applying get_conversations gives\n",
      "      [{'from': 'human', 'value': 'Provide a brief description of the given image.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of an isolated laurel wreath with an emotionless text face'}]\n",
      "\n",
      "Final sample: (PILImage mode=RGB size=224x224, [{'from': 'human', 'value': 'Provide a brief description of the given image.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of an isolated laurel wreath with an emotionless text face'}])\n",
      "\n",
      "\n",
      "Collecting items from {'project_name': 'llava', 'version': 1.0, 'paths': {'data_base': '/workspace/llava/data/', 'output_dir': '/workspace/llava/output/', 'stage1_data': 'llava_pretrain/llava_pretrain.jsonl', 'stage1_images': 'llava_pretrain/images', 'stage2_data': 'llava_instruct_150k/llava_v1_5_mix665k.jsonl', 'stage2_images': '/', 'vqav2_test': 'vqav2/test2015', 'vqav2_test_annotations': 'vqav2/v2_mscoco_test2015_annotations.json', 'textvqa_val': 'textvqa/TextVQA_0.5.1_val.json', 'textvqa_images': 'textvqa/train_images', 'stage1_projector_weights': 'stage1_projector.pth', 'stage2_model_weights': 'stage2_full_model'}, 'model': {'llm_name_or_path': 'lmsys/vicuna-7b-v1.5', 'vision_encoder_name_or_path': 'openai/clip-vit-large-patch14-336', 'vision_feature_layer': -2, 'image_token': '<image>', 'image_token_index_marker': -200, 'projector': {'type': 'mlp_2x', 'input_dim': 1024, 'output_dim': 4096}, 'peft': {'use_lora': True, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'v_proj']}, 'adaptive_patcher': {'enabled': False, 'strategy': None}, 'use_activation_checkpointing': True}, 'data': {'image_size': 336, 'image_aspect_ratio_padding': 'pad', 'image_mean': [0.48145466, 0.4578275, 0.40821073], 'image_std': [0.26862954, 0.26130258, 0.27577711], 'tokenizer_padding_side': 'right', 'tokenizer_model_max_length': 2048, 'stage1_text_template': 'plain', 'stage2_text_template': 'v1', 'batch_size_per_device_stage1': 8, 'batch_size_per_device_stage2': 4, 'num_workers': 4}, 'training': {'seed': 42, 'num_epochs_stage1': 1, 'num_epochs_stage2': 3, 'learning_rate_stage1': 0.0001, 'learning_rate_stage2': 2e-05, 'weight_decay': 0.0, 'optimizer': 'AdamW', 'scheduler': 'cosine', 'warmup_ratio': 0.03, 'gradient_accumulation_steps': 4, 'use_mixed_precision': True, 'freeze_vision_encoder_stage1': True, 'freeze_llm_stage1': True, 'freeze_vision_encoder_stage2': True, 'save_strategy': 'epoch', 'save_total_limit': 1}, 'evaluation': {'eval_batch_size_per_device': 4}, 'logging': {'wandb': {'enabled': True, 'project': 'adaptive_patching_vit', 'entity': 'your_wandb_entity', 'log_model': False}}, 'environment': {'use_deepspeed': False}}\n",
      "Loading Stage 1 items from: /workspace/llava/data/llava_pretrain/llava_pretrain.jsonl\n",
      "Assuming images relative to: /workspace/llava/data/llava_pretrain/images\n",
      "Found 595375 samples for Stage 1.\n",
      "Found 595375 items\n",
      "2 datasets of sizes 589422,5953\n",
      "Setting up Pipeline: get_image_path -> PILBase.create\n",
      "Setting up Pipeline: get_conversations\n",
      "Setting up after_item: Pipeline: LLaVATextTokenizer -> Resize -- {'size': (336, 336), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n",
      " -> ToTensor\n",
      "Setting up before_batch: Pipeline: \n",
      "Setting up after_batch: Pipeline: LLaVABatchTransform -> IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n",
      "\n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: LLaVATextTokenizer -> Resize -- {'size': (336, 336), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n",
      " -> ToTensor\n",
      "    starting from\n",
      "      (PILImage mode=RGB size=224x224, [{'from': 'human', 'value': 'Provide a brief description of the given image.\\n<image>'}, {'from': 'gpt', 'value': 'illustration of an isolated laurel wreath with an emotionless text face'}])\n",
      "    applying LLaVATextTokenizer gives\n",
      "      (PILImage mode=RGB size=224x224, [1, 32000, 13, 453, 11036, 310, 385, 23968, 425, 545, 29880, 281, 276, 493, 411, 385, 953, 8194, 2222, 1426, 3700])\n",
      "    applying Resize -- {'size': (336, 336), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (<Resampling.BILINEAR: 2>, <Resampling.NEAREST: 0>), 'p': 1.0}\n",
      " gives\n",
      "      (PILImage mode=RGB size=336x336, [1, 32000, 13, 453, 11036, 310, 385, 23968, 425, 545, 29880, 281, 276, 493, 411, 385, 953, 8194, 2222, 1426, 3700])\n",
      "    applying ToTensor gives\n",
      "      (TensorImage of size 3x336x336, [1, 32000, 13, 453, 11036, 310, 385, 23968, 425, 545, 29880, 281, 276, 493, 411, 385, 953, 8194, 2222, 1426, 3700])\n",
      "\n",
      "Adding the next 3 samples\n",
      "\n",
      "No before_batch transform to apply\n",
      "\n",
      "Collating items in a batch\n",
      "\n",
      "Applying batch_tfms to the batch built\n",
      "  Pipeline: LLaVABatchTransform -> IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n",
      "\n",
      "    starting from\n",
      "      (TensorImage of size 4x3x336x336, [tensor([1, 1, 1, 1], device='cuda:0'), tensor([32000, 32000, 32000, 32000], device='cuda:0'), tensor([13, 13, 13, 13], device='cuda:0'), tensor([  453, 11147,  2783,  6115], device='cuda:0'), tensor([11036,  4272, 29886,   264], device='cuda:0'), tensor([  310, 11952,   568, 28453], device='cuda:0'), tensor([ 385,  411,  278, 1919], device='cuda:0'), tensor([23968, 18647,  7786, 14842], device='cuda:0'), tensor([ 425,  869, 7910, 1429], device='cuda:0')])\n",
      "    applying LLaVABatchTransform gives\n",
      "      (TensorImage of size 4x3x336x336, [tensor([1, 1, 1, 1], device='cuda:0'), tensor([32000, 32000, 32000, 32000], device='cuda:0'), tensor([13, 13, 13, 13], device='cuda:0'), tensor([  453, 11147,  2783,  6115], device='cuda:0'), tensor([11036,  4272, 29886,   264], device='cuda:0'), tensor([  310, 11952,   568, 28453], device='cuda:0'), tensor([ 385,  411,  278, 1919], device='cuda:0'), tensor([23968, 18647,  7786, 14842], device='cuda:0'), tensor([ 425,  869, 7910, 1429], device='cuda:0')])\n",
      "    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n",
      " gives\n",
      "      (TensorImage of size 4x3x336x336, [tensor([1, 1, 1, 1], device='cuda:0'), tensor([32000, 32000, 32000, 32000], device='cuda:0'), tensor([13, 13, 13, 13], device='cuda:0'), tensor([  453, 11147,  2783,  6115], device='cuda:0'), tensor([11036,  4272, 29886,   264], device='cuda:0'), tensor([  310, 11952,   568, 28453], device='cuda:0'), tensor([ 385,  411,  278, 1919], device='cuda:0'), tensor([23968, 18647,  7786, 14842], device='cuda:0'), tensor([ 425,  869, 7910, 1429], device='cuda:0')])\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "if LLaVADataBlockStage1:\n",
    "    try:\n",
    "        config = load_config('../configs/config.yaml')\n",
    "        print(\"Attempting DataBlock summary...\")\n",
    "        # Pass the config dictionary AS the source argument\n",
    "        LLaVADataBlockStage1.summary(source=config, bs=4) # Pass config to source\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nSkipping summary: FileNotFoundError: {e}\")\n",
    "        print(\"Please ensure 'paths.data_base', 'paths.stage1_data', and 'paths.stage1_images' are correctly set in configs/config.yaml and point to existing data.\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"\\nSkipping summary: Exception occurred: {e}\")\n",
    "        traceback.print_exc() # Print stack trace for debugging\n",
    "        print(\"Check paths in config.yaml and ensure data files are accessible and correctly formatted.\")\n",
    "else:\n",
    "    print(\"LLaVADataBlockStage1 not defined, cannot show summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6: Create DataLoaders (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how to create the `DataLoaders` object from the `DataBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stage1_dataloaders(config: dict, dblock: DataBlock = LLaVADataBlockStage1) -> DataLoaders:\n",
    "    \"\"\"Creates fastai DataLoaders for Stage 1 training.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "        dblock: The configured DataBlock for Stage 1 (defaults to LLaVADataBlockStage1).\n",
    "\n",
    "    Returns:\n",
    "        A fastai DataLoaders object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the DataBlock is not defined.\n",
    "        FileNotFoundError: If data paths are invalid during DataBlock processing.\n",
    "    \"\"\"\n",
    "    if dblock is None:\n",
    "        raise ValueError(\"Stage 1 DataBlock is not defined. Ensure tokenizer loaded correctly.\")\n",
    "\n",
    "    batch_size = config.get('data', {}).get('batch_size_per_device_stage1', 8)\n",
    "    num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "\n",
    "    print(f\"Creating Stage 1 DataLoaders with batch size: {batch_size}, num_workers: {num_workers}\")\n",
    "\n",
    "    # The DataBlock's get_items function needs the config,\n",
    "    # we pass it here when calling dataloaders()\n",
    "    try:\n",
    "        # Pass config explicitly to dataloaders, which passes it down to get_items\n",
    "        dls = dblock.dataloaders(source=config, # Pass config to be used by get_items\n",
    "                                 bs=batch_size,\n",
    "                                 num_workers=num_workers)\n",
    "        print(\"DataLoaders created successfully.\")\n",
    "        return dls\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error creating DataLoaders: {e}\")\n",
    "        print(\"Please ensure data paths in config.yaml are correct and data exists.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An unexpected error occurred during DataLoaders creation: {e}\")\n",
    "        traceback.print_exc() # Print full traceback for debugging\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Creating Stage 1 DataLoaders with batch size: 8, num_workers: 4\n",
      "Loading Stage 1 items from: /workspace/llava/data/llava_pretrain/llava_pretrain.jsonl\n",
      "Assuming images relative to: /workspace/llava/data/llava_pretrain/images\n",
      "Found 595375 samples for Stage 1.\n",
      "DataLoaders created successfully.\n",
      "\n",
      "Testing one_batch...\n",
      "one_batch() retrieved. Check keys and shapes.\n",
      "len(b): 2\n",
      "b[0]: TensorImage([[[[0.0353, 0.0549, 0.0745,  ..., 0.3529, 0.3569, 0.3098],\n",
      "               [0.0784, 0.0588, 0.0471,  ..., 0.4078, 0.3922, 0.3294],\n",
      "               [0.1529, 0.0863, 0.0275,  ..., 0.4471, 0.4078, 0.3255],\n",
      "               ...,\n",
      "               [0.0510, 0.1686, 0.3137,  ..., 0.1059, 0.1059, 0.1137],\n",
      "               [0.0706, 0.1137, 0.1765,  ..., 0.0745, 0.0784, 0.0824],\n",
      "               [0.0902, 0.0824, 0.0706,  ..., 0.0471, 0.0510, 0.0549]],\n",
      "\n",
      "              [[0.0863, 0.1059, 0.1255,  ..., 0.3608, 0.3647, 0.3176],\n",
      "               [0.1294, 0.1098, 0.0980,  ..., 0.4157, 0.4000, 0.3373],\n",
      "               [0.2039, 0.1373, 0.0784,  ..., 0.4549, 0.4157, 0.3333],\n",
      "               ...,\n",
      "               [0.0588, 0.1725, 0.3176,  ..., 0.0980, 0.1020, 0.1137],\n",
      "               [0.0745, 0.1176, 0.1804,  ..., 0.0706, 0.0745, 0.0824],\n",
      "               [0.0941, 0.0863, 0.0745,  ..., 0.0471, 0.0510, 0.0549]],\n",
      "\n",
      "              [[0.0196, 0.0392, 0.0588,  ..., 0.3490, 0.3529, 0.3059],\n",
      "               [0.0627, 0.0471, 0.0353,  ..., 0.4039, 0.3882, 0.3255],\n",
      "               [0.1373, 0.0745, 0.0157,  ..., 0.4431, 0.4039, 0.3216],\n",
      "               ...,\n",
      "               [0.0196, 0.1373, 0.2824,  ..., 0.0980, 0.1059, 0.1137],\n",
      "               [0.0471, 0.0902, 0.1529,  ..., 0.0706, 0.0784, 0.0824],\n",
      "               [0.0706, 0.0627, 0.0510,  ..., 0.0471, 0.0510, 0.0549]]],\n",
      "\n",
      "\n",
      "             [[[0.5647, 0.6824, 0.8196,  ..., 0.3216, 0.2706, 0.2431],\n",
      "               [0.6118, 0.7137, 0.8353,  ..., 0.2941, 0.2627, 0.2353],\n",
      "               [0.6824, 0.7608, 0.8549,  ..., 0.2627, 0.2549, 0.2275],\n",
      "               ...,\n",
      "               [0.1020, 0.0902, 0.0745,  ..., 0.3216, 0.2980, 0.2902],\n",
      "               [0.1020, 0.0902, 0.0706,  ..., 0.3804, 0.3412, 0.3059],\n",
      "               [0.1020, 0.0902, 0.0706,  ..., 0.4275, 0.3725, 0.3176]],\n",
      "\n",
      "              [[0.5608, 0.6784, 0.8157,  ..., 0.3216, 0.2706, 0.2431],\n",
      "               [0.6078, 0.7098, 0.8314,  ..., 0.2941, 0.2627, 0.2353],\n",
      "               [0.6784, 0.7569, 0.8510,  ..., 0.2627, 0.2549, 0.2275],\n",
      "               ...,\n",
      "               [0.1059, 0.0941, 0.0784,  ..., 0.3569, 0.3373, 0.3255],\n",
      "               [0.1059, 0.0941, 0.0745,  ..., 0.4196, 0.3804, 0.3451],\n",
      "               [0.1059, 0.0941, 0.0745,  ..., 0.4667, 0.4118, 0.3569]],\n",
      "\n",
      "              [[0.5412, 0.6588, 0.7961,  ..., 0.2824, 0.2314, 0.2039],\n",
      "               [0.5882, 0.6902, 0.8118,  ..., 0.2549, 0.2235, 0.1961],\n",
      "               [0.6588, 0.7373, 0.8314,  ..., 0.2235, 0.2157, 0.1882],\n",
      "               ...,\n",
      "               [0.1765, 0.1647, 0.1451,  ..., 0.4039, 0.3843, 0.3725],\n",
      "               [0.1765, 0.1647, 0.1451,  ..., 0.4667, 0.4275, 0.3922],\n",
      "               [0.1765, 0.1647, 0.1451,  ..., 0.5137, 0.4588, 0.4039]]],\n",
      "\n",
      "\n",
      "             [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               ...,\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "              [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               ...,\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "              [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               ...,\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "             ...,\n",
      "\n",
      "\n",
      "             [[[0.9569, 0.9569, 0.9569,  ..., 0.8980, 0.8980, 0.8980],\n",
      "               [0.9569, 0.9569, 0.9569,  ..., 0.8980, 0.8980, 0.8980],\n",
      "               [0.9569, 0.9569, 0.9569,  ..., 0.8980, 0.8980, 0.8980],\n",
      "               ...,\n",
      "               [0.7451, 0.7373, 0.7216,  ..., 0.6392, 0.6392, 0.6392],\n",
      "               [0.7569, 0.7490, 0.7333,  ..., 0.6549, 0.6549, 0.6549],\n",
      "               [0.7647, 0.7529, 0.7373,  ..., 0.6588, 0.6588, 0.6588]],\n",
      "\n",
      "              [[0.9608, 0.9608, 0.9608,  ..., 0.8941, 0.8941, 0.8941],\n",
      "               [0.9608, 0.9608, 0.9608,  ..., 0.8941, 0.8941, 0.8941],\n",
      "               [0.9608, 0.9608, 0.9608,  ..., 0.8941, 0.8941, 0.8941],\n",
      "               ...,\n",
      "               [0.7412, 0.7333, 0.7176,  ..., 0.6353, 0.6353, 0.6353],\n",
      "               [0.7529, 0.7451, 0.7294,  ..., 0.6510, 0.6510, 0.6510],\n",
      "               [0.7608, 0.7490, 0.7333,  ..., 0.6549, 0.6549, 0.6549]],\n",
      "\n",
      "              [[0.9804, 0.9804, 0.9804,  ..., 0.9137, 0.9137, 0.9137],\n",
      "               [0.9804, 0.9804, 0.9804,  ..., 0.9137, 0.9137, 0.9137],\n",
      "               [0.9804, 0.9804, 0.9804,  ..., 0.9137, 0.9137, 0.9137],\n",
      "               ...,\n",
      "               [0.7608, 0.7529, 0.7373,  ..., 0.6588, 0.6588, 0.6588],\n",
      "               [0.7725, 0.7647, 0.7490,  ..., 0.6745, 0.6745, 0.6745],\n",
      "               [0.7804, 0.7686, 0.7529,  ..., 0.6784, 0.6784, 0.6784]]],\n",
      "\n",
      "\n",
      "             [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               ...,\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "              [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               ...,\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "              [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               ...,\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "             [[[0.8510, 0.8510, 0.8510,  ..., 0.9137, 0.9059, 0.8980],\n",
      "               [0.8510, 0.8549, 0.8549,  ..., 0.9176, 0.9098, 0.9059],\n",
      "               [0.8510, 0.8549, 0.8549,  ..., 0.9255, 0.9137, 0.9098],\n",
      "               ...,\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "              [[0.7333, 0.7333, 0.7333,  ..., 0.9059, 0.9098, 0.9137],\n",
      "               [0.7333, 0.7373, 0.7373,  ..., 0.9098, 0.9137, 0.9137],\n",
      "               [0.7333, 0.7373, 0.7373,  ..., 0.9098, 0.9137, 0.9137],\n",
      "               ...,\n",
      "               [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
      "               [0.9922, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "              [[0.6392, 0.6392, 0.6392,  ..., 0.9216, 0.9255, 0.9255],\n",
      "               [0.6392, 0.6431, 0.6431,  ..., 0.9216, 0.9255, 0.9255],\n",
      "               [0.6392, 0.6431, 0.6431,  ..., 0.9216, 0.9216, 0.9216],\n",
      "               ...,\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 0.9922, 0.9922, 0.9922],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 0.9922, 0.9922, 0.9922],\n",
      "               [1.0000, 1.0000, 1.0000,  ..., 0.9922, 0.9922, 0.9922]]]],\n",
      "            device='cuda:0')\n",
      "b[1]: [tensor([1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), tensor([32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000],\n",
      "       device='cuda:0'), tensor([13, 13, 13, 13, 13, 13, 13, 13], device='cuda:0'), tensor([10532, 29874,   305,   386, 29874,  8159,   453, 11991],\n",
      "       device='cuda:0'), tensor([12045, 14892,  2021,  1331,   261,  3573, 11036,  6509],\n",
      "       device='cuda:0'), tensor([  287,   833,  8247, 29887,   616, 21760,   310,  5199],\n",
      "       device='cuda:0'), tensor([  278,   271, 21046,  4357,  1776,   310,   263,  4086],\n",
      "       device='cuda:0'), tensor([2358, 1791,  373, 2462,  310,  263, 1361,  297], device='cuda:0'), tensor([  493, 29879,   263,  8090, 28396,  4123,  2135,   263],\n",
      "       device='cuda:0'), tensor([ 310, 1546,  274,  363, 1919,  767, 4847,  770], device='cuda:0'), tensor([  278, 13814,  1296,  2022, 10067, 22049, 12500,  8345],\n",
      "       device='cuda:0')]\n",
      "len(b[0]): 8\n",
      "len(b[1]): 11\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Example usage: Create DataLoaders (requires config.yaml and actual data)\n",
    "try:\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    dls = get_stage1_dataloaders(config)\n",
    "\n",
    "    # print(f\"DataLoaders created. Testing show_batch...\")\n",
    "    # Now that batch transforms are complete, show_batch should work if data exists\n",
    "    # Ensure you have data available at the paths specified in config.yaml\n",
    "    # dls.show_batch(max_n=4, figsize=(12, 8))\n",
    "    print(\"\\nTesting one_batch...\")\n",
    "    b = dls.one_batch()\n",
    "    print(\"one_batch() retrieved. Check keys and shapes.\")\n",
    "    # Example: print shapes of tensors in the batch\n",
    "    print(f'len(b): {len(b)}')\n",
    "    \n",
    "    print(f'b[0]: {b[0]}')\n",
    "    print(f'b[1]: {b[1]}')\n",
    "    \n",
    "    print(f'len(b[0]): {len(b[0])}')\n",
    "    print(f'len(b[1]): {len(b[1])}')\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nError creating DataLoaders: FileNotFoundError - {e}\")\n",
    "    print(\"Please ensure 'paths.data_base', 'paths.stage1_data', and 'paths.stage1_images' are correctly set in configs/config.yaml and point to existing data.\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\nAn unexpected error occurred during DataLoaders creation/testing: {e}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Update Data Handling for Stage 2 (Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will contain functions to create DataLoaders specifically for Stage 2 instruction tuning, using the appropriate chat template and label masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Placeholder for get_stage2_dataloaders function\n",
    "def get_stage2_dataloaders(config: dict):\n",
    "    \"\"\"Creates fastai DataLoaders for Stage 2 training (Placeholder).\"\"\"\n",
    "    # Similar to stage 1 but uses different template/masking\n",
    "    # Define llava_datablock_stage2 or custom dataset logic\n",
    "    print(\"get_stage2_dataloaders - Placeholder: Not implemented yet.\")\n",
    "    # Example structure:\n",
    "    # from .preprocessing import format_v1_template # Need v1 formatter\n",
    "    # llava_tokenizer_tfm_stage2 = LLaVATextTokenizer(tokenizer, template_formatter=format_v1_template)\n",
    "    # llava_batch_tfm_stage2 = LLaVABatchTransform(tokenizer=tokenizer, template='v1') # Need to adapt batch tfm for v1 masking\n",
    "    # LLaVADataBlockStage2 = DataBlock(\n",
    "    #     blocks=(ImageBlock(cls=PILImage), TransformBlock),\n",
    "    #     get_items=partial(get_llava_items, stage=2),\n",
    "    #     get_x=get_image_path,\n",
    "    #     get_y=get_conversations,\n",
    "    #     splitter=RandomSplitter(valid_pct=0.01, seed=42),\n",
    "    #     item_tfms=[\n",
    "    #         *basic_image_item_tfms,\n",
    "    #         llava_tokenizer_tfm_stage2\n",
    "    #     ],\n",
    "    #     batch_tfms=[\n",
    "    #         IntToFloatTensor(div_mask=torch.BoolTensor([True,False])),\n",
    "    #         clip_normalize,\n",
    "    #         llava_batch_tfm_stage2\n",
    "    #     ]\n",
    "    # )\n",
    "    # batch_size = config.get('data', {}).get('batch_size_per_device_stage2', 4)\n",
    "    # num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "    # dls = LLaVADataBlockStage2.dataloaders(config=config, bs=batch_size, num_workers=num_workers)\n",
    "    # return dls\n",
    "    raise NotImplementedError(\"Stage 2 DataLoaders are not yet implemented.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.4: Implement Custom Evaluation Set Handling (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Placeholder for loading custom eval set\n",
    "def get_custom_eval_dataloaders(config: dict):\n",
    "    \"\"\"Creates fastai DataLoaders for the custom evaluation set (Placeholder).\"\"\"\n",
    "    print(\"get_custom_eval_dataloaders - Placeholder: Not implemented yet.\")\n",
    "    # Similar logic to get_stage1/2_dataloaders but points to custom eval data paths\n",
    "    # Might use stage 2 templates/transforms or custom ones.\n",
    "    raise NotImplementedError(\"Custom Eval DataLoaders are not yet implemented.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/10_data_loading.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "Adaptive_Patching_VIT_fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
