{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "> Functions and classes for loading and parsing datasets for LLaVA-style training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# --- Project Root Setup --- \n",
    "project_root = Path(os.getcwd())\n",
    "# Check if running from nbs/\n",
    "if project_root.name == 'nbs' and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "# Check if running from scripts/\n",
    "elif project_root.name == 'scripts' and (project_root.parent / 'settings.ini').exists():\n",
    "     project_root = project_root.parent\n",
    "elif not (project_root / 'settings.ini').exists():\n",
    "     # Try going up one level if settings.ini not found directly\n",
    "     if (project_root.parent / 'settings.ini').exists():\n",
    "          project_root = project_root.parent\n",
    "     else:\n",
    "          print(\"Warning: Could not automatically determine project root. Assuming current dir.\")\n",
    "          # Fallback: Assume running from project root if structure unknown\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    # print(f\"Project root already in sys.path: {project_root_str}\") # Less verbose\n",
    "    pass\n",
    "# --- End Project Root Setup --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llava.llava'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m         tokenizer, \n\u001b[1;32m      3\u001b[0m         LLaVATextTokenizer, \n\u001b[1;32m      4\u001b[0m         clip_normalize, \n\u001b[1;32m      5\u001b[0m         format_plain_template, \n\u001b[1;32m      6\u001b[0m         format_v1_template, \u001b[38;5;66;03m# Import the V1 formatter\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         LLaVABatchTransform \u001b[38;5;66;03m# Import the updated batch transform\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llava.llava'"
     ]
    }
   ],
   "source": [
    "from llava.data.preprocessing import (\n",
    "        tokenizer, \n",
    "        LLaVATextTokenizer, \n",
    "        clip_normalize, \n",
    "        format_plain_template, \n",
    "        format_v1_template, # Import the V1 formatter\n",
    "        LLaVABatchTransform # Import the updated batch transform\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: llava.data.preprocessing not found or incomplete. Data loading might fail.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Union, Tuple\n",
    "from dataclasses import dataclass\n",
    "import PIL.Image\n",
    "from functools import partial\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.block import DataBlock, TransformBlock, CategoryBlock # Added CategoryBlock just in case\n",
    "from fastai.data.transforms import parent_label, GrandparentSplitter, RandomSplitter, IntToFloatTensor\n",
    "from fastai.data.core import DataLoaders, DataLoader # Import DataLoaders\n",
    "\n",
    "# Attempt to import from llava utils, handle potential ImportError if running standalone\n",
    "try:\n",
    "    from llava.utils import load_config\n",
    "except ImportError:\n",
    "    print(\"Warning: llava.utils not found. load_config function might be unavailable.\")\n",
    "    def load_config(path): return {}\n",
    "    \n",
    "# Import necessary items from preprocessing notebook\n",
    "try:\n",
    "    from llava.data.preprocessing import (\n",
    "        tokenizer, \n",
    "        LLaVATextTokenizer, \n",
    "        clip_normalize, \n",
    "        format_plain_template, \n",
    "        format_v1_template, # Import the V1 formatter\n",
    "        LLaVABatchTransform # Import the updated batch transform\n",
    "    )\n",
    "except ImportError:\n",
    "     print(\"Warning: llava.data.preprocessing not found or incomplete. Data loading might fail.\")\n",
    "     # Define dummy classes/functions if needed for basic script execution\n",
    "     tokenizer = None\n",
    "     clip_normalize = lambda x: x # Dummy normalize\n",
    "     def format_plain_template(conv, tok): return \"<image>\\nplaceholder\"\n",
    "     def format_v1_template(conv, tok): return \"USER: <image>\\nplaceholder ASSISTANT: response\"\n",
    "     class LLaVATextTokenizer(Transform):\n",
    "        def __init__(self, *args, **kwargs): pass\n",
    "        def encodes(self, x): return [0, 1, 2]\n",
    "     class LLaVABatchTransform(Transform):\n",
    "        def __init__(self, *args, **kwargs): self.split_idx=None\n",
    "        def encodes(self, x): return {'pixel_values': x[0], 'input_ids': x[1], 'attention_mask': torch.ones_like(x[1]), 'labels': x[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Data Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need functions to parse the JSONL files commonly used in LLaVA datasets. Each line typically contains:\n",
    "- `id`: A unique identifier for the sample.\n",
    "- `image`: The filename of the image (often relative to an `image_folder`).\n",
    "- `conversations`: A list of dictionaries, where each dictionary has `from` ('human' or 'gpt') and `value` (the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class LLaVASample:\n",
    "    \"\"\"Represents a single sample from a LLaVA-style dataset.\n",
    "\n",
    "    Attributes:\n",
    "        sample_id: Unique identifier for the sample.\n",
    "        image_path: Absolute or relative path to the image file.\n",
    "        conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
    "        data_source: Optional field indicating the source dataset.\n",
    "    \"\"\"\n",
    "    sample_id: str\n",
    "    image_path: Path\n",
    "    conversations: List[Dict[str, str]]\n",
    "    data_source: str | None = None\n",
    "    image_folder: Path | None = None # Store base image folder for resolving relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LLaVASample\n",
       "\n",
       ">      LLaVASample (sample_id:str, image_path:pathlib.Path,\n",
       ">                   conversations:List[Dict[str,str]],\n",
       ">                   data_source:str|None=None,\n",
       ">                   image_folder:pathlib.Path|None=None)\n",
       "\n",
       "*Represents a single sample from a LLaVA-style dataset.\n",
       "\n",
       "Attributes:\n",
       "    sample_id: Unique identifier for the sample.\n",
       "    image_path: Absolute or relative path to the image file.\n",
       "    conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
       "    data_source: Optional field indicating the source dataset.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LLaVASample\n",
       "\n",
       ">      LLaVASample (sample_id:str, image_path:pathlib.Path,\n",
       ">                   conversations:List[Dict[str,str]],\n",
       ">                   data_source:str|None=None,\n",
       ">                   image_folder:pathlib.Path|None=None)\n",
       "\n",
       "*Represents a single sample from a LLaVA-style dataset.\n",
       "\n",
       "Attributes:\n",
       "    sample_id: Unique identifier for the sample.\n",
       "    image_path: Absolute or relative path to the image file.\n",
       "    conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
       "    data_source: Optional field indicating the source dataset.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLaVASample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_llava_jsonl(jsonl_path: Union[str, Path], image_folder: Union[str, Path]) -> List[LLaVASample]:\n",
    "    \"\"\"Parses a LLaVA-style JSON Lines file (.jsonl) and prepares LLaVASample objects.\n",
    "    Args:\n",
    "        jsonl_path: Path to the JSON Lines file.\n",
    "        image_folder: Path to the directory containing the images.\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "    Raises:\n",
    "        FileNotFoundError: If the JSONL file does not exist.\n",
    "        json.JSONDecodeError: If a line in the file is not valid JSON.\n",
    "    \"\"\"\n",
    "    jsonl_path = Path(jsonl_path)\n",
    "    image_folder = Path(image_folder).resolve() # Resolve to absolute path\n",
    "    if not jsonl_path.is_file():\n",
    "        raise FileNotFoundError(f\"JSON Lines file not found: {jsonl_path}\")\n",
    "\n",
    "    samples = []\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line.strip()) # Parse each line as JSON\n",
    "                # Allow flexibility: 'id' might not exist in all formats (e.g., pure caption data)\n",
    "                # Minimum requirement: 'image' and ('conversations' or 'caption')\n",
    "                is_valid = isinstance(data, dict) and 'image' in data and ('conversations' in data or 'caption' in data)\n",
    "                if not is_valid:\n",
    "                    print(f\"Warning: Skipping line {i+1} due to missing 'image' or 'conversations'/'caption' key in {jsonl_path}. Data: {data}\")\n",
    "                    continue\n",
    "                \n",
    "                sample_id = data.get('id', f\"item_{i}\") # Use index if ID is missing\n",
    "                image_ref = data['image']\n",
    "                \n",
    "                # Handle image path: could be relative to image_folder or potentially elsewhere\n",
    "                # LLaVA often stores relative paths. Assume relative to image_folder for now.\n",
    "                image_path_relative = Path(image_ref) \n",
    "                # Store the relative path in the sample, resolve later if needed or assume loader does.\n",
    "                # Let's store the intended image folder as well for easier resolution later.\n",
    "                \n",
    "                conversations = data.get('conversations')\n",
    "                caption = data.get('caption')\n",
    "                data_source = data.get('data_source')\n",
    "                \n",
    "                # Handle cases where only 'caption' is present (e.g., CC3M format)\n",
    "                if conversations is None and caption is not None:\n",
    "                    conversations = [\n",
    "                         {\"from\": \"human\", \"value\": \"<image>\"}, \n",
    "                         {\"from\": \"gpt\", \"value\": caption}\n",
    "                     ]\n",
    "                elif not isinstance(conversations, list) or not all(isinstance(turn, dict) and 'from' in turn and 'value' in turn for turn in conversations):\n",
    "                    print(f\"Warning: Skipping line {i+1} due to invalid 'conversations' format in {jsonl_path}.\")\n",
    "                    continue\n",
    "\n",
    "                samples.append(LLaVASample(\n",
    "                    sample_id=str(sample_id),\n",
    "                    image_path=image_path_relative, # Store relative path\n",
    "                    conversations=conversations,\n",
    "                    data_source=data_source,\n",
    "                    image_folder=image_folder # Store base folder\n",
    "                ))\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line {i+1} in {jsonl_path}: {e}. Line content: '{line.strip()}'\")\n",
    "                continue # Skip malformed lines\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {i+1} in {jsonl_path}: {e}. Data: {line.strip()}\")\n",
    "                continue\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### parse_llava_jsonl\n",
       "\n",
       ">      parse_llava_jsonl (jsonl_path:Union[str,pathlib.Path],\n",
       ">                         image_folder:Union[str,pathlib.Path])\n",
       "\n",
       "*Parses a LLaVA-style JSON Lines file (.jsonl) and prepares LLaVASample objects.\n",
       "Args:\n",
       "    jsonl_path: Path to the JSON Lines file.\n",
       "    image_folder: Path to the directory containing the images.\n",
       "Returns:\n",
       "    A list of LLaVASample objects.\n",
       "Raises:\n",
       "    FileNotFoundError: If the JSONL file does not exist.\n",
       "    json.JSONDecodeError: If a line in the file is not valid JSON.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### parse_llava_jsonl\n",
       "\n",
       ">      parse_llava_jsonl (jsonl_path:Union[str,pathlib.Path],\n",
       ">                         image_folder:Union[str,pathlib.Path])\n",
       "\n",
       "*Parses a LLaVA-style JSON Lines file (.jsonl) and prepares LLaVASample objects.\n",
       "Args:\n",
       "    jsonl_path: Path to the JSON Lines file.\n",
       "    image_folder: Path to the directory containing the images.\n",
       "Returns:\n",
       "    A list of LLaVASample objects.\n",
       "Raises:\n",
       "    FileNotFoundError: If the JSONL file does not exist.\n",
       "    json.JSONDecodeError: If a line in the file is not valid JSON.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(parse_llava_jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Image Loading and Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This involves defining the `ImageBlock` and basic `item_tfms` for loading and resizing images. Normalization stats were defined in `11_data_preprocessing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define the ImageBlock using PILImage for loading images\n",
    "image_block = ImageBlock(cls=PILImage)\n",
    "\n",
    "# Define basic item transforms for image processing\n",
    "# 1. Resize images to 336x336, padding if necessary\n",
    "#    method='pad': Pads the image to the target size.\n",
    "#    pad_mode='const': Uses a constant value for padding.\n",
    "#    pad_value=0: Uses black for padding (common for vision models).\n",
    "# 2. Convert the image to a PyTorch tensor.\n",
    "# Note: Normalization will be applied in batch_tfms using clip_normalize.\n",
    "# Update: Use Resize(336, method='pad', pad_mode=PadMode.Constant, pad_value=0) based on tech spec\n",
    "basic_image_item_tfms = [\n",
    "    Resize(336, method='pad', pad_mode=PadMode.Constant, pad_value=0), # Explicit padding\n",
    "    ToTensor(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Define Custom Dataset/DataBlock (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the fastai `DataBlock` for Stage 1 projector pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_llava_items(config_source: dict, stage: Union[int, str] = 1) -> List[LLaVASample]:\n",
    "    \"\"\"Loads LLaVA samples for a specific stage or dataset based on config.\n",
    "\n",
    "    Args:\n",
    "        config_source: The main configuration dictionary (passed as the first argument).\n",
    "        stage: The training stage (1 or 2) or a dataset name string (e.g., 'vqav2_test', 'custom_eval').\n",
    "\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "    \"\"\"\n",
    "    config = config_source\n",
    "    if not isinstance(config, dict):\n",
    "         raise TypeError(f\"Expected configuration dictionary as the first argument, but got type {type(config_source)}\")\n",
    "\n",
    "    data_base_path = Path(config['paths']['data_base'])\n",
    "    dataset_key = None\n",
    "    dataset_type = \"Training Stage\"\n",
    "\n",
    "    if stage == 1:\n",
    "        dataset_key = 'stage1_data'\n",
    "        images_key = 'stage1_images'\n",
    "    elif stage == 2:\n",
    "        dataset_key = 'stage2_data'\n",
    "        images_key = 'stage2_images'\n",
    "    elif isinstance(stage, str):\n",
    "        dataset_key = stage\n",
    "        images_key = stage\n",
    "        dataset_type = \"Dataset\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid stage/dataset identifier: {stage}. Must be 1, 2, or a dataset name string from config.\")\n",
    "\n",
    "    # Get paths from config\n",
    "    dataset_config = config['paths'].get(dataset_key)\n",
    "    if dataset_config is None:\n",
    "        raise ValueError(f\"{dataset_type} '{stage}' not found in config paths.\")\n",
    "\n",
    "    # Handle different config structures (simple path or dict with annotations/images)\n",
    "    if isinstance(dataset_config, str): # e.g., stage1_data: path/to/file.jsonl\n",
    "        json_rel_path = dataset_config\n",
    "        # Try to get corresponding image key, default if needed\n",
    "        images_rel_path_config = config['paths'].get(images_key.replace('_data', '_images')) \n",
    "        if not images_rel_path_config: # Handle stage 1/2 vs test set conventions\n",
    "             images_rel_path_config = config['paths'].get(images_key) \n",
    "             if isinstance(images_rel_path_config, dict):\n",
    "                  images_rel_path = images_rel_path_config.get('images', '.') # Default to base if only annotation provided for stage\n",
    "             else:\n",
    "                  images_rel_path = images_rel_path_config or '.'\n",
    "        else:\n",
    "            images_rel_path = images_rel_path_config\n",
    "            \n",
    "    elif isinstance(dataset_config, dict): # e.g., vqav2_test: {annotations: ..., images: ...}\n",
    "        json_rel_path = dataset_config.get('annotations')\n",
    "        images_rel_path = dataset_config.get('images')\n",
    "        if json_rel_path is None or images_rel_path is None:\n",
    "             raise ValueError(f\"Configuration for dataset '{stage}' must contain 'annotations' and 'images' keys.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid configuration format for dataset '{stage}' in config paths.\")\n",
    "\n",
    "    json_path = data_base_path / json_rel_path\n",
    "    base_image_folder = data_base_path / images_rel_path\n",
    "\n",
    "    print(f\"Loading {dataset_type} '{stage}' items from: {json_path}\")\n",
    "    print(f\"Assuming image paths relative to: {base_image_folder}\")\n",
    "\n",
    "    if not json_path.exists():\n",
    "        raise FileNotFoundError(f\"{dataset_type} '{stage}' JSON/JSONL file not found: {json_path}\")\n",
    "\n",
    "    # Use the JSONL parsing function\n",
    "    samples = parse_llava_jsonl(json_path, base_image_folder)\n",
    "    print(f\"Found {len(samples)} samples for {dataset_type} '{stage}'.\")\n",
    "    return samples\n",
    "\n",
    "def get_image_path(sample: LLaVASample) -> Path:\n",
    "    \"\"\"Extracts and potentially resolves the image path from a LLaVASample.\"\"\"\n",
    "    # Resolve the path relative to the stored image_folder\n",
    "    if sample.image_folder and not sample.image_path.is_absolute():\n",
    "        resolved_path = sample.image_folder / sample.image_path\n",
    "        # Add check if file exists for debugging\n",
    "        if not resolved_path.exists():\n",
    "            # Try resolving relative to the config's data_base path as a fallback\n",
    "            try:\n",
    "                _cfg = load_config(project_root / 'configs' / 'config.yaml')\n",
    "                db_path = Path(_cfg['paths']['data_base'])\n",
    "                alt_path = db_path / sample.image_path\n",
    "                if alt_path.exists():\n",
    "                    return alt_path\n",
    "                else:\n",
    "                     print(f\"Warning: Image file not found at resolved path: {resolved_path} or alternative {alt_path}\")\n",
    "            except Exception:\n",
    "                 print(f\"Warning: Image file not found at resolved path: {resolved_path}\")\n",
    "            # Return the best guess path even if not found, error handled by ImageBlock later\n",
    "            return resolved_path\n",
    "        return resolved_path\n",
    "    return sample.image_path\n",
    "\n",
    "def get_conversations(sample: LLaVASample) -> list:\n",
    "    \"\"\"Extracts the conversations from a LLaVASample.\"\"\"\n",
    "    return sample.conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1 template assistant role tokens: [1792, 29889]\n",
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0, Template: plain\n",
      "LLaVADataBlockStage1 defined.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Define the DataBlock for Stage 1 (Projector Pre-training)\n",
    "LLaVADataBlockStage1 = None\n",
    "if tokenizer and 'LLaVABatchTransform' in globals() and 'LLaVATextTokenizer' in globals():\n",
    "    # Instantiate the batch transform for stage 1 ('plain' template)\n",
    "    llava_batch_tfm_stage1 = LLaVABatchTransform(tokenizer=tokenizer, \n",
    "                                                 normalize_tfm=clip_normalize,\n",
    "                                                 template='plain')\n",
    "\n",
    "    LLaVADataBlockStage1 = DataBlock(\n",
    "        blocks=(ImageBlock(cls=PILImage), TransformBlock), # Output of get_y is list (handled by TransformBlock)\n",
    "        get_items=partial(get_llava_items, stage=1),\n",
    "        get_x=get_image_path,\n",
    "        get_y=get_conversations,\n",
    "        splitter=RandomSplitter(valid_pct=0.01, seed=42), # Example split\n",
    "        item_tfms=[\n",
    "            *basic_image_item_tfms,\n",
    "            # Apply text tokenization using the 'plain' formatter\n",
    "            LLaVATextTokenizer(tokenizer, template_formatter=format_plain_template)\n",
    "        ],\n",
    "        batch_tfms=[\n",
    "            # Apply batch transformations including masking for 'plain' template\n",
    "            llava_batch_tfm_stage1\n",
    "        ]\n",
    "    )\n",
    "    print(\"LLaVADataBlockStage1 defined.\")\n",
    "else:\n",
    "    LLaVADataBlockStage1 = None\n",
    "    print(\"Tokenizer, LLaVABatchTransform, or LLaVATextTokenizer not available, LLaVADataBlockStage1 not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6: Create DataLoaders (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how to create the `DataLoaders` object from the `DataBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stage1_dataloaders(config: dict, dblock: DataBlock = LLaVADataBlockStage1) -> DataLoaders:\n",
    "    \"\"\"Creates fastai DataLoaders for Stage 1 training.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "        dblock: The configured DataBlock for Stage 1 (defaults to LLaVADataBlockStage1).\n",
    "\n",
    "    Returns:\n",
    "        A fastai DataLoaders object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the DataBlock is not defined.\n",
    "        FileNotFoundError: If data paths are invalid during DataBlock processing.\n",
    "    \"\"\"\n",
    "    if dblock is None:\n",
    "        raise ValueError(\"Stage 1 DataBlock is not defined. Ensure dependencies are available.\")\n",
    "\n",
    "    batch_size = config.get('data', {}).get('batch_size_per_device_stage1', 8)\n",
    "    num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "\n",
    "    print(f\"Creating Stage 1 DataLoaders with batch size: {batch_size}, num_workers: {num_workers}\")\n",
    "\n",
    "    # The DataBlock's get_items function needs the config,\n",
    "    # we pass it here when calling dataloaders()\n",
    "    try:\n",
    "        # Pass config explicitly to dataloaders, which passes it down to get_items\n",
    "        dls = dblock.dataloaders(source=config, # Pass config to be used by get_items\n",
    "                                 bs=batch_size,\n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=(torch.cuda.is_available())) # Pin memory if using GPU\n",
    "        print(\"DataLoaders created successfully.\")\n",
    "        return dls\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error creating DataLoaders: {e}\")\n",
    "        print(\"Please ensure data paths in config.yaml are correct and data exists.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An unexpected error occurred during DataLoaders creation: {e}\")\n",
    "        traceback.print_exc() # Print full traceback for debugging\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Update Data Handling for Stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the function to create DataLoaders specifically for Stage 2 instruction tuning, using the Vicuna v1 chat template and corresponding label masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1 template assistant role tokens: [1792, 29889]\n",
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0, Template: v1\n",
      "LLaVADataBlockStage2 defined.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Define the DataBlock for Stage 2 (Instruction Tuning)\n",
    "LLaVADataBlockStage2 = None\n",
    "if tokenizer and 'LLaVABatchTransform' in globals() and 'LLaVATextTokenizer' in globals() and 'format_v1_template' in globals():\n",
    "    # Instantiate the batch transform for stage 2 ('v1' template)\n",
    "    llava_batch_tfm_stage2 = LLaVABatchTransform(tokenizer=tokenizer, \n",
    "                                                 normalize_tfm=clip_normalize,\n",
    "                                                 template='v1')\n",
    "    \n",
    "    LLaVADataBlockStage2 = DataBlock(\n",
    "        blocks=(ImageBlock(cls=PILImage), TransformBlock),\n",
    "        # Use stage=2 for get_items\n",
    "        get_items=partial(get_llava_items, stage=2),\n",
    "        get_x=get_image_path,\n",
    "        get_y=get_conversations,\n",
    "        splitter=RandomSplitter(valid_pct=0.01, seed=42), # Adjust split as needed\n",
    "        item_tfms=[\n",
    "            *basic_image_item_tfms,\n",
    "            # Apply text tokenization using the 'v1' formatter\n",
    "            LLaVATextTokenizer(tokenizer, template_formatter=format_v1_template)\n",
    "        ],\n",
    "        batch_tfms=[\n",
    "            # Apply batch transformations including masking for 'v1' template\n",
    "            llava_batch_tfm_stage2\n",
    "        ]\n",
    "    )\n",
    "    print(\"LLaVADataBlockStage2 defined.\")\n",
    "else:\n",
    "    print(\"Dependencies missing, LLaVADataBlockStage2 not defined.\")\n",
    "\n",
    "# Function to create Stage 2 DataLoaders\n",
    "def get_stage2_dataloaders(config: dict, dblock: DataBlock = LLaVADataBlockStage2) -> DataLoaders:\n",
    "    \"\"\"Creates fastai DataLoaders for Stage 2 training (Instruction Tuning).\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "        dblock: The configured DataBlock for Stage 2 (defaults to LLaVADataBlockStage2).\n",
    "\n",
    "    Returns:\n",
    "        A fastai DataLoaders object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the DataBlock is not defined.\n",
    "        FileNotFoundError: If data paths are invalid during DataBlock processing.\n",
    "    \"\"\"\n",
    "    if dblock is None:\n",
    "        raise ValueError(\"Stage 2 DataBlock is not defined. Ensure dependencies are available.\")\n",
    "\n",
    "    batch_size = config.get('data', {}).get('batch_size_per_device_stage2', 4) # Use stage2 batch size\n",
    "    num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "\n",
    "    print(f\"Creating Stage 2 DataLoaders with batch size: {batch_size}, num_workers: {num_workers}\")\n",
    "\n",
    "    try:\n",
    "        # Pass config to dataloaders to be used by get_items\n",
    "        dls = dblock.dataloaders(source=config, \n",
    "                                 bs=batch_size,\n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=(torch.cuda.is_available()))\n",
    "        print(\"Stage 2 DataLoaders created successfully.\")\n",
    "        return dls\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error creating Stage 2 DataLoaders: {e}\")\n",
    "        print(\"Please ensure Stage 2 data paths in config.yaml are correct and data exists.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An unexpected error occurred during Stage 2 DataLoaders creation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Stage 2 DataLoaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Creating dummy Stage 2 JSONL: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "Creating Stage 2 DataLoaders with batch size: 4, num_workers: 4\n",
      "Loading Stage 2 items from: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "Assuming image paths relative to: /workspace/llava/data\n",
      "Found 2 samples for Stage 2.\n",
      "Stage 2 DataLoaders created successfully.\n",
      "\n",
      "Testing one_batch for Stage 2 DataLoaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/fastai/data/load.py:148: UserWarning: Your `DataLoader`'s `num_workers` is `4` but `batch_size` is small (`2`). Toggling `num_workers=0` for debug purposes.\n",
      "You can ignore this warning if you are not debugging. Initial `num_workers` is restored in `__del__`\n",
      "  if self.num_workers > 0 and warn:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_batch() retrieved. Check shapes and content.\n",
      "Batch Keys: dict_keys(['pixel_values', 'input_ids', 'attention_mask', 'labels'])\n",
      "pixel_values shape: torch.Size([2, 3, 336, 336])\n",
      "input_ids shape: torch.Size([2, 68])\n",
      "attention_mask shape: torch.Size([2, 68])\n",
      "labels shape: torch.Size([2, 68])\n",
      "\n",
      "--- Decoded Example from Batch --- \n",
      "Image Shape: torch.Size([3, 336, 336])\n",
      "Decoded Text: USER: <image> Describe image. ASSISTANT: It is a red object.\n",
      "--- End Decoded Example ---\n",
      "\n",
      "Stage 2 DataLoaders test passed (basic check).\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "try:\n",
    "    # Use relative path from nbs directory for testing\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # --- Test Setup: Create dummy Stage 2 data ---\n",
    "    data_base = Path(config['paths']['data_base'])\n",
    "    stage2_json_rel = Path(config['paths']['stage2_data'])\n",
    "    # Stage 2 images might reference paths from stage 1 or other datasets\n",
    "    stage1_img_rel = Path(config['paths']['stage1_images'])\n",
    "    stage1_img_path = data_base / stage1_img_rel\n",
    "\n",
    "    stage2_json_path = data_base / stage2_json_rel\n",
    "    stage2_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    stage1_img_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Assume dummy images from Stage 1 exist\n",
    "    # Use string paths relative to data_base for JSON content\n",
    "    img1_rel_path = stage1_img_rel.name + '/dummy_img1.jpg' \n",
    "    img2_rel_path = stage1_img_rel.name + '/dummy_img2.png'\n",
    "    if not (stage1_img_path / 'dummy_img1.jpg').exists():\n",
    "         PIL.Image.new('RGB', (60, 30), color = 'red').save(stage1_img_path / 'dummy_img1.jpg')\n",
    "    if not (stage1_img_path / 'dummy_img2.png').exists():\n",
    "         PIL.Image.new('RGB', (60, 30), color = 'green').save(stage1_img_path / 'dummy_img2.png')\n",
    "         \n",
    "    if not stage2_json_path.exists() or stage2_json_path.stat().st_size < 10:\n",
    "        print(f\"Creating dummy Stage 2 JSONL: {stage2_json_path}\")\n",
    "        dummy_jsonl_content = [\n",
    "            {\"id\": \"s2_001\", \"image\": str(img1_rel_path), \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nDescribe image.\"}, {\"from\": \"gpt\", \"value\": \"It is a red object.\"}]}, \n",
    "            {\"id\": \"s2_002\", \"image\": str(img2_rel_path), \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nIs it green?\"}, {\"from\": \"gpt\", \"value\": \"Yes, it appears green.\"}]},\n",
    "        ]\n",
    "        with open(stage2_json_path, 'w') as f:\n",
    "            for item in dummy_jsonl_content:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "    # --- End Test Setup ---\n",
    "\n",
    "    # Get Stage 2 DataLoaders\n",
    "    dls_stage2 = get_stage2_dataloaders(config)\n",
    "    \n",
    "    assert isinstance(dls_stage2, DataLoaders)\n",
    "    assert len(dls_stage2.train_ds) > 0\n",
    "    assert len(dls_stage2.valid_ds) > 0\n",
    "    \n",
    "    print(\"\\nTesting one_batch for Stage 2 DataLoaders...\")\n",
    "    b = dls_stage2.one_batch()\n",
    "    print(\"one_batch() retrieved. Check shapes and content.\")\n",
    "    assert isinstance(b, dict) # Batch transform should output dict\n",
    "    assert 'pixel_values' in b\n",
    "    assert 'input_ids' in b\n",
    "    assert 'attention_mask' in b\n",
    "    assert 'labels' in b\n",
    "    print(f\"Batch Keys: {b.keys()}\")\n",
    "    print(f\"pixel_values shape: {b['pixel_values'].shape}\")\n",
    "    print(f\"input_ids shape: {b['input_ids'].shape}\")\n",
    "    print(f\"attention_mask shape: {b['attention_mask'].shape}\")\n",
    "    print(f\"labels shape: {b['labels'].shape}\")\n",
    "\n",
    "    # Decode one example for visual inspection\n",
    "    print(\"\\n--- Decoded Example from Batch --- \")\n",
    "    # Re-create batch transform locally for decode if needed\n",
    "    temp_batch_tfm = LLaVABatchTransform(tokenizer, clip_normalize, template='v1')\n",
    "    decoded_batch = temp_batch_tfm.decode(b)\n",
    "    img_decoded, text_decoded = decoded_batch[0][0], decoded_batch[1][0]\n",
    "    print(f\"Image Shape: {img_decoded.shape}\")\n",
    "    print(f\"Decoded Text: {text_decoded}\")\n",
    "    print(\"--- End Decoded Example ---\")\n",
    "    \n",
    "    print(\"\\nStage 2 DataLoaders test passed (basic check).\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Skipping test: FileNotFoundError - {e}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred during Stage 2 DataLoaders test: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DataLoader Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates a DataLoader specifically for a test set, using Stage 2 preprocessing but without shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_test_dataloader(config: dict, dataset_name: str, dblock: DataBlock = LLaVADataBlockStage2) -> DataLoader:\n",
    "    \"\"\"Creates a fastai DataLoader for a specific test dataset.\n",
    "\n",
    "    Uses the Stage 2 DataBlock definition but configures it for the specified test set.\n",
    "    Disables shuffling and uses the full dataset (no splitter).\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "        dataset_name: The key corresponding to the test set in config['paths'] \n",
    "                      (e.g., 'vqav2_test', 'textvqa_val', 'custom_eval').\n",
    "        dblock: The configured DataBlock to use (defaults to LLaVADataBlockStage2).\n",
    "\n",
    "    Returns:\n",
    "        A fastai DataLoader object for the test set.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the DataBlock is not defined or the dataset_name is not found in config.\n",
    "        FileNotFoundError: If data paths are invalid during DataBlock processing.\n",
    "    \"\"\"\n",
    "    if dblock is None:\n",
    "        raise ValueError(\"Stage 2 DataBlock is not defined. Cannot create test dataloader.\")\n",
    "\n",
    "    if dataset_name not in config['paths']:\n",
    "         raise ValueError(f\"Test dataset '{dataset_name}' not found in config['paths'].\")\n",
    "\n",
    "    # Use evaluation batch size, fall back to stage 2 size if not specified\n",
    "    batch_size = config.get('evaluation', {}).get('eval_batch_size_per_device', \n",
    "                    config.get('data', {}).get('batch_size_per_device_stage2', 4))\n",
    "    num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "\n",
    "    print(f\"Creating Test DataLoader for '{dataset_name}' with batch size: {batch_size}, num_workers: {num_workers}\")\n",
    "\n",
    "    # Create a temporary copy of the datablock to modify for the test set\n",
    "    # Avoids modifying the original Stage 2 datablock\n",
    "    test_dblock = copy.deepcopy(dblock) \n",
    "    \n",
    "    # 1. Change get_items to point to the test dataset\n",
    "    test_dblock.get_items = partial(get_llava_items, stage=dataset_name)\n",
    "    # 2. Remove splitter to use the full dataset\n",
    "    test_dblock.splitter = None \n",
    "\n",
    "    try:\n",
    "        # Create datasets object first\n",
    "        datasets = test_dblock.datasets(source=config)\n",
    "        \n",
    "        if len(datasets) == 0:\n",
    "             print(f\"Warning: No items found for dataset '{dataset_name}'. DataLoader will be empty.\")\n",
    "             \n",
    "        # Create the test DataLoader (index 0 since there's no split)\n",
    "        # Ensure shuffle=False for evaluation\n",
    "        test_dl = DataLoader(datasets,\n",
    "                             bs=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             pin_memory=(torch.cuda.is_available()),\n",
    "                             shuffle=False, # Important for evaluation\n",
    "                             drop_last=False # Keep all samples\n",
    "                            )\n",
    "        \n",
    "        # Store items in dl for later retrieval if needed (e.g., for sample IDs)\n",
    "        # This relies on DataBlock populating `items` correctly\n",
    "        test_dl.items = datasets.items\n",
    "        test_dl.idxs = list(range(len(datasets))) # Store original indices\n",
    "        \n",
    "        print(f\"Test DataLoader for '{dataset_name}' created successfully ({len(datasets)} samples).\")\n",
    "        return test_dl\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error creating Test DataLoader for {dataset_name}: {e}\")\n",
    "        print(\"Please ensure test data paths in config.yaml are correct and data exists.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An unexpected error occurred during Test DataLoader creation for {dataset_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Custom Eval DataLoader Creation (Step 7.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Creating dummy custom_eval JSONL: /workspace/llava/data/custom_eval/annotations.jsonl\n",
      "Creating dummy custom_eval image: /workspace/llava/data/custom_eval/images/custom_img_1.jpg\n",
      "Creating dummy custom_eval image: /workspace/llava/data/custom_eval/images/custom_img_2.jpg\n",
      "Creating Test DataLoader for 'custom_eval' with batch size: 4, num_workers: 4\n",
      "Loading Dataset 'custom_eval' items from: /workspace/llava/data/custom_eval/annotations.jsonl\n",
      "Assuming image paths relative to: /workspace/llava/data/custom_eval/images\n",
      "Found 2 samples for Dataset 'custom_eval'.\n",
      "Test DataLoader for 'custom_eval' created successfully (2 samples).\n",
      "Custom Eval DataLoader test passed.\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "import shutil\n",
    "try:\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # --- Setup: Create dummy custom_eval data --- \n",
    "    data_base = Path(config['paths']['data_base'])\n",
    "    custom_eval_config = config['paths'].get('custom_eval')\n",
    "    if not custom_eval_config:\n",
    "         raise ValueError(\"Missing 'custom_eval' configuration in paths.\")\n",
    "         \n",
    "    custom_ann_rel = Path(custom_eval_config['annotations'])\n",
    "    custom_img_rel = Path(custom_eval_config['images'])\n",
    "    \n",
    "    custom_ann_path = data_base / custom_ann_rel\n",
    "    custom_img_path = data_base / custom_img_rel\n",
    "    \n",
    "    custom_ann_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    custom_img_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not custom_ann_path.exists() or custom_ann_path.stat().st_size < 10:\n",
    "        print(f\"Creating dummy custom_eval JSONL: {custom_ann_path}\")\n",
    "        dummy_content = [\n",
    "            {\"id\": \"cust_001\", \"image\": \"custom_img_1.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nWhat is the small text?\"}, {\"from\": \"gpt\", \"value\": \"Micro text.\"}]},\n",
    "            {\"id\": \"cust_002\", \"image\": \"custom_img_2.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nIdentify the object.\"}, {\"from\": \"gpt\", \"value\": \"A detailed widget.\"}]},\n",
    "        ]\n",
    "        with open(custom_ann_path, 'w') as f: [f.write(json.dumps(item) + '\\n') for item in dummy_content]\n",
    "    \n",
    "    img1_path = custom_img_path / 'custom_img_1.jpg'\n",
    "    img2_path = custom_img_path / 'custom_img_2.jpg'\n",
    "    if not img1_path.exists():\n",
    "        PIL.Image.new('RGB', (64, 64), color = 'blue').save(img1_path)\n",
    "        print(f\"Creating dummy custom_eval image: {img1_path}\")\n",
    "    if not img2_path.exists():\n",
    "        PIL.Image.new('RGB', (32, 32), color = 'yellow').save(img2_path)\n",
    "        print(f\"Creating dummy custom_eval image: {img2_path}\")\n",
    "    # --- End Setup --- \n",
    "    \n",
    "    # Get the custom eval dataloader\n",
    "    if LLaVADataBlockStage2 is None:\n",
    "         print(\"Warning: LLaVADataBlockStage2 not defined. Skipping custom eval dataloader test.\")\n",
    "    else:\n",
    "         dl_custom = get_test_dataloader(config, 'custom_eval', dblock=LLaVADataBlockStage2)\n",
    "         assert isinstance(dl_custom, DataLoader)\n",
    "         assert len(dl_custom.dataset) == 2 # Should match dummy data\n",
    "         assert dl_custom.bs > 0\n",
    "         print(\"Custom Eval DataLoader test passed.\")\n",
    "    \n",
    "    # Clean up dummy data (optional)\n",
    "    # shutil.rmtree(data_base / 'custom_eval', ignore_errors=True)\n",
    "\n",
    "except ValueError as e:\n",
    "     print(f\"Skipping test: Configuration missing or invalid - {e}\")\n",
    "except FileNotFoundError as e:\n",
    "     print(f\"Skipping test: FileNotFoundError - {e}\")\n",
    "except Exception as e:\n",
    "     import traceback\n",
    "     print(f\"An error occurred during Custom Eval DataLoader test: {e}\")\n",
    "     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
