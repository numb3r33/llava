{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "> Functions and classes for loading and parsing datasets for LLaVA-style training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Union\n",
    "from dataclasses import dataclass\n",
    "import PIL.Image\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.block import DataBlock, TransformBlock\n",
    "from fastai.data.transforms import parent_label, GrandparentSplitter # Example imports, adjust as needed\n",
    "\n",
    "from Adaptive_Patching_VIT_fastai.utils import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Data Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need functions to parse the JSONL files commonly used in LLaVA datasets. Each line typically contains:\n",
    "- `id`: A unique identifier for the sample.\n",
    "- `image`: The filename of the image (often relative to an `image_folder`).\n",
    "- `conversations`: A list of dictionaries, where each dictionary has `from` ('human' or 'gpt') and `value` (the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class LLaVASample:\n",
    "    \"\"\"Represents a single sample from a LLaVA-style dataset.\n",
    "\n",
    "    Attributes:\n",
    "        sample_id: Unique identifier for the sample.\n",
    "        image_path: Absolute path to the image file.\n",
    "        conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
    "        data_source: Optional field indicating the source dataset.\n",
    "    \"\"\"\n",
    "    sample_id: str\n",
    "    image_path: Path\n",
    "    conversations: List[Dict[str, str]]\n",
    "    data_source: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "#| export\n",
       "@dataclass\n",
       "class LLaVASample:\n",
       "    \"\"\"Represents a single sample from a LLaVA-style dataset.\n",
       "\n",
       "    Attributes:\n",
       "        sample_id: Unique identifier for the sample.\n",
       "        image_path: Absolute path to the image file.\n",
       "        conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
       "        data_source: Optional field indicating the source dataset.\n",
       "    \"\"\"\n",
       "    sample_id: str\n",
       "    image_path: Path\n",
       "    conversations: List[Dict[str, str]]\n",
       "    data_source: str | None = None\n",
       "```"
      ],
      "text/plain": [
       "<showdoc.show_doc at 0x7f7a407e20d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLaVASample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_llava_jsonl(jsonl_path: Union[str, Path], image_folder: Union[str, Path]) -> List[LLaVASample]:\n",
    "    \"\"\"Parses a LLaVA-style JSONL file and resolves image paths.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path: Path to the JSONL file.\n",
    "        image_folder: Path to the directory containing the images referenced in the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the JSONL file does not exist.\n",
    "        json.JSONDecodeError: If a line in the file is not valid JSON.\n",
    "    \"\"\"\n",
    "    jsonl_path = Path(jsonl_path)\n",
    "    image_folder = Path(image_folder)\n",
    "\n",
    "    if not jsonl_path.is_file():\n",
    "        raise FileNotFoundError(f\"JSONL file not found: {jsonl_path}\")\n",
    "\n",
    "    samples = []\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise json.JSONDecodeError(f\"Error decoding JSON on line {i+1} in {jsonl_path}: {e.msg}\", e.doc, e.pos)\n",
    "\n",
    "            # Check for required keys\n",
    "            if not all(k in data for k in ['id', 'image', 'conversations']):\n",
    "                print(f\"Warning: Skipping line {i+1} due to missing keys ('id', 'image', or 'conversations') in {jsonl_path}.\\n\",\n",
    "                      f\"Data: {data}\")\n",
    "                continue\n",
    "\n",
    "            sample_id = data['id']\n",
    "            # Construct the full image path. Assumes 'image' key holds a relative path or filename.\n",
    "            # Handle potential nested structure like {'bytes': ..., 'path': ...} if found in parquet conversions\n",
    "            image_ref = data['image']\n",
    "            if isinstance(image_ref, dict) and 'path' in image_ref:\n",
    "                # Handle cases where image info is nested (e.g., from parquet processing)\n",
    "                image_filename = image_ref['path']\n",
    "            elif isinstance(image_ref, str):\n",
    "                image_filename = image_ref\n",
    "            else:\n",
    "                 print(f\"Warning: Skipping line {i+1} due to unexpected image field format in {jsonl_path}.\\n\",\n",
    "                       f\"Expected string or dict with 'path', got: {type(image_ref)}\")\n",
    "                 continue\n",
    "            \n",
    "            # Ensure image_filename is treated as relative to image_folder\n",
    "            # Path(image_filename).name gets the final component if it was an absolute path by mistake\n",
    "            image_path = image_folder / Path(image_filename).name \n",
    "\n",
    "            conversations = data['conversations']\n",
    "            data_source = data.get('data_source') # Optional field\n",
    "\n",
    "            # Basic validation for conversations format\n",
    "            if not isinstance(conversations, list) or not all(isinstance(turn, dict) and 'from' in turn and 'value' in turn for turn in conversations):\n",
    "                 print(f\"Warning: Skipping line {i+1} due to invalid 'conversations' format in {jsonl_path}.\")\n",
    "                 continue\n",
    "            \n",
    "            # Check if image file actually exists (optional, can slow down parsing)\n",
    "            # if not image_path.is_file():\n",
    "            #     print(f\"Warning: Image file not found for sample {sample_id} at {image_path}, skipping.\")\n",
    "            #     continue\n",
    "\n",
    "            samples.append(LLaVASample(\n",
    "                sample_id=str(sample_id),\n",
    "                image_path=image_path,\n",
    "                conversations=conversations,\n",
    "                data_source=data_source\n",
    "            ))\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "#| export\n",
       "def parse_llava_jsonl(jsonl_path: str | Path, image_folder: str | Path) -> List[LLaVASample]:\n",
       "    \"\"\"Parses a LLaVA-style JSONL file and resolves image paths.\n",
       "\n",
       "    Args:\n",
       "        jsonl_path: Path to the JSONL file.\n",
       "        image_folder: Path to the directory containing the images referenced in the JSONL file.\n",
       "\n",
       "    Returns:\n",
       "        A list of LLaVASample objects.\n",
       "\n",
       "    Raises:\n",
       "        FileNotFoundError: If the JSONL file does not exist.\n",
       "        json.JSONDecodeError: If a line in the file is not valid JSON.\n",
       "    \"\"\"\n",
       "    jsonl_path = Path(jsonl_path)\n",
       "    image_folder = Path(image_folder)\n",
       "\n",
       "    if not jsonl_path.is_file():\n",
       "        raise FileNotFoundError(f\"JSONL file not found: {jsonl_path}\")\n",
       "\n",
       "    samples = []\n",
       "    with open(jsonl_path, 'r') as f:\n",
       "        for i, line in enumerate(f):\n",
       "            try:\n",
       "                data = json.loads(line)\n",
       "            except json.JSONDecodeError as e:\n",
       "                raise json.JSONDecodeError(f\"Error decoding JSON on line {i+1} in {jsonl_path}: {e.msg}\", e.doc, e.pos)\n",
       "\n",
       "            # Check for required keys\n",
       "            if not all(k in data for k in ['id', 'image', 'conversations']):\n",
       "                print(f\"Warning: Skipping line {i+1} due to missing keys ('id', 'image', or 'conversations') in {jsonl_path}.\\n\",\n",
       "                      f\"Data: {data}\")\n",
       "                continue\n",
       "\n",
       "            sample_id = data['id']\n",
       "            # Construct the full image path. Assumes 'image' key holds a relative path or filename.\n",
       "            # Handle potential nested structure like {'bytes': ..., 'path': ...} if found in parquet conversions\n",
       "            image_ref = data['image']\n",
       "            if isinstance(image_ref, dict) and 'path' in image_ref:\n",
       "                # Handle cases where image info is nested (e.g., from parquet processing)\n",
       "                image_filename = image_ref['path']\n",
       "            elif isinstance(image_ref, str):\n",
       "                image_filename = image_ref\n",
       "            else:\n",
       "                 print(f\"Warning: Skipping line {i+1} due to unexpected image field format in {jsonl_path}.\\n\",\n",
       "                       f\"Expected string or dict with 'path', got: {type(image_ref)}\")\n",
       "                 continue\n",
       "            \n",
       "            # Ensure image_filename is treated as relative to image_folder\n",
       "            # Path(image_filename).name gets the final component if it was an absolute path by mistake\n",
       "            image_path = image_folder / Path(image_filename).name \n",
       "\n",
       "            conversations = data['conversations']\n",
       "            data_source = data.get('data_source') # Optional field\n",
       "\n",
       "            # Basic validation for conversations format\n",
       "            if not isinstance(conversations, list) or not all(isinstance(turn, dict) and 'from' in turn and 'value' in turn for turn in conversations):\n",
       "                 print(f\"Warning: Skipping line {i+1} due to invalid 'conversations' format in {jsonl_path}.\")\n",
       "                 continue\n",
       "            \n",
       "            # Check if image file actually exists (optional, can slow down parsing)\n",
       "            # if not image_path.is_file():\n",
       "            #     print(f\"Warning: Image file not found for sample {sample_id} at {image_path}, skipping.\")\n",
       "            #     continue\n",
       "\n",
       "            samples.append(LLaVASample(\n",
       "                sample_id=str(sample_id),\n",
       "                image_path=image_path,\n",
       "                conversations=conversations,\n",
       "                data_source=data_source\n",
       "            ))\n",
       "\n",
       "    return samples\n",
       "```"
      ],
      "text/plain": [
       "<showdoc.show_doc at 0x7f7a406003a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(parse_llava_jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Create dummy data for testing\n",
    "dummy_data_dir = Path('./dummy_data')\n",
    "dummy_img_dir = dummy_data_dir / 'images'\n",
    "dummy_jsonl_path = dummy_data_dir / 'dummy_llava_data.jsonl'\n",
    "\n",
    "dummy_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create dummy image files\n",
    "try:\n",
    "    PIL.Image.new('RGB', (60, 30), color = 'red').save(dummy_img_dir / 'img1.jpg')\n",
    "    PIL.Image.new('RGB', (60, 30), color = 'green').save(dummy_img_dir / 'img2.png')\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not create dummy image files (PIL might not be fully installed or usable): {e}\")\n",
    "\n",
    "# Create dummy jsonl content\n",
    "dummy_jsonl_content = [\n",
    "    {\"id\": \"sample1\", \"image\": \"img1.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nDescribe this.\"}, {\"from\": \"gpt\", \"value\": \"It is red.\"}]}, \n",
    "    {\"id\": \"sample2\", \"image\": \"img2.png\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nWhat color?\"}, {\"from\": \"gpt\", \"value\": \"Green.\"}]},\n",
    "    {\"id\": \"sample3_missing_keys\", \"conversations\": []}, # Missing image/id\n",
    "    {\"id\": \"sample4_bad_conv\", \"image\": \"img1.jpg\", \"conversations\": \"not a list\"}, # Bad conversation format\n",
    "    {\"id\": \"sample5_missing_img_file\", \"image\": \"nonexistent.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\"}, {\"from\": \"gpt\", \"value\": \"...\"}]},\n",
    "]\n",
    "\n",
    "with open(dummy_jsonl_path, 'w') as f:\n",
    "    for item in dummy_jsonl_content:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Test parsing\n",
    "try:\n",
    "    parsed_samples = parse_llava_jsonl(dummy_jsonl_path, dummy_img_dir)\n",
    "    print(f\"Successfully parsed {len(parsed_samples)} samples:\")\n",
    "    for sample in parsed_samples:\n",
    "        print(sample)\n",
    "\n",
    "    # Basic checks\n",
    "    assert len(parsed_samples) == 2 # Only valid samples should be parsed\n",
    "    assert parsed_samples[0].sample_id == 'sample1'\n",
    "    # Resolve paths for comparison\n",
    "    assert parsed_samples[0].image_path.resolve() == (dummy_img_dir / 'img1.jpg').resolve()\n",
    "    assert parsed_samples[1].sample_id == 'sample2'\n",
    "    assert parsed_samples[1].image_path.resolve() == (dummy_img_dir / 'img2.png').resolve()\n",
    "    assert parsed_samples[0].conversations[0]['from'] == 'human'\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON Parsing Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Clean up dummy data (optional)\n",
    "# import shutil\n",
    "# shutil.rmtree(dummy_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Image Loading and Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This involves defining the `ImageBlock` and basic `item_tfms` for loading and resizing images. Normalization stats will be defined in `11_data_preprocessing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define the ImageBlock using PILImage for loading images\n",
    "image_block = ImageBlock(cls=PILImage)\n",
    "\n",
    "# Define basic item transforms for image processing\n",
    "# 1. Resize images to 336x336, padding if necessary\n",
    "#    method='pad': Pads the image to the target size.\n",
    "#    pad_mode='const': Uses a constant value for padding.\n",
    "#    pad_value=0: Uses black for padding (common for vision models).\n",
    "# 2. Convert the image to a PyTorch tensor.\n",
    "# Note: Normalization will be defined separately and likely applied in batch_tfms.\n",
    "basic_image_item_tfms = [\n",
    "    Resize(336, method='pad', pad_mode=PadMode.Constant, pad_value=0),\n",
    "    ToTensor(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Define Custom Dataset/DataBlock (Stage 1 - Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will define the fastai `DataBlock` or a custom `Datasets` class to integrate the parsing, image loading, and text processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for get_items function for DataBlock\n",
    "# def get_llava_items(config):\n",
    "#     # Load config, parse jsonl using parse_llava_jsonl\n",
    "#     # Return list of LLaVASample objects or similar structure\n",
    "#     pass\n",
    "\n",
    "# Placeholder for DataBlock definition\n",
    "# llava_datablock_stage1 = DataBlock(\n",
    "#     blocks=(image_block, TransformBlock), # TextBlock or custom block needed for text\n",
    "#     get_items=get_llava_items,\n",
    "#     get_x=lambda sample: sample.image_path, # Function to get image path from sample\n",
    "#     get_y=lambda sample: sample.conversations, # Function to get conversations from sample\n",
    "#     splitter=RandomSplitter(),\n",
    "#     item_tfms=basic_image_item_tfms + [YourTextTokenizerTransform], # Add text transforms later\n",
    "#     batch_tfms=[YourCustomBatchTransform] # Custom batch transform for padding, masking, normalization\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6: Create DataLoaders (Stage 1 - Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will show how to create the `DataLoaders` object from the `DataBlock` or custom `Datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for creating DataLoaders\n",
    "# def get_stage1_dataloaders(config):\n",
    "#     # Load config, define datablock/dataset\n",
    "#     # items = get_llava_items(config)\n",
    "#     # dls = llava_datablock_stage1.dataloaders(items, bs=config['data']['batch_size_per_device_stage1'])\n",
    "#     # return dls\n",
    "#     pass\n",
    "\n",
    "# Example: Test show_batch\n",
    "# config = load_config('configs/config.yaml') # Load config first\n",
    "# dls = get_stage1_dataloaders(config)\n",
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Update Data Handling for Stage 2 (Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will contain functions to create DataLoaders specifically for Stage 2 instruction tuning, using the appropriate chat template and label masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for get_stage2_dataloaders function\n",
    "# def get_stage2_dataloaders(config):\n",
    "#     # Similar to stage 1 but uses different template/masking\n",
    "#     # Define llava_datablock_stage2 or custom dataset logic\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.4: Implement Custom Evaluation Set Handling (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for loading custom eval set\n",
    "# def get_custom_eval_dataloaders(config):\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "metadata": {
   "path": "nbs/10_data_loading.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "Adaptive_Patching_VIT_fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}