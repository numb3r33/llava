{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "> Functions and classes for loading and parsing datasets for LLaVA-style training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Union, Tuple\n",
    "from dataclasses import dataclass\n",
    "import PIL.Image\n",
    "from functools import partial\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.block import DataBlock, TransformBlock, CategoryBlock # Added CategoryBlock just in case\n",
    "from fastai.data.transforms import parent_label, GrandparentSplitter, RandomSplitter, IntToFloatTensor\n",
    "\n",
    "from Adaptive_Patching_VIT_fastai.utils import load_config\n",
    "# Import necessary items from preprocessing notebook\n",
    "from Adaptive_Patching_VIT_fastai.data.preprocessing import (\n",
    "    tokenizer, \n",
    "    LLaVATextTokenizer, \n",
    "    clip_normalize, \n",
    "    format_plain_template, # Needed if not using LLaVATextTokenizer directly\n",
    "    LLaVABatchTransform # Import the custom batch transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Data Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need functions to parse the JSONL files commonly used in LLaVA datasets. Each line typically contains:\n",
    "- `id`: A unique identifier for the sample.\n",
    "- `image`: The filename of the image (often relative to an `image_folder`).\n",
    "- `conversations`: A list of dictionaries, where each dictionary has `from` ('human' or 'gpt') and `value` (the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class LLaVASample:\n",
    "    \"\"\"Represents a single sample from a LLaVA-style dataset.\n",
    "\n",
    "    Attributes:\n",
    "        sample_id: Unique identifier for the sample.\n",
    "        image_path: Absolute path to the image file.\n",
    "        conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
    "        data_source: Optional field indicating the source dataset.\n",
    "    \"\"\"\n",
    "    sample_id: str\n",
    "    image_path: Path\n",
    "    conversations: List[Dict[str, str]]\n",
    "    data_source: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "#| export\n",
       "@dataclass\n",
       "class LLaVASample:\n",
       "    \"\"\"Represents a single sample from a LLaVA-style dataset.\\n\\n    Attributes:\\n        sample_id: Unique identifier for the sample.\\n        image_path: Absolute path to the image file.\\n        conversations: List of conversation turns (dictionaries with 'from' and 'value').\\n        data_source: Optional field indicating the source dataset.\\n    \"\"\"\n",
       "    sample_id: str\n",
       "    image_path: Path\n",
       "    conversations: List[Dict[str, str]]\n",
       "    data_source: str | None = None\n",
       "```"
      ],
      "text/plain": [
       "<showdoc.show_doc at 0x7f1e42e4bb80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLaVASample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_llava_jsonl(jsonl_path: Union[str, Path], image_folder: Union[str, Path]) -> List[LLaVASample]:\n",
    "    \"\"\"Parses a LLaVA-style JSONL file and resolves image paths.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path: Path to the JSONL file.\n",
    "        image_folder: Path to the directory containing the images referenced in the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the JSONL file does not exist.\n",
    "        json.JSONDecodeError: If a line in the file is not valid JSON.\n",
    "    \"\"\"\n",
    "    jsonl_path = Path(jsonl_path)\n",
    "    image_folder = Path(image_folder)\n",
    "\n",
    "    if not jsonl_path.is_file():\n",
    "        raise FileNotFoundError(f\"JSONL file not found: {jsonl_path}\")\n",
    "\n",
    "    samples = []\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise json.JSONDecodeError(f\"Error decoding JSON on line {i+1} in {jsonl_path}: {e.msg}\", e.doc, e.pos)\n",
    "\n",
    "            # Check for required keys\n",
    "            if not all(k in data for k in ['id', 'image', 'conversations']):\n",
    "                print(f\"Warning: Skipping line {i+1} due to missing keys ('id', 'image', or 'conversations') in {jsonl_path}.\\n\",\n",
    "                      f\"Data: {data}\")\n",
    "                continue\n",
    "\n",
    "            sample_id = data['id']\n",
    "            # Construct the full image path. Assumes 'image' key holds a relative path or filename.\n",
    "            # Handle potential nested structure like {'bytes': ..., 'path': ...} if found in parquet conversions\n",
    "            image_ref = data['image']\n",
    "            if isinstance(image_ref, dict) and 'path' in image_ref:\n",
    "                # Handle cases where image info is nested (e.g., from parquet processing)\n",
    "                image_filename = image_ref['path']\n",
    "            elif isinstance(image_ref, str):\n",
    "                image_filename = image_ref\n",
    "            else:\n",
    "                 print(f\"Warning: Skipping line {i+1} due to unexpected image field format in {jsonl_path}.\\n\",\n",
    "                       f\"Expected string or dict with 'path', got: {type(image_ref)}\")\n",
    "                 continue\n",
    "\n",
    "            # Resolve the image path: assume image_filename is relative to image_folder\n",
    "            # Path.name is used to handle cases where an absolute path might be mistakenly stored\n",
    "            # but we want it relative to the designated image_folder.\n",
    "            image_path = image_folder / Path(image_filename).name\n",
    "\n",
    "            conversations = data['conversations']\n",
    "            data_source = data.get('data_source') # Optional field\n",
    "\n",
    "            # Basic validation for conversations format\n",
    "            if not isinstance(conversations, list) or not all(isinstance(turn, dict) and 'from' in turn and 'value' in turn for turn in conversations):\n",
    "                 print(f\"Warning: Skipping line {i+1} due to invalid 'conversations' format in {jsonl_path}.\")\n",
    "                 continue\n",
    "\n",
    "            # Check if image file actually exists (optional, can slow down parsing)\n",
    "            # if not image_path.is_file():\n",
    "            #     print(f\"Warning: Image file not found for sample {sample_id} at {image_path}, skipping.\")\n",
    "            #     continue\n",
    "\n",
    "            samples.append(LLaVASample(\n",
    "                sample_id=str(sample_id),\n",
    "                image_path=image_path,\n",
    "                conversations=conversations,\n",
    "                data_source=data_source\n",
    "            ))\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "#| export\n",
       "def parse_llava_jsonl(jsonl_path: str | Path, image_folder: str | Path) -> List[LLaVASample]:\n",
       "    \"\"\"Parses a LLaVA-style JSONL file and resolves image paths.\\n\\n    Args:\\n        jsonl_path: Path to the JSONL file.\\n        image_folder: Path to the directory containing the images referenced in the JSONL file.\\n\\n    Returns:\\n        A list of LLaVASample objects.\\n\\n    Raises:\\n        FileNotFoundError: If the JSONL file does not exist.\\n        json.JSONDecodeError: If a line in the file is not valid JSON.\\n    \"\"\"\n",
       "    jsonl_path = Path(jsonl_path)\n",
       "    image_folder = Path(image_folder)\n",
       "\n",
       "    if not jsonl_path.is_file():\n",
       "        raise FileNotFoundError(f\"JSONL file not found: {jsonl_path}\")\n",
       "\n",
       "    samples = []\n",
       "    with open(jsonl_path, 'r') as f:\n",
       "        for i, line in enumerate(f):\n",
       "            try:\n",
       "                data = json.loads(line)\n",
       "            except json.JSONDecodeError as e:\n",
       "                raise json.JSONDecodeError(f\"Error decoding JSON on line {i+1} in {jsonl_path}: {e.msg}\", e.doc, e.pos)\n",
       "\n",
       "            # Check for required keys\n",
       "            if not all(k in data for k in ['id', 'image', 'conversations']):\n",
       "                print(f\"Warning: Skipping line {i+1} due to missing keys ('id', 'image', or 'conversations') in {jsonl_path}.\\n\",\n",
       "                      f\"Data: {data}\")\n",
       "                continue\n",
       "\n",
       "            sample_id = data['id']\n",
       "            # Construct the full image path. Assumes 'image' key holds a relative path or filename.\n",
       "            # Handle potential nested structure like {'bytes': ..., 'path': ...} if found in parquet conversions\n",
       "            image_ref = data['image']\n",
       "            if isinstance(image_ref, dict) and 'path' in image_ref:\n",
       "                # Handle cases where image info is nested (e.g., from parquet processing)\n",
       "                image_filename = image_ref['path']\n",
       "            elif isinstance(image_ref, str):\n",
       "                image_filename = image_ref\n",
       "            else:\n",
       "                 print(f\"Warning: Skipping line {i+1} due to unexpected image field format in {jsonl_path}.\\n\",\n",
       "                       f\"Expected string or dict with 'path', got: {type(image_ref)}\")\n",
       "                 continue\n",
       "            \n",
       "            # Resolve the image path: assume image_filename is relative to image_folder\n",
       "            # Path.name is used to handle cases where an absolute path might be mistakenly stored\n",
       "            # but we want it relative to the designated image_folder.\n",
       "            image_path = image_folder / Path(image_filename).name \n",
       "\n",
       "            conversations = data['conversations']\n",
       "            data_source = data.get('data_source') # Optional field\n",
       "\n",
       "            # Basic validation for conversations format\n",
       "            if not isinstance(conversations, list) or not all(isinstance(turn, dict) and 'from' in turn and 'value' in turn for turn in conversations):\n",
       "                 print(f\"Warning: Skipping line {i+1} due to invalid 'conversations' format in {jsonl_path}.\")\n",
       "                 continue\n",
       "            \n",
       "            # Check if image file actually exists (optional, can slow down parsing)\n",
       "            # if not image_path.is_file():\n",
       "            #     print(f\"Warning: Image file not found for sample {sample_id} at {image_path}, skipping.\")\n",
       "            #     continue\n",
       "\n",
       "            samples.append(LLaVASample(\n",
       "                sample_id=str(sample_id),\n",
       "                image_path=image_path,\n",
       "                conversations=conversations,\n",
       "                data_source=data_source\n",
       "            ))\n",
       "\n",
       "    return samples\n",
       "```"
      ],
      "text/plain": [
       "<showdoc.show_doc at 0x7f1e42e4b280>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(parse_llava_jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy directory: dummy_data/images\n",
      "Note: Could not create dummy image files (PIL might not be fully installed or usable): name 'Image' is not defined\n",
      "Created dummy jsonl file: dummy_data/dummy_llava_data.jsonl\n",
      "Warning: Skipping line 3 due to missing keys ('id', 'image', or 'conversations') in dummy_data/dummy_llava_data.jsonl.\n",
      "Data: {'id': 'sample3_missing_keys', 'conversations': []}\n",
      "Warning: Skipping line 4 due to invalid 'conversations' format in dummy_data/dummy_llava_data.jsonl.\n",
      "Successfully parsed 3 samples:\n",
      "LLaVASample(sample_id='sample1', image_path=PosixPath('dummy_data/images/img1.jpg'), conversations=[{'from': 'human', 'value': '<image>\\nDescribe this.'}, {'from': 'gpt', 'value': 'It is red.'}], data_source=None)\n",
      "LLaVASample(sample_id='sample2', image_path=PosixPath('dummy_data/images/img2.png'), conversations=[{'from': 'human', 'value': '<image>\\nWhat color?'}, {'from': 'gpt', 'value': 'Green.'}], data_source=None)\n",
      "LLaVASample(sample_id='sample5_missing_img_file', image_path=PosixPath('dummy_data/images/nonexistent.jpg'), conversations=[{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': '...'}], data_source=None)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Create dummy data for testing\n",
    "dummy_data_dir = Path('./dummy_data')\n",
    "dummy_img_dir = dummy_data_dir / 'images'\n",
    "dummy_jsonl_path = dummy_data_dir / 'dummy_llava_data.jsonl'\n",
    "\n",
    "dummy_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created dummy directory: {dummy_img_dir}\")\n",
    "\n",
    "# Create dummy image files (attempt, may fail if PIL issues persist)\n",
    "try:\n",
    "    PIL.Image.new('RGB', (60, 30), color = 'red').save(dummy_img_dir / 'img1.jpg')\n",
    "    PIL.Image.new('RGB', (60, 30), color = 'green').save(dummy_img_dir / 'img2.png')\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not create dummy image files (PIL might not be fully installed or usable): {e}\")\n",
    "\n",
    "# Create dummy jsonl content\n",
    "dummy_jsonl_content = [\n",
    "    {\"id\": \"sample1\", \"image\": \"img1.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nDescribe this.\"}, {\"from\": \"gpt\", \"value\": \"It is red.\"}]}, \n",
    "    {\"id\": \"sample2\", \"image\": \"img2.png\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nWhat color?\"}, {\"from\": \"gpt\", \"value\": \"Green.\"}]},\n",
    "    {\"id\": \"sample3_missing_keys\", \"conversations\": []}, # Missing image/id\n",
    "    {\"id\": \"sample4_bad_conv\", \"image\": \"img1.jpg\", \"conversations\": \"not a list\"}, # Bad conversation format\n",
    "    {\"id\": \"sample5_missing_img_file\", \"image\": \"nonexistent.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\"}, {\"from\": \"gpt\", \"value\": \"...\"}]},\n",
    "]\n",
    "\n",
    "with open(dummy_jsonl_path, 'w') as f:\n",
    "    for item in dummy_jsonl_content:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "print(f\"Created dummy jsonl file: {dummy_jsonl_path}\")\n",
    "\n",
    "# Test parsing\n",
    "try:\n",
    "    parsed_samples = parse_llava_jsonl(dummy_jsonl_path, dummy_img_dir)\n",
    "    print(f\"Successfully parsed {len(parsed_samples)} samples:\")\n",
    "    for sample in parsed_samples:\n",
    "        print(sample)\n",
    "\n",
    "    # Basic checks (adjust expected length based on warnings/skips)\n",
    "    # Check based on the print output, 3 samples are expected now (1, 2, 5)\n",
    "    assert len(parsed_samples) == 3 \n",
    "    assert parsed_samples[0].sample_id == 'sample1'\n",
    "    # Resolve paths for comparison\n",
    "    assert parsed_samples[0].image_path.resolve() == (dummy_img_dir / 'img1.jpg').resolve()\n",
    "    assert parsed_samples[1].sample_id == 'sample2'\n",
    "    assert parsed_samples[1].image_path.resolve() == (dummy_img_dir / 'img2.png').resolve()\n",
    "    assert parsed_samples[0].conversations[0]['from'] == 'human'\n",
    "    assert parsed_samples[2].sample_id == 'sample5_missing_img_file'\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON Parsing Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Clean up dummy data (optional)\n",
    "# import shutil\n",
    "# if dummy_data_dir.exists():\n",
    "#    shutil.rmtree(dummy_data_dir)\n",
    "#    print(f\"Cleaned up dummy data directory: {dummy_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Image Loading and Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This involves defining the `ImageBlock` and basic `item_tfms` for loading and resizing images. Normalization stats were defined in `11_data_preprocessing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define the ImageBlock using PILImage for loading images\n",
    "image_block = ImageBlock(cls=PILImage)\n",
    "\n",
    "# Define basic item transforms for image processing\n",
    "# 1. Resize images to 336x336, padding if necessary\n",
    "#    method='pad': Pads the image to the target size.\n",
    "#    pad_mode='const': Uses a constant value for padding.\n",
    "#    pad_value=0: Uses black for padding (common for vision models).\n",
    "# 2. Convert the image to a PyTorch tensor.\n",
    "# Note: Normalization will be applied in batch_tfms using clip_normalize.\n",
    "basic_image_item_tfms = [\n",
    "    Resize(336, method='pad', pad_mode=PadMode.Constant, pad_value=0),\n",
    "    ToTensor(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Define Custom Dataset/DataBlock (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the fastai `DataBlock` for Stage 1 projector pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_llava_items(config: dict, stage: int = 1) -> List[LLaVASample]:\n",
    "    \"\"\"Loads LLaVA samples for a specific stage based on config.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "        stage: The training stage (1 or 2).\n",
    "\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If stage is not 1 or 2.\n",
    "        KeyError: If required path configurations are missing.\n",
    "    \"\"\"\n",
    "    data_base_path = Path(config['paths']['data_base'])\n",
    "    if stage == 1:\n",
    "        jsonl_rel_path = config['paths']['stage1_data']\n",
    "        images_rel_path = config['paths']['stage1_images']\n",
    "    elif stage == 2:\n",
    "        jsonl_rel_path = config['paths']['stage2_data']\n",
    "        # Stage 2 images might be relative to base path or absolute paths might be in JSONL\n",
    "        images_rel_path = config['paths'].get('stage2_images', '.') # Assume relative to base if specified\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid stage specified: {stage}. Must be 1 or 2.\")\n",
    "\n",
    "    jsonl_path = data_base_path / jsonl_rel_path\n",
    "    image_folder = data_base_path / images_rel_path\n",
    "\n",
    "    print(f\"Loading Stage {stage} items from: {jsonl_path}\")\n",
    "    print(f\"Assuming images relative to: {image_folder}\")\n",
    "\n",
    "    # Check if paths exist\n",
    "    if not jsonl_path.exists():\n",
    "        raise FileNotFoundError(f\"Stage {stage} JSONL file not found: {jsonl_path}\")\n",
    "    if not image_folder.is_dir() and stage == 1: # Only strictly check image folder for stage 1\n",
    "        print(f\"Warning: Stage {stage} image folder not found or not a directory: {image_folder}\")\n",
    "        # Proceed cautiously, parsing might still work if JSONL contains absolute paths\n",
    "        # or if image existence check is disabled in parse_llava_jsonl\n",
    "\n",
    "    samples = parse_llava_jsonl(jsonl_path, image_folder)\n",
    "    print(f\"Found {len(samples)} samples for Stage {stage}.\")\n",
    "    return samples\n",
    "\n",
    "def get_image_path(sample: LLaVASample) -> Path:\n",
    "    \"\"\"Extracts the image path from a LLaVASample.\"\"\"\n",
    "    return sample.image_path\n",
    "\n",
    "def get_conversations(sample: LLaVASample) -> List[Dict[str, str]]:\n",
    "    \"\"\"Extracts the conversations from a LLaVASample.\"\"\"\n",
    "    return sample.conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 2\n",
      "LLaVADataBlockStage1 defined.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Define the DataBlock for Stage 1 (Projector Pre-training)\n",
    "if tokenizer:\n",
    "    # Instantiate the custom batch transform (needs tokenizer)\n",
    "    llava_batch_tfm = LLaVABatchTransform(tokenizer=tokenizer)\n",
    "    \n",
    "    LLaVADataBlockStage1 = DataBlock(\n",
    "        blocks=(ImageBlock(cls=PILImage), TransformBlock), # Input: Image, Target: Processed Conversations (token IDs)\n",
    "        get_items=partial(get_llava_items, stage=1), # Use partial to pass stage=1 to the item getter\n",
    "        get_x=get_image_path, # Function to get image path from sample\n",
    "        get_y=get_conversations, # Function to get conversations for text processing\n",
    "        splitter=RandomSplitter(valid_pct=0.01, seed=42), # Small validation set for monitoring\n",
    "        item_tfms=[\n",
    "            *basic_image_item_tfms, # Apply resize/ToTensor to images (x)\n",
    "            LLaVATextTokenizer(tokenizer, template_formatter=format_plain_template) # Apply template+tokenization to conversations (y)\n",
    "        ],\n",
    "        # batch_tfms are applied after items are collated into a batch\n",
    "        batch_tfms=[ \n",
    "            IntToFloatTensor(div_mask=torch.BoolTensor([True,False])), # Convert image tensor to float (but not text IDs)\n",
    "            clip_normalize,   # Apply normalization (to image tensor)\n",
    "            llava_batch_tfm   # Apply custom batch padding, masking, etc.\n",
    "        ]\n",
    "    )\n",
    "    print(\"LLaVADataBlockStage1 defined.\")\n",
    "else:\n",
    "    LLaVADataBlockStage1 = None\n",
    "    print(\"Tokenizer not available, LLaVADataBlockStage1 not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (DataBlock Definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting DataBlock summary...\n",
      "DataBlock Summary:\n",
      "  Setting up Pipeline: partial -> get_image_path\n",
      "  Setting up Pipeline: partial -> get_conversations -> LLaVATextTokenizer\n",
      "  \n",
      "  Building validation fold (1%)\n",
      "    Setting up Pipeline: partial -> get_image_path\n",
      "    Setting up Pipeline: partial -> get_conversations -> LLaVATextTokenizer\n",
      "    \n",
      "\n",
      "Skipping summary: FileNotFoundError: [Errno 2] No such file or directory: '/path/to/your/datasets/llava_pretrain/llava_pretrain.jsonl'\n",
      "Please ensure 'paths.data_base', 'paths.stage1_data', and 'paths.stage1_images' are correctly set in configs/config.yaml and point to existing data.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "if LLaVADataBlockStage1:\n",
    "    try:\n",
    "        # Load config to pass to get_items via the DataBlock\n",
    "        config = load_config('configs/config.yaml') \n",
    "        print(\"Attempting DataBlock summary...\")\n",
    "        # This will call get_items(config) internally\n",
    "        # It requires valid paths in config.yaml and actual data to succeed fully.\n",
    "        LLaVADataBlockStage1.summary(config=config, bs=4)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nSkipping summary: FileNotFoundError: {e}\")\n",
    "        print(\"Please ensure 'paths.data_base', 'paths.stage1_data', and 'paths.stage1_images' are correctly set in configs/config.yaml and point to existing data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nSkipping summary: Exception occurred during gathering samples: {e}\")\n",
    "        print(\"Check paths in config.yaml and ensure data files are accessible and correctly formatted.\")\n",
    "else:\n",
    "    print(\"LLaVADataBlockStage1 not defined, cannot show summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6: Create DataLoaders (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how to create the `DataLoaders` object from the `DataBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stage1_dataloaders(config: dict, dblock: DataBlock = LLaVADataBlockStage1) -> DataLoaders:\n",
    "    \"\"\"Creates fastai DataLoaders for Stage 1 training.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "        dblock: The configured DataBlock for Stage 1 (defaults to LLaVADataBlockStage1).\n",
    "\n",
    "    Returns:\n",
    "        A fastai DataLoaders object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the DataBlock is not defined.\n",
    "        FileNotFoundError: If data paths are invalid during DataBlock processing.\n",
    "    \"\"\"\n",
    "    if dblock is None:\n",
    "        raise ValueError(\"Stage 1 DataBlock is not defined. Ensure tokenizer loaded correctly.\")\n",
    "\n",
    "    batch_size = config.get('data', {}).get('batch_size_per_device_stage1', 8)\n",
    "    num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "\n",
    "    print(f\"Creating Stage 1 DataLoaders with batch size: {batch_size}, num_workers: {num_workers}\")\n",
    "\n",
    "    # The DataBlock's get_items function needs the config,\n",
    "    # we pass it here when calling dataloaders()\n",
    "    try:\n",
    "        # Pass config explicitly to dataloaders, which passes it down to get_items\n",
    "        dls = dblock.dataloaders(source=None, # Source is determined by get_items\n",
    "                                 config=config, # Pass config to be used by get_items\n",
    "                                 bs=batch_size,\n",
    "                                 num_workers=num_workers)\n",
    "        print(\"DataLoaders created successfully.\")\n",
    "        return dls\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error creating DataLoaders: {e}\")\n",
    "        print(\"Please ensure data paths in config.yaml are correct and data exists.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An unexpected error occurred during DataLoaders creation: {e}\")\n",
    "        traceback.print_exc() # Print full traceback for debugging\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from configs/config.yaml\n",
      "Creating Stage 1 DataLoaders with batch size: 8, num_workers: 4\n",
      "Loading Stage 1 items from: /path/to/your/datasets/llava_pretrain/llava_pretrain.jsonl\n",
      "Assuming images relative to: /path/to/your/datasets/llava_pretrain/images\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/your/datasets/llava_pretrain/llava_pretrain.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------",
      "FileNotFoundError                         Traceback (most recent call last)",
      "Cell In[12], line 5",
      "      2 try:",
      "      3     config_path = 'configs/config.yaml'",
      "      4     config = load_config(config_path)",
      "      5     print(f\"Loaded config from {config_path}\")",
      "----> 6     dls = get_stage1_dataloaders(config)",
      "      8     print(f\"DataLoaders created. Testing show_batch...\")",
      "      9     # Now that batch transforms are complete, show_batch should work if data exists",
      "     10     # Ensure you have data available at the paths specified in config.yaml",
      "     11     # dls.show_batch(max_n=4, figsize=(12, 8))",
      "     12     # print(\"\\nTesting one_batch...\")",
      "     13     # b = dls.one_batch()",
      "     14     # print(\"one_batch() retrieved. Check keys and shapes.\")",
      "     15     # # Example: print shapes of tensors in the batch",
      "     16     # for k, v in b.items():",
      "     17     #     if isinstance(v, torch.Tensor):",
      "     18     #         print(f\"  {k}: {v.shape}\")",
      "     19     #     else:",
      "     20     #         print(f\"  {k}: {type(v)}\")",
      "     22     print(\"\\nNote: Full testing requires valid data paths and files.\")",
      "     24 except FileNotFoundError as e:",
      "",
      "Cell In[11], line 25, in get_stage1_dataloaders(config, dblock)",
      "     22 # The DataBlock's get_items function needs the config,",
      "     23 # we pass it here when calling dataloaders()",
      "     24 try:",
      "     25     # Pass config explicitly to dataloaders, which passes it down to get_items",
      "---> 26     dls = dblock.dataloaders(source=None, # Source is determined by get_items",
      "     27                              config=config, # Pass config to be used by get_items",
      "     28                              bs=batch_size,",
      "     29                              num_workers=num_workers)",
      "     30     print(\"DataLoaders created successfully.\")",
      "     31     return dls",
      "     32 except FileNotFoundError as e:",
      "",
      "File /opt/conda/envs/adaptive-patching/lib/python3.10/site-packages/fastai/data/block.py:158, in DataBlock.dataloaders(self, source, path, verbose, bs, num_workers, pin_memory, timeout, batch_size, shuffle, drop_last, shuffle_train, n, device, dl_type, dl_kwargs, **kwargs)",
      "    155 if source is not None: self.source = source",
      "    156 self.bs = batch_size or bs",
      "    157 dsets = self.datasets(source, verbose=verbose, **kwargs)",
      "--> 158 dls = self._dls(dsets, path, True, batch_size, num_workers, pin_memory, timeout,",
      "    159                 shuffle, drop_last, shuffle_train, n, device, dl_type, dl_kwargs)",
      "    160 self._dls_args = (path, True, batch_size, num_workers, pin_memory, timeout,",
      "    161                    shuffle, drop_last, shuffle_train, n, device, dl_type, dl_kwargs)",
      "    162 return dls",
      "",
      "File /opt/conda/envs/adaptive-patching/lib/python3.10/site-packages/fastai/data/block.py:147, in DataBlock.datasets(self, source, verbose, **kwargs)",
      "    145 self.source = source",
      "    146 if self._items is None: self._items = self.get_items(source, **kwargs)",
      "--> 147 splits = self.splitter(self._items)",
      "    148 pv(f'Splitting items into {splits}', verbose)",
      "    149 if not self.n_inp:",
      "    150     # Check that the first item of the datasets is consistent with `n_inp`",
      "    151     test_ds = self.do_item_tfms(self._items[0], None)",
      "    152     self.n_inp = 1 if not isinstance(test_ds, (tuple, list)) or len(test_ds)==1 else len(test_ds)-1",
      "",
      "File /opt/conda/envs/adaptive-patching/lib/python3.10/site-packages/fastcore/basics.py:486, in ReindexSplitter.__call__(self, items)",
      "    484 def __call__(self, items):",
      "    485     if self.splits is None: self.splits = self.split(items)",
      "--> 486     return self.splits",
      "",
      "File /opt/conda/envs/adaptive-patching/lib/python3.10/site-packages/fastcore/basics.py:479, in RandomSplitter.split(self, items)",
      "    477 \"Split `items` between train/val with `valid_pct` randomly.\"",
      "    478 start = self.seed",
      "--> 479 idxs = np.arange(len(items))",
      "    480 if start is not None: np.random.seed(self.seed)",
      "    481 cut = int(self.valid_pct * len(items))",
      "",
      "File /opt/conda/envs/adaptive-patching/lib/python3.10/site-packages/fastai/data/block.py:121, in DataBlock.get_items(self, source, **kwargs)",
      "    120 \"Pass `source` to `get_items` unless it's None, then pass `self.source`\"",
      "--> 121 return self._get_items(self.source if source is None else source, **kwargs)",
      "",
      "Cell In[8], line 27, in get_llava_items(config, stage)",
      "     25     # or if image existence check is disabled in parse_llava_jsonl",
      "     26     print(f\"Found {len(samples)} samples for Stage {stage}.\")",
      "---> 27 samples = parse_llava_jsonl(jsonl_path, image_folder)",
      "     28 return samples",
      "",
      "Cell In[4], line 16, in parse_llava_jsonl(jsonl_path, image_folder)",
      "     14 image_folder = Path(image_folder)",
      "     15 if not jsonl_path.is_file():",
      "---> 16     raise FileNotFoundError(f\"JSONL file not found: {jsonl_path}\")",
      "     18 samples = []",
      "     19 with open(jsonl_path, 'r') as f:",
      "",
      "FileNotFoundError: [Errno 2] No such file or directory: '/path/to/your/datasets/llava_pretrain/llava_pretrain.jsonl'"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Example usage: Create DataLoaders (requires config.yaml and actual data)\n",
    "try:\n",
    "    config_path = 'configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    dls = get_stage1_dataloaders(config)\n",
    "\n",
    "    print(f\"DataLoaders created. Testing show_batch...\")\n",
    "    # Now that batch transforms are complete, show_batch should work if data exists\n",
    "    # Ensure you have data available at the paths specified in config.yaml\n",
    "    # dls.show_batch(max_n=4, figsize=(12, 8))\n",
    "    # print(\"\\nTesting one_batch...\")\n",
    "    # b = dls.one_batch()\n",
    "    # print(\"one_batch() retrieved. Check keys and shapes.\")\n",
    "    # # Example: print shapes of tensors in the batch\n",
    "    # for k, v in b.items():\n",
    "    #     if isinstance(v, torch.Tensor):\n",
    "    #         print(f\"  {k}: {v.shape}\")\n",
    "    #     else:\n",
    "    #         print(f\"  {k}: {type(v)}\")\n",
    "\n",
    "    print(\"\\nNote: Full testing requires valid data paths and files.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nError creating DataLoaders: FileNotFoundError - {e}\")\n",
    "    print(\"Please ensure 'paths.data_base', 'paths.stage1_data', and 'paths.stage1_images' are correctly set in configs/config.yaml and point to existing data.\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\nAn unexpected error occurred during DataLoaders creation/testing: {e}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Update Data Handling for Stage 2 (Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will contain functions to create DataLoaders specifically for Stage 2 instruction tuning, using the appropriate chat template and label masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "# Placeholder for get_stage2_dataloaders function\n",
    "def get_stage2_dataloaders(config: dict):\n",
    "    \"\"\"Creates fastai DataLoaders for Stage 2 training (Placeholder).\"\"\"\n",
    "    # Similar to stage 1 but uses different template/masking\n",
    "    # Define llava_datablock_stage2 or custom dataset logic\n",
    "    print(\"get_stage2_dataloaders - Placeholder: Not implemented yet.\")\n",
    "    # Example structure:\n",
    "    # from .preprocessing import format_v1_template # Need v1 formatter\n",
    "    # llava_tokenizer_tfm_stage2 = LLaVATextTokenizer(tokenizer, template_formatter=format_v1_template)\n",
    "    # llava_batch_tfm_stage2 = LLaVABatchTransform(tokenizer=tokenizer, template='v1') # Need to adapt batch tfm for v1 masking\n",
    "    # LLaVADataBlockStage2 = DataBlock(\n",
    "    #     blocks=(ImageBlock(cls=PILImage), TransformBlock),\n",
    "    #     get_items=partial(get_llava_items, stage=2),\n",
    "    #     get_x=get_image_path,\n",
    "    #     get_y=get_conversations,\n",
    "    #     splitter=RandomSplitter(valid_pct=0.01, seed=42),\n",
    "    #     item_tfms=[\n",
    "    #         *basic_image_item_tfms,\n",
    "    #         llava_tokenizer_tfm_stage2\n",
    "    #     ],\n",
    "    #     batch_tfms=[\n",
    "    #         IntToFloatTensor(div_mask=torch.BoolTensor([True,False])),\n",
    "    #         clip_normalize,\n",
    "    #         llava_batch_tfm_stage2\n",
    "    #     ]\n",
    "    # )\n",
    "    # batch_size = config.get('data', {}).get('batch_size_per_device_stage2', 4)\n",
    "    # num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "    # dls = LLaVADataBlockStage2.dataloaders(config=config, bs=batch_size, num_workers=num_workers)\n",
    "    # return dls\n",
    "    raise NotImplementedError(\"Stage 2 DataLoaders are not yet implemented.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.4: Implement Custom Evaluation Set Handling (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "# Placeholder for loading custom eval set\n",
    "def get_custom_eval_dataloaders(config: dict):\n",
    "    \"\"\"Creates fastai DataLoaders for the custom evaluation set (Placeholder).\"\"\"\n",
    "    print(\"get_custom_eval_dataloaders - Placeholder: Not implemented yet.\")\n",
    "    # Similar logic to get_stage1/2_dataloaders but points to custom eval data paths\n",
    "    # Might use stage 2 templates/transforms or custom ones.\n",
    "    raise NotImplementedError(\"Custom Eval DataLoaders are not yet implemented.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "metadata": {
   "path": "nbs/10_data_loading.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "Adaptive_Patching_VIT_fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}