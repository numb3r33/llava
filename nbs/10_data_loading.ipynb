{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "> Functions and classes for loading and parsing datasets for LLaVA-style training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n",
      "Loaded config from configs/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n",
      "CLIP Mean: [0.48145466, 0.4578275, 0.40821073]\n",
      "CLIP Std: [0.26862954, 0.26130258, 0.27577711]\n",
      "Fastai Normalize Transform: Normalize -- {'mean': tensor([[[[0.4815]],\n",
      "\n",
      "         [[0.4578]],\n",
      "\n",
      "         [[0.4082]]]]), 'std': tensor([[[[0.2686]],\n",
      "\n",
      "         [[0.2613]],\n",
      "\n",
      "         [[0.2758]]]]), 'axes': (0, 2, 3)}\n",
      "(enc:1,dec:1)\n",
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n",
      "V1 template assistant role tokens: [1792, 29889]\n",
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0, Template: plain\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Union, Tuple\n",
    "from dataclasses import dataclass\n",
    "import PIL.Image\n",
    "from functools import partial\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.block import DataBlock, TransformBlock, CategoryBlock # Added CategoryBlock just in case\n",
    "from fastai.data.transforms import parent_label, GrandparentSplitter, RandomSplitter, IntToFloatTensor\n",
    "from fastai.data.core import DataLoaders # Import DataLoaders\n",
    "\n",
    "# Attempt to import from llava utils, handle potential ImportError if running standalone\n",
    "try:\n",
    "    from llava.utils import load_config\n",
    "except ImportError:\n",
    "    print(\"Warning: llava.utils not found. load_config function might be unavailable.\")\n",
    "    def load_config(path): return {}\n",
    "    \n",
    "# Import necessary items from preprocessing notebook\n",
    "try:\n",
    "    from llava.data.preprocessing import (\n",
    "        tokenizer, \n",
    "        LLaVATextTokenizer, \n",
    "        clip_normalize, \n",
    "        format_plain_template, \n",
    "        format_v1_template, # Import the V1 formatter\n",
    "        LLaVABatchTransform # Import the updated batch transform\n",
    "    )\n",
    "except ImportError:\n",
    "     print(\"Warning: llava.data.preprocessing not found or incomplete. Data loading might fail.\")\n",
    "     # Define dummy classes/functions if needed for basic script execution\n",
    "     tokenizer = None\n",
    "     clip_normalize = lambda x: x # Dummy normalize\n",
    "     def format_plain_template(conv, tok): return \"<image>\\nplaceholder\"\n",
    "     def format_v1_template(conv, tok): return \"USER: <image>\\nplaceholder ASSISTANT: response\"\n",
    "     class LLaVATextTokenizer(Transform):\n",
    "        def __init__(self, *args, **kwargs): pass\n",
    "        def encodes(self, x): return [0, 1, 2]\n",
    "     class LLaVABatchTransform(Transform):\n",
    "        def __init__(self, *args, **kwargs): self.split_idx=None\n",
    "        def encodes(self, x): return {'pixel_values': x[0], 'input_ids': x[1], 'attention_mask': torch.ones_like(x[1]), 'labels': x[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Data Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need functions to parse the JSONL files commonly used in LLaVA datasets. Each line typically contains:\n",
    "- `id`: A unique identifier for the sample.\n",
    "- `image`: The filename of the image (often relative to an `image_folder`).\n",
    "- `conversations`: A list of dictionaries, where each dictionary has `from` ('human' or 'gpt') and `value` (the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class LLaVASample:\n",
    "    \"\"\"Represents a single sample from a LLaVA-style dataset.\n",
    "\n",
    "    Attributes:\n",
    "        sample_id: Unique identifier for the sample.\n",
    "        image_path: Absolute or relative path to the image file.\n",
    "        conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
    "        data_source: Optional field indicating the source dataset.\n",
    "    \"\"\"\n",
    "    sample_id: str\n",
    "    image_path: Path\n",
    "    conversations: List[Dict[str, str]]\n",
    "    data_source: str | None = None\n",
    "    image_folder: Path | None = None # Store base image folder for resolving relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LLaVASample\n",
       "\n",
       ">      LLaVASample (sample_id:str, image_path:pathlib.Path,\n",
       ">                   conversations:List[Dict[str,str]],\n",
       ">                   data_source:str|None=None,\n",
       ">                   image_folder:pathlib.Path|None=None)\n",
       "\n",
       "*Represents a single sample from a LLaVA-style dataset.\n",
       "\n",
       "Attributes:\n",
       "    sample_id: Unique identifier for the sample.\n",
       "    image_path: Absolute or relative path to the image file.\n",
       "    conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
       "    data_source: Optional field indicating the source dataset.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LLaVASample\n",
       "\n",
       ">      LLaVASample (sample_id:str, image_path:pathlib.Path,\n",
       ">                   conversations:List[Dict[str,str]],\n",
       ">                   data_source:str|None=None,\n",
       ">                   image_folder:pathlib.Path|None=None)\n",
       "\n",
       "*Represents a single sample from a LLaVA-style dataset.\n",
       "\n",
       "Attributes:\n",
       "    sample_id: Unique identifier for the sample.\n",
       "    image_path: Absolute or relative path to the image file.\n",
       "    conversations: List of conversation turns (dictionaries with 'from' and 'value').\n",
       "    data_source: Optional field indicating the source dataset.*"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLaVASample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_llava_jsonl(jsonl_path: Union[str, Path], image_folder: Union[str, Path]) -> List[LLaVASample]:\n",
    "    \"\"\"Parses a LLaVA-style JSON Lines file (.jsonl) and prepares LLaVASample objects.\n",
    "    Args:\n",
    "        jsonl_path: Path to the JSON Lines file.\n",
    "        image_folder: Path to the directory containing the images.\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "    Raises:\n",
    "        FileNotFoundError: If the JSONL file does not exist.\n",
    "        json.JSONDecodeError: If a line in the file is not valid JSON.\n",
    "    \"\"\"\n",
    "    jsonl_path = Path(jsonl_path)\n",
    "    image_folder = Path(image_folder).resolve() # Resolve to absolute path\n",
    "    if not jsonl_path.is_file():\n",
    "        raise FileNotFoundError(f\"JSON Lines file not found: {jsonl_path}\")\n",
    "\n",
    "    samples = []\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line.strip()) # Parse each line as JSON\n",
    "                if not isinstance(data, dict) or not all(k in data for k in ['id', 'image', 'conversations']):\n",
    "                    print(f\"Warning: Skipping line {i+1} due to missing keys or incorrect format in {jsonl_path}. Data: {data}\")\n",
    "                    continue\n",
    "                \n",
    "                sample_id = data['id']\n",
    "                image_ref = data['image']\n",
    "                \n",
    "                # Handle image path: could be relative to image_folder or potentially elsewhere\n",
    "                # LLaVA often stores relative paths. Assume relative to image_folder for now.\n",
    "                image_path_relative = Path(image_ref) \n",
    "                # Store the relative path in the sample, resolve later if needed or assume loader does.\n",
    "                # Let's store the intended image folder as well for easier resolution later.\n",
    "                \n",
    "                conversations = data['conversations']\n",
    "                data_source = data.get('data_source')\n",
    "\n",
    "                if not isinstance(conversations, list) or not all(isinstance(turn, dict) and 'from' in turn and 'value' in turn for turn in conversations):\n",
    "                    print(f\"Warning: Skipping line {i+1} due to invalid 'conversations' format in {jsonl_path}.\")\n",
    "                    continue\n",
    "\n",
    "                samples.append(LLaVASample(\n",
    "                    sample_id=str(sample_id),\n",
    "                    image_path=image_path_relative, # Store relative path\n",
    "                    conversations=conversations,\n",
    "                    data_source=data_source,\n",
    "                    image_folder=image_folder # Store base folder\n",
    "                ))\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line {i+1} in {jsonl_path}: {e}. Line content: '{line.strip()}'\")\n",
    "                continue # Skip malformed lines\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {i+1} in {jsonl_path}: {e}. Data: {line.strip()}\")\n",
    "                continue\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### parse_llava_jsonl\n",
       "\n",
       ">      parse_llava_jsonl (jsonl_path:Union[str,pathlib.Path],\n",
       ">                         image_folder:Union[str,pathlib.Path])\n",
       "\n",
       "*Parses a LLaVA-style JSON Lines file (.jsonl) and prepares LLaVASample objects.\n",
       "Args:\n",
       "    jsonl_path: Path to the JSON Lines file.\n",
       "    image_folder: Path to the directory containing the images.\n",
       "Returns:\n",
       "    A list of LLaVASample objects.\n",
       "Raises:\n",
       "    FileNotFoundError: If the JSONL file does not exist.\n",
       "    json.JSONDecodeError: If a line in the file is not valid JSON.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### parse_llava_jsonl\n",
       "\n",
       ">      parse_llava_jsonl (jsonl_path:Union[str,pathlib.Path],\n",
       ">                         image_folder:Union[str,pathlib.Path])\n",
       "\n",
       "*Parses a LLaVA-style JSON Lines file (.jsonl) and prepares LLaVASample objects.\n",
       "Args:\n",
       "    jsonl_path: Path to the JSON Lines file.\n",
       "    image_folder: Path to the directory containing the images.\n",
       "Returns:\n",
       "    A list of LLaVASample objects.\n",
       "Raises:\n",
       "    FileNotFoundError: If the JSONL file does not exist.\n",
       "    json.JSONDecodeError: If a line in the file is not valid JSON.*"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(parse_llava_jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Image Loading and Basic Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This involves defining the `ImageBlock` and basic `item_tfms` for loading and resizing images. Normalization stats were defined in `11_data_preprocessing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define the ImageBlock using PILImage for loading images\n",
    "image_block = ImageBlock(cls=PILImage)\n",
    "\n",
    "# Define basic item transforms for image processing\n",
    "# 1. Resize images to 336x336, padding if necessary\n",
    "#    method='pad': Pads the image to the target size.\n",
    "#    pad_mode='const': Uses a constant value for padding.\n",
    "#    pad_value=0: Uses black for padding (common for vision models).\n",
    "# 2. Convert the image to a PyTorch tensor.\n",
    "# Note: Normalization will be applied in batch_tfms using clip_normalize.\n",
    "# Update: Use Resize(336, method='pad', pad_mode=PadMode.Constant, pad_value=0) based on tech spec\n",
    "basic_image_item_tfms = [\n",
    "    Resize(336, method='pad', pad_mode=PadMode.Constant, pad_value=0), # Explicit padding\n",
    "    ToTensor(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Define Custom Dataset/DataBlock (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the fastai `DataBlock` for Stage 1 projector pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_llava_items(config_source: dict, stage: int = 1) -> List[LLaVASample]:\n",
    "    \"\"\"Loads LLaVA samples for a specific stage based on config passed as source.\n",
    "\n",
    "    Args:\n",
    "        config_source: The main configuration dictionary (passed as the first argument).\n",
    "        stage: The training stage (1 or 2).\n",
    "\n",
    "    Returns:\n",
    "        A list of LLaVASample objects.\n",
    "    \"\"\"\n",
    "    config = config_source\n",
    "    if not isinstance(config, dict):\n",
    "         raise TypeError(f\"Expected configuration dictionary as the first argument, but got type {type(config_source)}\")\n",
    "\n",
    "    data_base_path = Path(config['paths']['data_base'])\n",
    "    if stage == 1:\n",
    "        json_rel_path = config['paths']['stage1_data']\n",
    "        images_rel_path = config['paths']['stage1_images']\n",
    "    elif stage == 2:\n",
    "        json_rel_path = config['paths']['stage2_data']\n",
    "        # Stage 2 images might be in various locations relative to data_base or absolute.\n",
    "        # The `image_folder` in LLaVASample helps resolve this later.\n",
    "        images_rel_path = config['paths'].get('stage2_images', '.') # Default to data_base if specific path not given\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid stage specified: {stage}. Must be 1 or 2.\")\n",
    "\n",
    "    json_path = data_base_path / json_rel_path # Path to the JSON(L) file\n",
    "    # This base folder is used to resolve relative paths in the jsonl file\n",
    "    base_image_folder = data_base_path / images_rel_path \n",
    "\n",
    "    print(f\"Loading Stage {stage} items from: {json_path}\")\n",
    "    print(f\"Assuming image paths relative to: {base_image_folder}\")\n",
    "\n",
    "    if not json_path.exists():\n",
    "        raise FileNotFoundError(f\"Stage {stage} JSON(L) file not found: {json_path}\")\n",
    "\n",
    "    # Use the JSONL parsing function\n",
    "    samples = parse_llava_jsonl(json_path, base_image_folder)\n",
    "    print(f\"Found {len(samples)} samples for Stage {stage}.\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "def get_image_path(sample: LLaVASample) -> Path:\n",
    "    \"\"\"Extracts and potentially resolves the image path from a LLaVASample.\"\"\"\n",
    "    # Resolve the path relative to the stored image_folder\n",
    "    if sample.image_folder and not sample.image_path.is_absolute():\n",
    "        return sample.image_folder / sample.image_path\n",
    "    return sample.image_path\n",
    "\n",
    "def get_conversations(sample: LLaVASample) -> list:\n",
    "    \"\"\"Extracts the conversations from a LLaVASample.\"\"\"\n",
    "    return sample.conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1 template assistant role tokens: [1792, 29889]\n",
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0, Template: plain\n",
      "LLaVADataBlockStage1 defined.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Define the DataBlock for Stage 1 (Projector Pre-training)\n",
    "if tokenizer and 'LLaVABatchTransform' in globals() and 'LLaVATextTokenizer' in globals():\n",
    "    # Instantiate the batch transform for stage 1 ('plain' template)\n",
    "    llava_batch_tfm_stage1 = LLaVABatchTransform(tokenizer=tokenizer, \n",
    "                                                 normalize_tfm=clip_normalize,\n",
    "                                                 template='plain')\n",
    "\n",
    "    LLaVADataBlockStage1 = DataBlock(\n",
    "        blocks=(ImageBlock(cls=PILImage), TransformBlock), # Output of get_y is list (handled by TransformBlock)\n",
    "        get_items=partial(get_llava_items, stage=1),\n",
    "        get_x=get_image_path,\n",
    "        get_y=get_conversations,\n",
    "        splitter=RandomSplitter(valid_pct=0.01, seed=42), # Example split\n",
    "        item_tfms=[\n",
    "            *basic_image_item_tfms,\n",
    "            # Apply text tokenization using the 'plain' formatter\n",
    "            LLaVATextTokenizer(tokenizer, template_formatter=format_plain_template)\n",
    "        ],\n",
    "        batch_tfms=[\n",
    "            # Apply batch transformations including masking for 'plain' template\n",
    "            llava_batch_tfm_stage1\n",
    "        ]\n",
    "    )\n",
    "    print(\"LLaVADataBlockStage1 defined.\")\n",
    "else:\n",
    "    LLaVADataBlockStage1 = None\n",
    "    print(\"Tokenizer, LLaVABatchTransform, or LLaVATextTokenizer not available, LLaVADataBlockStage1 not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6: Create DataLoaders (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how to create the `DataLoaders` object from the `DataBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stage1_dataloaders(config: dict, dblock: DataBlock = LLaVADataBlockStage1) -> DataLoaders:\n",
    "    \"\"\"Creates fastai DataLoaders for Stage 1 training.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "        dblock: The configured DataBlock for Stage 1 (defaults to LLaVADataBlockStage1).\n",
    "\n",
    "    Returns:\n",
    "        A fastai DataLoaders object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the DataBlock is not defined.\n",
    "        FileNotFoundError: If data paths are invalid during DataBlock processing.\n",
    "    \"\"\"\n",
    "    if dblock is None:\n",
    "        raise ValueError(\"Stage 1 DataBlock is not defined. Ensure dependencies are available.\")\n",
    "\n",
    "    batch_size = config.get('data', {}).get('batch_size_per_device_stage1', 8)\n",
    "    num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "\n",
    "    print(f\"Creating Stage 1 DataLoaders with batch size: {batch_size}, num_workers: {num_workers}\")\n",
    "\n",
    "    # The DataBlock's get_items function needs the config,\n",
    "    # we pass it here when calling dataloaders()\n",
    "    try:\n",
    "        # Pass config explicitly to dataloaders, which passes it down to get_items\n",
    "        dls = dblock.dataloaders(source=config, # Pass config to be used by get_items\n",
    "                                 bs=batch_size,\n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=(torch.cuda.is_available())) # Pin memory if using GPU\n",
    "        print(\"DataLoaders created successfully.\")\n",
    "        return dls\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error creating DataLoaders: {e}\")\n",
    "        print(\"Please ensure data paths in config.yaml are correct and data exists.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An unexpected error occurred during DataLoaders creation: {e}\")\n",
    "        traceback.print_exc() # Print full traceback for debugging\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Update Data Handling for Stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the function to create DataLoaders specifically for Stage 2 instruction tuning, using the Vicuna v1 chat template and corresponding label masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1 template assistant role tokens: [1792, 29889]\n",
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0, Template: v1\n",
      "LLaVADataBlockStage2 defined.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Define the DataBlock for Stage 2 (Instruction Tuning)\n",
    "LLaVADataBlockStage2 = None\n",
    "if tokenizer and 'LLaVABatchTransform' in globals() and 'LLaVATextTokenizer' in globals() and 'format_v1_template' in globals():\n",
    "    # Instantiate the batch transform for stage 2 ('v1' template)\n",
    "    llava_batch_tfm_stage2 = LLaVABatchTransform(tokenizer=tokenizer, \n",
    "                                                 normalize_tfm=clip_normalize,\n",
    "                                                 template='v1')\n",
    "    \n",
    "    LLaVADataBlockStage2 = DataBlock(\n",
    "        blocks=(ImageBlock(cls=PILImage), TransformBlock),\n",
    "        # Use stage=2 for get_items\n",
    "        get_items=partial(get_llava_items, stage=2),\n",
    "        get_x=get_image_path,\n",
    "        get_y=get_conversations,\n",
    "        splitter=RandomSplitter(valid_pct=0.01, seed=42), # Adjust split as needed\n",
    "        item_tfms=[\n",
    "            *basic_image_item_tfms,\n",
    "            # Apply text tokenization using the 'v1' formatter\n",
    "            LLaVATextTokenizer(tokenizer, template_formatter=format_v1_template)\n",
    "        ],\n",
    "        batch_tfms=[\n",
    "            # Apply batch transformations including masking for 'v1' template\n",
    "            llava_batch_tfm_stage2\n",
    "        ]\n",
    "    )\n",
    "    print(\"LLaVADataBlockStage2 defined.\")\n",
    "else:\n",
    "    print(\"Dependencies missing, LLaVADataBlockStage2 not defined.\")\n",
    "\n",
    "# Function to create Stage 2 DataLoaders\n",
    "def get_stage2_dataloaders(config: dict, dblock: DataBlock = LLaVADataBlockStage2) -> DataLoaders:\n",
    "    \"\"\"Creates fastai DataLoaders for Stage 2 training (Instruction Tuning).\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "        dblock: The configured DataBlock for Stage 2 (defaults to LLaVADataBlockStage2).\n",
    "\n",
    "    Returns:\n",
    "        A fastai DataLoaders object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the DataBlock is not defined.\n",
    "        FileNotFoundError: If data paths are invalid during DataBlock processing.\n",
    "    \"\"\"\n",
    "    if dblock is None:\n",
    "        raise ValueError(\"Stage 2 DataBlock is not defined. Ensure dependencies are available.\")\n",
    "\n",
    "    batch_size = config.get('data', {}).get('batch_size_per_device_stage2', 4) # Use stage2 batch size\n",
    "    num_workers = config.get('data', {}).get('num_workers', 4)\n",
    "\n",
    "    print(f\"Creating Stage 2 DataLoaders with batch size: {batch_size}, num_workers: {num_workers}\")\n",
    "\n",
    "    try:\n",
    "        # Pass config to dataloaders to be used by get_items\n",
    "        dls = dblock.dataloaders(source=config, \n",
    "                                 bs=batch_size,\n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=(torch.cuda.is_available()))\n",
    "        print(\"Stage 2 DataLoaders created successfully.\")\n",
    "        return dls\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error creating Stage 2 DataLoaders: {e}\")\n",
    "        print(\"Please ensure Stage 2 data paths in config.yaml are correct and data exists.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An unexpected error occurred during Stage 2 DataLoaders creation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Stage 2 DataLoaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Creating dummy Stage 2 JSONL: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "Creating Stage 2 DataLoaders with batch size: 4, num_workers: 4\n",
      "Loading Stage 2 items from: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "Assuming image paths relative to: /workspace/llava/data\n",
      "Found 2 samples for Stage 2.\n",
      "Stage 2 DataLoaders created successfully.\n",
      "\n",
      "Testing one_batch for Stage 2 DataLoaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/fastai/data/load.py:148: UserWarning: Your `DataLoader`'s `num_workers` is `4` but `batch_size` is small (`2`). Toggling `num_workers=0` for debug purposes.\ You can ignore this warning if you are not debugging. Initial `num_workers` is restored in `__del__`\n",
      "  if self.num_workers > 0 and warn:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_batch() retrieved. Check shapes and content.\n",
      "Batch Keys: dict_keys(['pixel_values', 'input_ids', 'attention_mask', 'labels'])\n",
      "pixel_values shape: torch.Size([2, 3, 336, 336])\n",
      "input_ids shape: torch.Size([2, 68])\n",
      "attention_mask shape: torch.Size([2, 68])\n",
      "labels shape: torch.Size([2, 68])\n",
      "\n",
      "--- Decoded Example from Batch --- \n",
      "Image Shape: torch.Size([3, 336, 336])\n",
      "Decoded Text: USER: <image>\nDescribe image. ASSISTANT: It is a red object.\n",
      "--- End Decoded Example ---\n",
      "\n",
      "Stage 2 DataLoaders test passed (basic check).\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "try:\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # --- Test Setup: Create dummy Stage 2 data ---\n",
    "    data_base = Path(config['paths']['data_base'])\n",
    "    stage2_json_rel = Path(config['paths']['stage2_data'])\n",
    "    # Stage 2 images might reference paths from stage 1 or other datasets\n",
    "    stage1_img_rel = Path(config['paths']['stage1_images'])\n",
    "    stage1_img_path = data_base / stage1_img_rel\n",
    "\n",
    "    stage2_json_path = data_base / stage2_json_rel\n",
    "    stage2_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Assume dummy images from Stage 1 exist\n",
    "    img1_rel_path = stage1_img_rel.name + '/dummy_img1.jpg' # Path relative to data_base\n",
    "    img2_rel_path = stage1_img_rel.name + '/dummy_img2.png'\n",
    "    if not (stage1_img_path / 'dummy_img1.jpg').exists():\n",
    "         print(\"Warning: Dummy image dummy_img1.jpg not found. Creating.\")\n",
    "         stage1_img_path.mkdir(parents=True, exist_ok=True)\n",
    "         PIL.Image.new('RGB', (60, 30), color = 'red').save(stage1_img_path / 'dummy_img1.jpg')\n",
    "    if not (stage1_img_path / 'dummy_img2.png').exists():\n",
    "         print(\"Warning: Dummy image dummy_img2.png not found. Creating.\")\n",
    "         stage1_img_path.mkdir(parents=True, exist_ok=True)\n",
    "         PIL.Image.new('RGB', (60, 30), color = 'green').save(stage1_img_path / 'dummy_img2.png')\n",
    "         \n",
    "    if not stage2_json_path.exists() or stage2_json_path.stat().st_size < 10:\n",
    "        print(f\"Creating dummy Stage 2 JSONL: {stage2_json_path}\")\n",
    "        dummy_jsonl_content = [\n",
    "            {\"id\": \"s2_001\", \"image\": str(img1_rel_path), \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nDescribe image.\"}, {\"from\": \"gpt\", \"value\": \"It is a red object.\"}]}, \n",
    "            {\"id\": \"s2_002\", \"image\": str(img2_rel_path), \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nIs it green?\"}, {\"from\": \"gpt\", \"value\": \"Yes, it appears green.\"}]},\n",
    "        ]\n",
    "        with open(stage2_json_path, 'w') as f:\n",
    "            for item in dummy_jsonl_content:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "    # --- End Test Setup ---\n",
    "\n",
    "    # Get Stage 2 DataLoaders\n",
    "    dls_stage2 = get_stage2_dataloaders(config)\n",
    "    \n",
    "    assert isinstance(dls_stage2, DataLoaders)\n",
    "    assert len(dls_stage2.train_ds) > 0\n",
    "    assert len(dls_stage2.valid_ds) > 0\n",
    "    \n",
    "    print(\"\\nTesting one_batch for Stage 2 DataLoaders...\")\n",
    "    b = dls_stage2.one_batch()\n",
    "    print(\"one_batch() retrieved. Check shapes and content.\")\n",
    "    assert isinstance(b, dict) # Batch transform should output dict\n",
    "    assert 'pixel_values' in b\n",
    "    assert 'input_ids' in b\n",
    "    assert 'attention_mask' in b\n",
    "    assert 'labels' in b\n",
    "    print(f\"Batch Keys: {b.keys()}\")\n",
    "    print(f\"pixel_values shape: {b['pixel_values'].shape}\")\n",
    "    print(f\"input_ids shape: {b['input_ids'].shape}\")\n",
    "    print(f\"attention_mask shape: {b['attention_mask'].shape}\")\n",
    "    print(f\"labels shape: {b['labels'].shape}\")\n",
    "\n",
    "    # Decode one example for visual inspection\n",
    "    print(\"\\n--- Decoded Example from Batch --- \")\n",
    "    decoded_batch = LLaVABatchTransform(tokenizer, clip_normalize, template='v1').decode(b)\n",
    "    img_decoded, text_decoded = decoded_batch[0][0], decoded_batch[1][0]\n",
    "    print(f\"Image Shape: {img_decoded.shape}\")\n",
    "    print(f\"Decoded Text: {text_decoded}\")\n",
    "    print(\"--- End Decoded Example ---\")\n",
    "    \n",
    "    print(\"\\nStage 2 DataLoaders test passed (basic check).\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Skipping test: FileNotFoundError - {e}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred during Stage 2 DataLoaders test: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.4: Implement Custom Evaluation Set Handling (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Placeholder for loading custom eval set\n",
    "def get_custom_eval_dataloaders(config: dict):\n",
    "    \"\"\"Creates fastai DataLoaders for the custom evaluation set (Placeholder).\"\"\"\n",
    "    print(\"get_custom_eval_dataloaders - Placeholder: Not implemented yet.\")\n",
    "    # Similar logic to get_stage1/2_dataloaders but points to custom eval data paths\n",
    "    # Might use stage 2 templates/transforms or custom ones.\n",
    "    raise NotImplementedError(\"Custom Eval DataLoaders are not yet implemented.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/10_data_loading.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "llava"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}