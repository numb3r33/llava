{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> Functions for evaluating LLaVA models, including prediction generation and metric calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Union, Optional, Dict, Any, List # Added Dict, Any, List\n",
    "import wandb # Added wandb import\n",
    "\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.load import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root\n",
    "project_root = Path(os.getcwd())\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")\n",
    "    \n",
    "# Import necessary llava components\n",
    "try:\n",
    "    from llava.utils import load_config, init_wandb # Added init_wandb\n",
    "    from llava.data.loading import get_test_dataloader, LLaVADataBlockStage2 # Assumes this function exists in 10_data_loading\n",
    "    from llava.model.baseline import BaselineLLaVAModel # Or AdaptiveLLaVAModel later\n",
    "    from llava.data.preprocessing import tokenizer, DEFAULT_IMAGE_TOKEN, IGNORE_INDEX, IMAGE_TOKEN_INDEX_PLACEHOLDER # Import tokenizer & constants\n",
    "    # Import PEFT related things if needed for model loading check\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        _peft_available = True\n",
    "    except ImportError:\n",
    "        PeftModel = None \n",
    "        _peft_available = False\n",
    "    # from llava.training.stage2 import get_stage2_learner # Maybe needed for loading full learner state\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing llava modules: {e}. Make sure nbdev_export was run.\")\n",
    "    # Define placeholders if running standalone or during initial setup\n",
    "    def load_config(path): return {}\n",
    "    def init_wandb(*args, **kwargs): pass\n",
    "    def get_test_dataloader(config, dataset_name, dblock=None): raise NotImplementedError\n",
    "    class BaselineLLaVAModel(torch.nn.Module): \n",
    "        def __init__(self, *args, **kwargs): \n",
    "            super().__init__()\n",
    "            self.dummy = torch.nn.Linear(1,1)\n",
    "            self.image_token_index_marker = -200 # Define necessary attributes\n",
    "            self.projector = torch.nn.Linear(10,10)\n",
    "            self.language_model = self # Make model act like LLM for generate\n",
    "            self.vision_tower = self # Dummy vision tower\n",
    "            self.config = {} # Dummy config\n",
    "        def encode_image(self, *args, **kwargs): return torch.randn(1,5,10) # Dummy image features B, P, D\n",
    "        def get_input_embeddings(self): return torch.nn.Embedding(100, 10)\n",
    "        def forward(self, *args, **kwargs): return CausalLMOutputWithPast(logits=torch.randn(1, 10, 100))\n",
    "        def generate(self, *args, **kwargs): return torch.randint(0, 100, (1, kwargs.get('max_new_tokens', 10)))\n",
    "        def to(self, *args, **kwargs): return self # Avoid device errors in dummy\n",
    "        def eval(self): pass\n",
    "    class DummyTokenizer:\n",
    "         def __init__(self): self.eos_token_id=1; self.pad_token_id=0\n",
    "         def decode(self, *args, **kwargs): return \"dummy decoded text\"\n",
    "    tokenizer = DummyTokenizer()\n",
    "    DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "    IGNORE_INDEX = -100\n",
    "    IMAGE_TOKEN_INDEX_PLACEHOLDER = -200\n",
    "    LLaVADataBlockStage2 = None # Placeholder\n",
    "    PeftModel = None\n",
    "    _peft_available = False\n",
    "    # def get_stage2_learner(config): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Prediction Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a trained fastai `Learner` and a test `DataLoader`, generates predictions, decodes them into text, and saves them to a JSON file. It now also includes efficiency logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_predictions(learner: Learner, \n",
    "                         test_dl: DataLoader, \n",
    "                         output_file: Union[str, Path], \n",
    "                         max_len: int = 200, # Max generation length\n",
    "                         temperature: float = 0.2, # Generation temperature\n",
    "                         top_p: Optional[float] = None, # Nucleus sampling top_p\n",
    "                        ):\n",
    "    \"\"\"Generates predictions for a test dataloader and saves them to a JSON Lines file.\n",
    "    Includes efficiency logging (latency, peak VRAM).\n",
    "\n",
    "    Uses the HF generate method on the underlying language model component,\n",
    "    manually preparing the combined image and text embeddings.\n",
    "\n",
    "    Args:\n",
    "        learner: Trained fastai Learner object containing the model.\n",
    "                 Expected to have `learner.model`, `learner.dls.device`,\n",
    "                 and potentially `learner.tokenizer` or `learner.dls.tokenizer`.\n",
    "        test_dl: DataLoader for the test set.\n",
    "        output_file: Path to save the JSON Lines prediction file.\n",
    "        max_len: Maximum number of new tokens to generate.\n",
    "        temperature: Sampling temperature for generation. Set to 0 for greedy decoding.\n",
    "        top_p: If set (and temperature > 0), use nucleus sampling with this top_p value.\n",
    "    \"\"\"\n",
    "    output_file = Path(output_file)\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model = learner.model\n",
    "    # Get tokenizer - check learner, then dls, then global scope\n",
    "    if hasattr(learner, 'tokenizer') and learner.tokenizer is not None:\n",
    "        tok = learner.tokenizer\n",
    "    elif hasattr(learner.dls, 'tokenizer') and learner.dls.tokenizer is not None:\n",
    "        tok = learner.dls.tokenizer\n",
    "    else:\n",
    "        global tokenizer # Use the global tokenizer imported/defined earlier\n",
    "        if tokenizer is None:\n",
    "             raise AttributeError(\"Tokenizer not found in learner, dls, or globally. Cannot decode predictions.\")\n",
    "        print(\"Warning: Using global tokenizer instance for generation.\")\n",
    "        tok = tokenizer\n",
    "        \n",
    "    # Ensure model components exist\n",
    "    if not all(hasattr(model, attr) and getattr(model, attr) is not None \n",
    "               for attr in ['vision_tower', 'projector', 'language_model', 'get_input_embeddings', 'encode_image']):\n",
    "        raise AttributeError(\"Model is missing required components (vision_tower, projector, language_model, etc.)\")\n",
    "        \n",
    "    model.eval() # Set model to evaluation mode\n",
    "    results = []\n",
    "    total_time = 0\n",
    "    num_samples = 0\n",
    "    device = learner.dls.device # Get device from learner\n",
    "\n",
    "    print(f\"Generating predictions for {len(test_dl.dataset)} samples...\")\n",
    "    print(f\"Saving predictions to: {output_file}\")\n",
    "    \n",
    "    # Reset CUDA memory stats before generation loop for accurate peak measurement\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats(device=device) \n",
    "        print(\"Reset CUDA peak memory stats before generation.\")\n",
    "\n",
    "    # Use context manager for file writing\n",
    "    with torch.no_grad(), open(output_file, 'w') as f_out:\n",
    "        # Iterate through batches\n",
    "        for batch in tqdm(test_dl, desc=\"Generating Predictions\"):\n",
    "            start_time = time.time() # Start timing for this batch\n",
    "            \n",
    "            # Move batch items to appropriate device\n",
    "            if not isinstance(batch, dict):\n",
    "                 print(f\"Warning: Expected batch to be a dict, got {type(batch)}. Attempting to proceed assuming basic structure.\")\n",
    "                 pixel_values = batch[0].to(device)\n",
    "                 input_ids = batch[1].to(device)\n",
    "                 batch_size = pixel_values.shape[0]\n",
    "                 # Need a way to get IDs if batch isn't dict\n",
    "                 batch_ids = [f\"unknown_{num_samples+i}\" for i in range(batch_size)]\n",
    "            else:\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device) \n",
    "                batch_size = pixel_values.shape[0]\n",
    "                # Assuming the dataloader added the original sample ID to the batch dict\n",
    "                # This depends on modifying the DataBlock/Dataset/Collate\n",
    "                # For now, let's try getting it from the dataloader's dataset if possible\n",
    "                batch_ids = []\n",
    "                start_ds_idx = test_dl.num_workers * test_dl.offs if hasattr(test_dl, 'offs') else num_samples\n",
    "                for i in range(batch_size):\n",
    "                    current_item_idx = start_ds_idx + i\n",
    "                    item_id = f\"sample_{current_item_idx}\" # Default ID\n",
    "                    try:\n",
    "                        if hasattr(test_dl, 'dataset') and current_item_idx < len(test_dl.dataset):\n",
    "                            item_data = test_dl.dataset[current_item_idx]\n",
    "                            # Adapt based on what dataset[idx] returns\n",
    "                            if hasattr(item_data, 'sample_id'): # If LLaVASample\n",
    "                                item_id = item_data.sample_id\n",
    "                            elif isinstance(item_data, dict) and 'id' in item_data: # If dict from JSONL\n",
    "                                item_id = item_data['id']\n",
    "                            # Add other checks if dataset format is different\n",
    "                    except Exception:\n",
    "                        pass # Use default ID\n",
    "                    batch_ids.append(item_id)\n",
    "\n",
    "            # --- Generation using underlying HF LLM --- \n",
    "            # 1. Encode images and project features\n",
    "            image_features = model.encode_image(pixel_values) # (B, P, D_clip)\n",
    "            if image_features is None: \n",
    "                print(\"Warning: Image encoding failed for batch. Skipping.\")\n",
    "                num_samples += batch_size # Account for skipped samples in latency calculation\n",
    "                total_time += (time.time() - start_time) # Add time spent\n",
    "                continue # Skip this batch\n",
    "            projected_features = model.projector(image_features) # (B, P, D_llm)\n",
    "            \n",
    "            outputs_list = []\n",
    "            # Process each sample in the batch individually for embedding preparation\n",
    "            for i in range(batch_size):\n",
    "                current_input_ids = input_ids[i:i+1] \n",
    "                current_proj_features = projected_features[i:i+1]\n",
    "                \n",
    "                marker = getattr(model, 'image_token_index_marker', IMAGE_TOKEN_INDEX_PLACEHOLDER)\n",
    "                image_token_indices = torch.where(current_input_ids[0] == marker)[0]\n",
    "                \n",
    "                if len(image_token_indices) == 0:\n",
    "                    print(f\"Warning: Image token marker {marker} not found in sample {num_samples + i}. Skipping generation.\")\n",
    "                    outputs_list.append(torch.tensor([tok.eos_token_id], device=device)) \n",
    "                    continue\n",
    "                \n",
    "                image_token_start_index = image_token_indices[0].item()\n",
    "\n",
    "                # Prepare prompt embeddings\n",
    "                input_ids_no_marker = current_input_ids.clone()\n",
    "                input_ids_no_marker[input_ids_no_marker == marker] = 0 \n",
    "                text_embeddings = model.get_input_embeddings()(input_ids_no_marker)\n",
    "                \n",
    "                text_emb_before = text_embeddings[:, :image_token_start_index]\n",
    "                text_emb_after = text_embeddings[:, image_token_start_index + 1:]\n",
    "                \n",
    "                prompt_embeds = torch.cat([\n",
    "                    text_emb_before,\n",
    "                    current_proj_features.to(text_embeddings.device, dtype=text_embeddings.dtype),\n",
    "                    text_emb_after\n",
    "                ], dim=1)\n",
    "                \n",
    "                # Generate using the underlying LLM\n",
    "                llm_component = model.language_model\n",
    "                if _peft_available and isinstance(llm_component, PeftModel):\n",
    "                     llm_component = llm_component.base_model.model \n",
    "                \n",
    "                prompt_attention_mask = torch.ones(prompt_embeds.shape[:2], dtype=torch.long, device=device)\n",
    "                \n",
    "                try:\n",
    "                    gen_params = {\n",
    "                        \"inputs_embeds\": prompt_embeds,\n",
    "                        \"attention_mask\": prompt_attention_mask,\n",
    "                        \"max_new_tokens\": max_len,\n",
    "                        \"eos_token_id\": tok.eos_token_id,\n",
    "                        \"pad_token_id\": tok.pad_token_id if tok.pad_token_id is not None else tok.eos_token_id,\n",
    "                        \"do_sample\": (temperature > 0),\n",
    "                        \"num_beams\": 1\n",
    "                    }\n",
    "                    if temperature > 0:\n",
    "                        gen_params[\"temperature\"] = temperature\n",
    "                        if top_p is not None:\n",
    "                            gen_params[\"top_p\"] = top_p\n",
    "                    else: \n",
    "                        gen_params[\"temperature\"] = 1.0 \n",
    "                        gen_params[\"do_sample\"] = False\n",
    "\n",
    "                    output_ids_gen = llm_component.generate(**gen_params)\n",
    "                    \n",
    "                    output_ids_gen = output_ids_gen[:, prompt_embeds.shape[1]:]\n",
    "                    outputs_list.append(output_ids_gen[0]) \n",
    "                except Exception as gen_e:\n",
    "                     print(f\"Error during generation for sample {num_samples + i}: {gen_e}\")\n",
    "                     outputs_list.append(torch.tensor([tok.eos_token_id], device=device)) # Fallback\n",
    "            # --- End Generation for Batch --- #\n",
    "\n",
    "            end_time = time.time() # End timing for this batch\n",
    "            total_time += (end_time - start_time)\n",
    "            num_samples_in_batch = batch_size\n",
    "\n",
    "            # --- Decode and Save Results for the Batch --- \n",
    "            # Use the batch_ids retrieved earlier\n",
    "            item_ids = batch_ids\n",
    "                 \n",
    "            # Decode and write each result in the batch\n",
    "            for i, gen_ids in enumerate(outputs_list):\n",
    "                decoded_text = tok.decode(gen_ids.cpu(), skip_special_tokens=True).strip()\n",
    "                result_entry = {\n",
    "                    \"id\": item_ids[i], # Use the ID retrieved from the dataset/batch\n",
    "                    \"prediction\": decoded_text,\n",
    "                }\n",
    "                f_out.write(json.dumps(result_entry) + '\\n')\n",
    "                results.append(result_entry)\n",
    "            \n",
    "            num_samples += num_samples_in_batch # Update total sample count\n",
    "\n",
    "    # --- Log Efficiency Metrics (Step 5.4) --- \n",
    "    avg_latency = (total_time / num_samples) * 1000 if num_samples > 0 else 0\n",
    "    print(f\"Finished generation. Saved {len(results)} predictions to {output_file}\")\n",
    "    print(f\"Average inference latency: {avg_latency:.2f} ms/sample\")\n",
    "    \n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\"eval/avg_inference_latency_ms\": avg_latency})\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        peak_vram_gb = torch.cuda.max_memory_allocated(device=device) / (1024**3) \n",
    "        print(f\"Peak Inference VRAM used: {peak_vram_gb:.2f} GB\")\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\"eval/peak_inference_vram_gb\": peak_vram_gb})\n",
    "        # Reset peak stats after logging for this run\n",
    "        torch.cuda.reset_peak_memory_stats(device=device) \n",
    "        \n",
    "    return results # Return list of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3 & 5.5: Integrate External Evaluation Scripts (Implement VQAv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for calling external evaluation scripts. The `evaluate_vqa` function is now implemented using the official VQA tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def evaluate_vqa(preds_file: Union[str, Path], \n",
    "                 gt_file: Union[str, Path], \n",
    "                 ques_file: Optional[Union[str, Path]] = None, # Path to questions file if needed by API\n",
    "                 **kwargs):\n",
    "    \"\"\"Evaluates VQAv2 predictions using the official VQA evaluation tools.\n",
    "    \n",
    "    Args:\n",
    "        preds_file: Path to the JSON Lines prediction file generated by `generate_predictions`\n",
    "                    (expected format: {'id': question_id, 'prediction': answer}).\n",
    "        gt_file: Path to the ground truth annotation file (e.g., v2_mscoco_val2014_annotations.json).\n",
    "        ques_file: Path to the questions file (e.g., v2_OpenEnded_mscoco_val2014_questions.json). \n",
    "                   Required by the standard VQA eval tools.\n",
    "        **kwargs: Additional arguments (unused currently).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing VQA scores (e.g., {'overall': score, 'yes/no': ..., 'number': ..., 'other': ...}), \n",
    "        or {'overall': 0.0} if evaluation fails.\n",
    "    \"\"\"\n",
    "    print(f\"--- VQA Evaluation --- \")\n",
    "    print(f\"Predictions file: {preds_file}\")\n",
    "    print(f\"Ground Truth Annotation file: {gt_file}\")\n",
    "    print(f\"Ground Truth Questions file: {ques_file}\")\n",
    "    results = {\"overall\": 0.0} # Default return value\n",
    "    \n",
    "    if ques_file is None:\n",
    "        print(\"Error: Path to questions file (ques_file) is required for VQA evaluation.\")\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        # --- Load VQA Tools --- \n",
    "        # Assumes the VQA evaluation tools directory ('PythonHelperTools') is in the Python path.\n",
    "        # User needs to download from https://visualqa.org/download.html\n",
    "        # and add the PythonHelperTools directory to their PYTHONPATH or sys.path\n",
    "        try:\n",
    "            from vqaTools.vqa import VQA\n",
    "            from vqaEvaluation.vqaEval import VQAEval\n",
    "        except ImportError:\n",
    "            print(\"Error: VQA evaluation tools (vqaTools/vqaEvaluation) not found in PYTHONPATH.\")\n",
    "            print(\"Please download from https://visualqa.org/download.html and add PythonHelperTools to your path.\")\n",
    "            return results\n",
    "        \n",
    "        # --- Load and Reformat Predictions --- \n",
    "        vqa_preds_formatted = []\n",
    "        print(f\"Loading and reformatting predictions from {preds_file}...\")\n",
    "        with open(preds_file, 'r') as f_pred:\n",
    "            for line in f_pred:\n",
    "                try:\n",
    "                    pred_item = json.loads(line.strip())\n",
    "                    # VQA API expects integer question_id\n",
    "                    question_id = int(pred_item['id']) \n",
    "                    answer = pred_item['prediction']\n",
    "                    vqa_preds_formatted.append({\n",
    "                        'question_id': question_id,\n",
    "                        'answer': answer\n",
    "                    })\n",
    "                except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "                    print(f\"Warning: Skipping invalid prediction line: {line.strip()}. Error: {e}\")\n",
    "        print(f\"Loaded {len(vqa_preds_formatted)} predictions for evaluation.\")\n",
    "        if not vqa_preds_formatted:\n",
    "            print(\"Error: No valid predictions found in the file.\")\n",
    "            return results\n",
    "        \n",
    "        # --- Run VQA Evaluation --- \n",
    "        print(\"Initializing VQA evaluation...\")\n",
    "        vqa_ann = VQA(gt_file, ques_file) # Load annotations and questions\n",
    "        vqa_pred = vqa_ann.loadRes(vqa_preds_formatted, ques_file) # Load predictions\n",
    "        \n",
    "        vqa_eval = VQAEval(vqa_ann, vqa_pred, n=2) # n=2 is standard for VQA\n",
    "        \n",
    "        print(\"Running evaluation...\")\n",
    "        vqa_eval.evaluate() # Perform evaluation\n",
    "        \n",
    "        print(\"Evaluation complete. Results:\")\n",
    "        vqa_eval.showEvals() # Print detailed results\n",
    "        \n",
    "        # Extract results into a dictionary\n",
    "        results = {\n",
    "            \"overall\": vqa_eval.accuracy['overall'],\n",
    "            \"yes/no\": vqa_eval.accuracy['perAnswerType']['yes/no'],\n",
    "            \"number\": vqa_eval.accuracy['perAnswerType']['number'],\n",
    "            \"other\": vqa_eval.accuracy['perAnswerType']['other'],\n",
    "        }\n",
    "        print(f\"Parsed VQA Accuracy: {results}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "         print(f\"Error: Required file not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during VQA evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Log metric to W&B if active\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\"eval/vqa_score_overall\": results.get(\"overall\", 0.0)})\n",
    "        wandb.log({\"eval/vqa_score_yes_no\": results.get(\"yes/no\", 0.0)})\n",
    "        wandb.log({\"eval/vqa_score_number\": results.get(\"number\", 0.0)})\n",
    "        wandb.log({\"eval/vqa_score_other\": results.get(\"other\", 0.0)})\n",
    "        \n",
    "    return results\n",
    "\n",
    "#| export\n",
    "def evaluate_textvqa(preds_file: Union[str, Path], gt_file: Union[str, Path], **kwargs):\n",
    "    \"\"\"Placeholder function to evaluate TextVQA/DocVQA predictions using ANLS.\n",
    "\n",
    "    Args:\n",
    "        preds_file: Path to the JSON prediction file (format depends on benchmark, often list of dicts with id/pred).\n",
    "        gt_file: Path to the ground truth annotation file (e.g., TextVQA JSON format).\n",
    "        **kwargs: Additional arguments for ANLS calculation (e.g., case sensitivity).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing ANLS score (e.g., {'anls': score}).\n",
    "    \"\"\"\n",
    "    print(f\"--- TextVQA/DocVQA ANLS Evaluation (Placeholder) ---\")\n",
    "    print(f\"Predictions file: {preds_file}\")\n",
    "    print(f\"Ground Truth file: {gt_file}\")\n",
    "    \n",
    "    # --- TODO: Implement actual ANLS calculation (Step 5.6) --- \n",
    "    # 1. Load predictions and ground truth data.\n",
    "    # 2. Implement the ANLS metric calculation.\n",
    "    #    Reference: https://rrc.cvc.uab.es/?ch=17&com=tasks (DocVQA 2021 Task 3)\n",
    "    #    ANLS (Average Normalized Levenshtein Similarity) requires calculating Levenshtein distance\n",
    "    #    and normalizing it based on string lengths, averaged over the dataset.\n",
    "    #    Consider using existing libraries or implementing the formula carefully.\n",
    "    # Example structure:\n",
    "    # try:\n",
    "    #     with open(preds_file, 'r') as f_pred, open(gt_file, 'r') as f_gt:\n",
    "    #         predictions = json.load(f_pred) # Or load line by line if jsonl\n",
    "    #         ground_truth = json.load(f_gt)\n",
    "    #     \n",
    "    #     # Preprocess/align predictions and ground truth based on IDs\n",
    "    #     gt_dict = {item['questionId']: item['answers'] for item in ground_truth['data']}\n",
    "    #     pred_dict = {item['id']: item['prediction'] for item in predictions} # Match ID key\n",
    "    #     \n",
    "    #     total_anls = 0\n",
    "    #     count = 0\n",
    "    #     for qid, prediction in pred_dict.items():\n",
    "    #         if str(qid) in gt_dict: # Ensure ID matching (might need type conversion)\n",
    "    #             gt_answers = gt_dict[str(qid)]\n",
    "    #             # Calculate ANLS for this sample (max over multiple GT answers)\n",
    "    #             sample_anls = calculate_single_anls(prediction, gt_answers)\n",
    "    #             total_anls += sample_anls\n",
    "    #             count += 1\n",
    "    #             \n",
    "    #     final_anls = total_anls / count if count > 0 else 0\n",
    "    #     results = {\"anls\": final_anls}\n",
    "    #     print(f\"Calculated ANLS: {final_anls:.4f}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error during ANLS calculation: {e}\")\n",
    "    #     results = {\"anls\": 0.0} # Return dummy score\n",
    "    # -------------------------------------------------------------\n",
    "\n",
    "    print(\"Actual ANLS calculation logic not yet implemented.\")\n",
    "    dummy_anls = 0.4 # Placeholder score\n",
    "    results = {\"anls\": dummy_anls} # Placeholder result\n",
    "\n",
    "    # Log metric to W&B if active\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\"eval/anls\": results.get(\"anls\", 0.0)})\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Helper function placeholder for ANLS calculation (to be implemented in Step 5.6)\n",
    "# def calculate_single_anls(prediction: str, ground_truths: List[str]) -> float:\n",
    "#     \"\"\"Calculates the ANLS score for a single prediction against multiple ground truths.\"\"\"\n",
    "#     # TODO: Implement Levenshtein distance and ANLS formula\n",
    "#     return 0.0 # Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_evaluation(config_path: Union[str, Path], \n",
    "                   model_checkpoint_path: Optional[Union[str, Path]] = None, \n",
    "                   dataset_name: str = 'vqav2_test', # Example default\n",
    "                   output_filename: Optional[str] = None,\n",
    "                   **kwargs # Allow passing generation args like temp, top_p\n",
    "                   ):\n",
    "    \"\"\"Runs the evaluation pipeline: load model, generate predictions, evaluate.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the main configuration YAML.\n",
    "        model_checkpoint_path: Path to the specific model checkpoint base name/directory to load \n",
    "                               (e.g., '/path/to/output/models/stage2_llava_lora'). \n",
    "                               If None, uses the path derived from config['paths']['stage2_model_weights'].\n",
    "                               Expects associated files like '_projector_final.pth' and '_lora_adapters'/'_full.pth'.\n",
    "        dataset_name: Name of the dataset split to evaluate (e.g., 'vqav2_test', 'textvqa_val').\n",
    "                      This key should exist in `config['paths']` pointing to dataset info.\n",
    "        output_filename: Name for the prediction output file (defaults based on model/dataset).\n",
    "        **kwargs: Additional arguments passed to `generate_predictions` (e.g., temperature, top_p).\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Evaluation Run --- \")\n",
    "    print(f\"Config: {config_path}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    \n",
    "    config = load_config(config_path)\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    eval_output_dir = output_dir / 'eval_results' / dataset_name\n",
    "    eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    models_dir = output_dir / 'models' # Where models are saved\n",
    "    \n",
    "    learner = None\n",
    "    run = None # Initialize wandb run object\n",
    "    try:\n",
    "        # --- Initialize W&B --- \n",
    "        if config.get('logging', {}).get('wandb', {}).get('enabled', False):\n",
    "             model_id_for_run = Path(model_checkpoint_path or config['paths']['stage2_model_weights']).stem\n",
    "             run_name = f\"eval_{model_id_for_run}_{dataset_name}_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "             run = init_wandb(config, job_type=\"evaluation\", run_name=run_name)\n",
    "        \n",
    "        # 1. Determine Model Path Base Name\n",
    "        if model_checkpoint_path is None:\n",
    "            model_base_name = Path(config['paths']['stage2_model_weights']).stem\n",
    "            model_load_base = models_dir / model_base_name\n",
    "            print(f\"Using model base name from config: {model_base_name}\")\n",
    "        else:\n",
    "            model_checkpoint_path = Path(model_checkpoint_path)\n",
    "            model_base_name = model_checkpoint_path.stem \n",
    "            model_load_base = model_checkpoint_path \n",
    "            print(f\"Using provided model base path: {model_load_base}\")\n",
    "        \n",
    "        # 2. Load Model Components Manually\n",
    "        print(\"Loading model components for evaluation...\")\n",
    "        model = BaselineLLaVAModel(config=config) \n",
    "        \n",
    "        # Load projector weights\n",
    "        proj_weights_path = model_load_base.parent / (model_load_base.name + \"_projector_final.pth\")\n",
    "        if proj_weights_path.exists():\n",
    "            model.projector.load_state_dict(torch.load(proj_weights_path, map_location='cpu'))\n",
    "            print(f\"Loaded projector weights from {proj_weights_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Projector weights not found at {proj_weights_path}. Using initial weights.\")\n",
    "            \n",
    "        # Load LoRA adapters or full LLM weights\n",
    "        use_lora_config = config.get('model', {}).get('peft', {}).get('use_lora', False)\n",
    "        lora_adapter_path = model_load_base.parent / (model_load_base.name + \"_lora_adapters\")\n",
    "\n",
    "        if use_lora_config:\n",
    "            if lora_adapter_path.exists() and _peft_available and isinstance(model.language_model, PeftModel):\n",
    "                print(f\"Loading LoRA adapters from {lora_adapter_path}\")\n",
    "                try:\n",
    "                    if hasattr(model.language_model, 'load_adapter'):\n",
    "                         model.language_model.load_adapter(str(lora_adapter_path), adapter_name=\"default\")\n",
    "                         print(\"LoRA adapters loaded successfully.\")\n",
    "                    else:\n",
    "                         print(\"Warning: Model's language_model does not have 'load_adapter' method. PEFT setup might be incorrect.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading LoRA adapters: {e}. Model LLM may not be correctly wrapped or adapters mismatch.\")\n",
    "            else:\n",
    "                print(f\"Warning: LoRA enabled in config but adapters not found at {lora_adapter_path} or PEFT issue.\")\n",
    "        else:\n",
    "             print(\"LoRA not used. Assuming full LLM fine-tuning.\")\n",
    "             print(\"Warning: Loading full fine-tuned LLM state is not fully implemented here yet. Model may use base weights.\")\n",
    "             \n",
    "        # 3. Create Learner Shell for Convenience\n",
    "        dummy_dl = DataLoader(list(range(1)), bs=1)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        dummy_dls = DataLoaders(dummy_dl, dummy_dl) \n",
    "        dummy_dls.device = device \n",
    "        learner = Learner(dls=dummy_dls, model=model, loss_func=lambda x,y: 0)\n",
    "        \n",
    "        global tokenizer \n",
    "        if tokenizer is None:\n",
    "             raise RuntimeError(\"Tokenizer not loaded, cannot proceed with evaluation.\")\n",
    "        learner.tokenizer = tokenizer \n",
    "        print(f\"Minimal Learner shell created. Model moved to {device}.\")\n",
    "        \n",
    "        # 4. Load Test Data\n",
    "        print(f\"Loading test data for {dataset_name}...\")\n",
    "        if LLaVADataBlockStage2 is None:\n",
    "             raise RuntimeError(\"LLaVADataBlockStage2 is not defined. Cannot load test data.\")\n",
    "        test_dl = get_test_dataloader(config, dataset_name, dblock=LLaVADataBlockStage2)\n",
    "        if test_dl is None:\n",
    "            raise RuntimeError(f\"Failed to load test dataloader for {dataset_name}\")\n",
    "\n",
    "        # 5. Generate Predictions\n",
    "        if output_filename is None:\n",
    "            output_filename = f\"preds_{model_base_name}_{dataset_name}.jsonl\"\n",
    "        preds_file = eval_output_dir / output_filename\n",
    "        \n",
    "        gen_kwargs = {k: v for k, v in kwargs.items() if k in ['max_len', 'temperature', 'top_p']}\n",
    "        \n",
    "        generate_predictions(learner, test_dl, preds_file, **gen_kwargs)\n",
    "\n",
    "        # 6. Run Evaluation Metrics\n",
    "        print(\"Running evaluation metrics...\")\n",
    "        gt_config = config['paths'].get(dataset_name)\n",
    "        if not gt_config or 'annotations' not in gt_config:\n",
    "            print(f\"Warning: Ground truth annotation path not found for '{dataset_name}' in config. Cannot run metrics.\")\n",
    "        else:\n",
    "            gt_file_path = Path(config['paths']['data_base']) / gt_config['annotations']\n",
    "            # Check for the questions file path, especially for VQA\n",
    "            ques_file_path = None\n",
    "            if 'questions' in gt_config:\n",
    "                 ques_file_path = Path(config['paths']['data_base']) / gt_config['questions']\n",
    "            elif 'vqav2' in dataset_name.lower(): # Infer default VQA questions file path if not specified\n",
    "                 ann_path_parts = list(gt_file_path.parts)\n",
    "                 if 'annotations' in ann_path_parts[-1]:\n",
    "                      ques_filename = gt_file_path.name.replace('annotations', 'questions').replace('v2_mscoco', 'v2_OpenEnded_mscoco')\n",
    "                      ques_file_path_default = gt_file_path.parent / ques_filename\n",
    "                      if ques_file_path_default.exists():\n",
    "                           ques_file_path = ques_file_path_default\n",
    "                           print(f\"Inferred VQA questions file path: {ques_file_path}\")\n",
    "            \n",
    "            if not gt_file_path.exists():\n",
    "                 print(f\"Warning: Ground truth file not found at {gt_file_path}. Cannot run metrics.\")\n",
    "            else:\n",
    "                 if 'vqav2' in dataset_name.lower():\n",
    "                     if ques_file_path and ques_file_path.exists():\n",
    "                         vqa_results = evaluate_vqa(preds_file, gt_file_path, ques_file=ques_file_path)\n",
    "                         print(f\"VQAv2 Results: {vqa_results}\")\n",
    "                     else:\n",
    "                          print(f\"Warning: VQA questions file not found or specified ({ques_file_path}). Cannot run VQA evaluation.\")\n",
    "                 elif 'textvqa' in dataset_name.lower():\n",
    "                     anls_results = evaluate_textvqa(preds_file, gt_file_path)\n",
    "                     print(f\"TextVQA ANLS Results: {anls_results}\")\n",
    "                 # --- Add more evaluation cases here --- #\n",
    "                 else:\n",
    "                     print(f\"No specific evaluation script configured for dataset: {dataset_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.finish(exit_code=1)\n",
    "    finally:\n",
    "        # Clean up memory\n",
    "        if learner is not None and hasattr(learner, 'model') and learner.model is not None:\n",
    "            learner.model.to('cpu')\n",
    "            del learner.model\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Cleaned up model memory.\")\n",
    "        # Finish W&B run if active and not already finished due to error\n",
    "        if run and wandb.run and wandb.run.id == run.id:\n",
    "             run.finish()\n",
    "             print(\"Finished W&B run.\")\n",
    "            \n",
    "    print(f\"--- Evaluation Run Complete --- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/40_evaluation.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "llava"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
