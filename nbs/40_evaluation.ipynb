{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> Functions for evaluating LLaVA models, including prediction generation and metric calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Union, Optional, Dict, Any # Added Dict, Any\n",
    "import wandb # Added wandb import\n",
    "\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.load import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root\n",
    "project_root = Path(os.getcwd())\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")\n",
    "    \n",
    "# Import necessary llava components\n",
    "try:\n",
    "    from llava.utils import load_config, init_wandb # Added init_wandb\n",
    "    from llava.data.loading import get_test_dataloader, LLaVADataBlockStage2 # Assumes this function exists in 10_data_loading\n",
    "    from llava.model.baseline import BaselineLLaVAModel # Or AdaptiveLLaVAModel later\n",
    "    from llava.data.preprocessing import tokenizer, DEFAULT_IMAGE_TOKEN, IGNORE_INDEX, IMAGE_TOKEN_INDEX_PLACEHOLDER # Import tokenizer & constants\n",
    "    # Import PEFT related things if needed for model loading check\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        _peft_available = True\n",
    "    except ImportError:\n",
    "        PeftModel = None \n",
    "        _peft_available = False\n",
    "    # from llava.training.stage2 import get_stage2_learner # Maybe needed for loading full learner state\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing llava modules: {e}. Make sure nbdev_export was run.\")\n",
    "    # Define placeholders if running standalone or during initial setup\n",
    "    def load_config(path): return {}\n",
    "    def init_wandb(*args, **kwargs): pass\n",
    "    def get_test_dataloader(config, dataset_name, dblock=None): raise NotImplementedError\n",
    "    class BaselineLLaVAModel(torch.nn.Module): \n",
    "        def __init__(self, *args, **kwargs): \n",
    "            super().__init__()\n",
    "            self.dummy = torch.nn.Linear(1,1)\n",
    "            self.image_token_index_marker = -200 # Define necessary attributes\n",
    "            self.projector = torch.nn.Linear(10,10)\n",
    "            self.language_model = self # Make model act like LLM for generate\n",
    "            self.vision_tower = self # Dummy vision tower\n",
    "            self.config = {} # Dummy config\n",
    "        def encode_image(self, *args, **kwargs): return torch.randn(1,5,10) # Dummy image features B, P, D\n",
    "        def get_input_embeddings(self): return torch.nn.Embedding(100, 10)\n",
    "        def forward(self, *args, **kwargs): return CausalLMOutputWithPast(logits=torch.randn(1, 10, 100))\n",
    "        def generate(self, *args, **kwargs): return torch.randint(0, 100, (1, kwargs.get('max_new_tokens', 10)))\n",
    "        def to(self, *args, **kwargs): return self # Avoid device errors in dummy\n",
    "        def eval(self): pass\n",
    "    class DummyTokenizer:\n",
    "         def __init__(self): self.eos_token_id=1; self.pad_token_id=0\n",
    "         def decode(self, *args, **kwargs): return \"dummy decoded text\"\n",
    "    tokenizer = DummyTokenizer()\n",
    "    DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "    IGNORE_INDEX = -100\n",
    "    IMAGE_TOKEN_INDEX_PLACEHOLDER = -200\n",
    "    LLaVADataBlockStage2 = None # Placeholder\n",
    "    PeftModel = None\n",
    "    _peft_available = False\n",
    "    # def get_stage2_learner(config): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Prediction Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a trained fastai `Learner` and a test `DataLoader`, generates predictions, decodes them into text, and saves them to a JSON file. It now also includes efficiency logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_predictions(learner: Learner, \n",
    "                         test_dl: DataLoader, \n",
    "                         output_file: Union[str, Path], \n",
    "                         max_len: int = 200, # Max generation length\n",
    "                         temperature: float = 0.2, # Generation temperature\n",
    "                         top_p: Optional[float] = None, # Nucleus sampling top_p\n",
    "                        ):\n",
    "    \"\"\"Generates predictions for a test dataloader and saves them to a JSON Lines file.\n",
    "    Includes efficiency logging (latency, peak VRAM).\n",
    "\n",
    "    Uses the HF generate method on the underlying language model component,\n",
    "    manually preparing the combined image and text embeddings.\n",
    "\n",
    "    Args:\n",
    "        learner: Trained fastai Learner object containing the model.\n",
    "                 Expected to have `learner.model`, `learner.dls.device`,\n",
    "                 and potentially `learner.tokenizer` or `learner.dls.tokenizer`.\n",
    "        test_dl: DataLoader for the test set.\n",
    "        output_file: Path to save the JSON Lines prediction file.\n",
    "        max_len: Maximum number of new tokens to generate.\n",
    "        temperature: Sampling temperature for generation. Set to 0 for greedy decoding.\n",
    "        top_p: If set (and temperature > 0), use nucleus sampling with this top_p value.\n",
    "    \"\"\"\n",
    "    output_file = Path(output_file)\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model = learner.model\n",
    "    # Get tokenizer - check learner, then dls, then global scope\n",
    "    if hasattr(learner, 'tokenizer') and learner.tokenizer is not None:\n",
    "        tok = learner.tokenizer\n",
    "    elif hasattr(learner.dls, 'tokenizer') and learner.dls.tokenizer is not None:\n",
    "        tok = learner.dls.tokenizer\n",
    "    else:\n",
    "        global tokenizer # Use the global tokenizer imported/defined earlier\n",
    "        if tokenizer is None:\n",
    "             raise AttributeError(\"Tokenizer not found in learner, dls, or globally. Cannot decode predictions.\")\n",
    "        print(\"Warning: Using global tokenizer instance for generation.\")\n",
    "        tok = tokenizer\n",
    "        \n",
    "    # Ensure model components exist\n",
    "    if not all(hasattr(model, attr) and getattr(model, attr) is not None \n",
    "               for attr in ['vision_tower', 'projector', 'language_model', 'get_input_embeddings', 'encode_image']):\n",
    "        raise AttributeError(\"Model is missing required components (vision_tower, projector, language_model, etc.)\")\n",
    "        \n",
    "    model.eval() # Set model to evaluation mode\n",
    "    results = []\n",
    "    total_time = 0\n",
    "    num_samples = 0\n",
    "    device = learner.dls.device # Get device from learner\n",
    "\n",
    "    print(f\"Generating predictions for {len(test_dl.dataset)} samples...\")\n",
    "    print(f\"Saving predictions to: {output_file}\")\n",
    "    \n",
    "    # Reset CUDA memory stats before generation loop for accurate peak measurement\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats(device=device) \n",
    "        print(\"Reset CUDA peak memory stats before generation.\")\n",
    "\n",
    "    # Use context manager for file writing\n",
    "    with torch.no_grad(), open(output_file, 'w') as f_out:\n",
    "        # Iterate through batches\n",
    "        for batch in tqdm(test_dl, desc=\"Generating Predictions\"):\n",
    "            start_time = time.time() # Start timing for this batch\n",
    "            \n",
    "            # Move batch items to appropriate device\n",
    "            if not isinstance(batch, dict):\n",
    "                 print(f\"Warning: Expected batch to be a dict, got {type(batch)}. Attempting to proceed assuming basic structure.\")\n",
    "                 pixel_values = batch[0].to(device)\n",
    "                 input_ids = batch[1].to(device)\n",
    "                 batch_size = pixel_values.shape[0]\n",
    "            else:\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device) \n",
    "                batch_size = pixel_values.shape[0]\n",
    "\n",
    "            # --- Generation using underlying HF LLM --- \n",
    "            # 1. Encode images and project features\n",
    "            image_features = model.encode_image(pixel_values) # (B, P, D_clip)\n",
    "            if image_features is None: \n",
    "                print(\"Warning: Image encoding failed for batch. Skipping.\")\n",
    "                num_samples += batch_size # Account for skipped samples in latency calculation\n",
    "                total_time += (time.time() - start_time) # Add time spent\n",
    "                continue # Skip this batch\n",
    "            projected_features = model.projector(image_features) # (B, P, D_llm)\n",
    "            \n",
    "            outputs_list = []\n",
    "            # Process each sample in the batch individually for embedding preparation\n",
    "            for i in range(batch_size):\n",
    "                current_input_ids = input_ids[i:i+1] \n",
    "                current_proj_features = projected_features[i:i+1]\n",
    "                \n",
    "                marker = getattr(model, 'image_token_index_marker', IMAGE_TOKEN_INDEX_PLACEHOLDER)\n",
    "                image_token_indices = torch.where(current_input_ids[0] == marker)[0]\n",
    "                \n",
    "                if len(image_token_indices) == 0:\n",
    "                    print(f\"Warning: Image token marker {marker} not found in sample {num_samples + i}. Skipping generation.\")\n",
    "                    outputs_list.append(torch.tensor([tok.eos_token_id], device=device)) \n",
    "                    continue\n",
    "                \n",
    "                image_token_start_index = image_token_indices[0].item()\n",
    "\n",
    "                # Prepare prompt embeddings\n",
    "                input_ids_no_marker = current_input_ids.clone()\n",
    "                input_ids_no_marker[input_ids_no_marker == marker] = 0 \n",
    "                text_embeddings = model.get_input_embeddings()(input_ids_no_marker)\n",
    "                \n",
    "                text_emb_before = text_embeddings[:, :image_token_start_index]\n",
    "                text_emb_after = text_embeddings[:, image_token_start_index + 1:]\n",
    "                \n",
    "                prompt_embeds = torch.cat([\n",
    "                    text_emb_before,\n",
    "                    current_proj_features.to(text_embeddings.device, dtype=text_embeddings.dtype),\n",
    "                    text_emb_after\n",
    "                ], dim=1)\n",
    "                \n",
    "                # Generate using the underlying LLM\n",
    "                llm_component = model.language_model\n",
    "                if _peft_available and isinstance(llm_component, PeftModel):\n",
    "                     llm_component = llm_component.base_model.model \n",
    "                \n",
    "                prompt_attention_mask = torch.ones(prompt_embeds.shape[:2], dtype=torch.long, device=device)\n",
    "                \n",
    "                try:\n",
    "                    gen_params = {\n",
    "                        \"inputs_embeds\": prompt_embeds,\n",
    "                        \"attention_mask\": prompt_attention_mask,\n",
    "                        \"max_new_tokens\": max_len,\n",
    "                        \"eos_token_id\": tok.eos_token_id,\n",
    "                        \"pad_token_id\": tok.pad_token_id if tok.pad_token_id is not None else tok.eos_token_id,\n",
    "                        \"do_sample\": (temperature > 0),\n",
    "                        \"num_beams\": 1\n",
    "                    }\n",
    "                    if temperature > 0:\n",
    "                        gen_params[\"temperature\"] = temperature\n",
    "                        if top_p is not None:\n",
    "                            gen_params[\"top_p\"] = top_p\n",
    "                    else: \n",
    "                        gen_params[\"temperature\"] = 1.0 \n",
    "                        gen_params[\"do_sample\"] = False\n",
    "\n",
    "                    output_ids_gen = llm_component.generate(**gen_params)\n",
    "                    \n",
    "                    output_ids_gen = output_ids_gen[:, prompt_embeds.shape[1]:]\n",
    "                    outputs_list.append(output_ids_gen[0]) \n",
    "                except Exception as gen_e:\n",
    "                     print(f\"Error during generation for sample {num_samples + i}: {gen_e}\")\n",
    "                     outputs_list.append(torch.tensor([tok.eos_token_id], device=device)) # Fallback\n",
    "            # --- End Generation for Batch --- #\n",
    "\n",
    "            end_time = time.time() # End timing for this batch\n",
    "            total_time += (end_time - start_time)\n",
    "            num_samples_in_batch = batch_size\n",
    "\n",
    "            # --- Decode and Save Results for the Batch --- \n",
    "            # Attempt to get item IDs (might need adjustment based on Dataset class)\n",
    "            start_ds_idx = test_dl.num_workers * test_dl.offs if hasattr(test_dl, 'offs') else num_samples # Estimate start index\n",
    "            item_ids = []\n",
    "            for i in range(num_samples_in_batch):\n",
    "                current_item_idx = start_ds_idx + i\n",
    "                item_id = f\"sample_{current_item_idx}\" # Default ID\n",
    "                try:\n",
    "                    # Try accessing dataset item directly - this is often the most reliable\n",
    "                    if hasattr(test_dl, 'dataset') and current_item_idx < len(test_dl.dataset):\n",
    "                        item_data = test_dl.dataset[current_item_idx] \n",
    "                        # Adapt based on what test_dl.dataset[idx] returns\n",
    "                        # If it's the LLaVASample object:\n",
    "                        if hasattr(item_data, 'sample_id'):\n",
    "                             item_id = item_data.sample_id\n",
    "                        # If it's a tuple from DataBlock (e.g., (image_path, conversations)):\n",
    "                        # Need a way to link back to original ID, maybe store IDs in dataset?\n",
    "                        # For now, stick to default if direct access fails.\n",
    "                except Exception as id_err:\n",
    "                    # print(f\"Could not get sample ID for index {current_item_idx}: {id_err}\")\n",
    "                    pass # Use default ID\n",
    "                item_ids.append(item_id)\n",
    "                 \n",
    "            # Decode and write each result in the batch\n",
    "            for i, gen_ids in enumerate(outputs_list):\n",
    "                decoded_text = tok.decode(gen_ids.cpu(), skip_special_tokens=True).strip()\n",
    "                result_entry = {\n",
    "                    \"id\": item_ids[i], \n",
    "                    \"prediction\": decoded_text,\n",
    "                }\n",
    "                f_out.write(json.dumps(result_entry) + '\\n')\n",
    "                results.append(result_entry)\n",
    "            \n",
    "            num_samples += num_samples_in_batch # Update total sample count\n",
    "\n",
    "    # --- Log Efficiency Metrics (Step 5.4) --- \n",
    "    avg_latency = (total_time / num_samples) * 1000 if num_samples > 0 else 0\n",
    "    print(f\"Finished generation. Saved {len(results)} predictions to {output_file}\")\n",
    "    print(f\"Average inference latency: {avg_latency:.2f} ms/sample\")\n",
    "    \n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\"eval/avg_inference_latency_ms\": avg_latency})\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        peak_vram_gb = torch.cuda.max_memory_allocated(device=device) / (1024**3) \n",
    "        print(f\"Peak Inference VRAM used: {peak_vram_gb:.2f} GB\")\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\"eval/peak_inference_vram_gb\": peak_vram_gb})\n",
    "        # Reset peak stats after logging for this run\n",
    "        torch.cuda.reset_peak_memory_stats(device=device) \n",
    "        \n",
    "    return results # Return list of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3: Integrate External Evaluation Scripts (Placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define placeholder functions for calling external evaluation scripts. These will be implemented fully in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def evaluate_vqa(preds_file: Union[str, Path], gt_file: Union[str, Path], **kwargs):\n",
    "    \"\"\"Placeholder function to evaluate VQAv2 predictions.\n",
    "    \n",
    "    Args:\n",
    "        preds_file: Path to the JSON prediction file (expected format: [{'question_id': id, 'answer': prediction}]).\n",
    "        gt_file: Path to the ground truth annotation file (e.g., VQA v2 format).\n",
    "        **kwargs: Additional arguments for the VQA eval API (e.g., version).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing VQA scores (e.g., {'overall': score}).\n",
    "    \"\"\"\n",
    "    print(f\"--- VQA Evaluation (Placeholder) ---\")\n",
    "    print(f\"Predictions file: {preds_file}\")\n",
    "    print(f\"Ground Truth file: {gt_file}\")\n",
    "    \n",
    "    # --- TODO: Implement actual VQA evaluation logic (Step 5.5) --- \n",
    "    # 1. Ensure predictions file is in the correct format for the official VQA eval tool.\n",
    "    # 2. Import or call the official VQA evaluation script/library.\n",
    "    #    (Requires downloading the VQA evaluation tools: https://visualqa.org/evaluation.html)\n",
    "    # Example structure:\n",
    "    # try:\n",
    "    #     from vqa_eval_tools.vqa import VQA\n",
    "    #     from vqa_eval_tools.vqaEval import VQAEval\n",
    "    #     \n",
    "    #     vqa_ann = VQA(gt_file, None) # Load annotations\n",
    "    #     vqa_pred = vqa_ann.loadRes(preds_file, gt_file) # Load predictions\n",
    "    #     \n",
    "    #     vqa_eval = VQAEval(vqa_ann, vqa_pred, n=2) # n=2 is standard for VQA\n",
    "    #     vqa_eval.evaluate()\n",
    "    #     \n",
    "    #     results = {\n",
    "    #         \"overall\": vqa_eval.accuracy['overall'],\n",
    "    #         \"yes/no\": vqa_eval.accuracy['perAnswerType']['yes/no'],\n",
    "    #         \"number\": vqa_eval.accuracy['perAnswerType']['number'],\n",
    "    #         \"other\": vqa_eval.accuracy['perAnswerType']['other'],\n",
    "    #     }\n",
    "    #     print(f\"Calculated VQA Accuracy: {results}\")\n",
    "    # except ImportError:\n",
    "    #     print(\"Error: VQA evaluation tools not found. Please install them.\")\n",
    "    #     results = {\"overall\": 0.0} # Return dummy score\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error during VQA evaluation: {e}\")\n",
    "    #     results = {\"overall\": 0.0} # Return dummy score\n",
    "    # -------------------------------------------------------------\n",
    "    \n",
    "    print(\"Actual VQA evaluation logic not yet implemented.\")\n",
    "    dummy_score = 0.5 # Placeholder score\n",
    "    results = {\"overall\": dummy_score} # Placeholder result\n",
    "\n",
    "    # Log metric to W&B if active\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\"eval/vqa_score_overall\": results.get(\"overall\", 0.0)})\n",
    "        # Log other scores if needed\n",
    "        # wandb.log({\"eval/vqa_score_yes_no\": results.get(\"yes/no\", 0.0)})\n",
    "        # wandb.log({\"eval/vqa_score_number\": results.get(\"number\", 0.0)})\n",
    "        # wandb.log({\"eval/vqa_score_other\": results.get(\"other\", 0.0)})\n",
    "        \n",
    "    return results\n",
    "\n",
    "#| export\n",
    "def evaluate_textvqa(preds_file: Union[str, Path], gt_file: Union[str, Path], **kwargs):\n",
    "    \"\"\"Placeholder function to evaluate TextVQA/DocVQA predictions using ANLS.\n",
    "\n",
    "    Args:\n",
    "        preds_file: Path to the JSON prediction file (format depends on benchmark, often list of dicts with id/pred).\n",
    "        gt_file: Path to the ground truth annotation file (e.g., TextVQA JSON format).\n",
    "        **kwargs: Additional arguments for ANLS calculation (e.g., case sensitivity).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing ANLS score (e.g., {'anls': score}).\n",
    "    \"\"\"\n",
    "    print(f\"--- TextVQA/DocVQA ANLS Evaluation (Placeholder) ---\")\n",
    "    print(f\"Predictions file: {preds_file}\")\n",
    "    print(f\"Ground Truth file: {gt_file}\")\n",
    "    \n",
    "    # --- TODO: Implement actual ANLS calculation (Step 5.6) --- \n",
    "    # 1. Load predictions and ground truth data.\n",
    "    # 2. Implement the ANLS metric calculation.\n",
    "    #    Reference: https://rrc.cvc.uab.es/?ch=17&com=tasks (DocVQA 2021 Task 3)\n",
    "    #    ANLS (Average Normalized Levenshtein Similarity) requires calculating Levenshtein distance\n",
    "    #    and normalizing it based on string lengths, averaged over the dataset.\n",
    "    #    Consider using existing libraries or implementing the formula carefully.\n",
    "    # Example structure:\n",
    "    # try:\n",
    "    #     with open(preds_file, 'r') as f_pred, open(gt_file, 'r') as f_gt:\n",
    "    #         predictions = json.load(f_pred) # Or load line by line if jsonl\n",
    "    #         ground_truth = json.load(f_gt)\n",
    "    #     \n",
    "    #     # Preprocess/align predictions and ground truth based on IDs\n",
    "    #     gt_dict = {item['questionId']: item['answers'] for item in ground_truth['data']}\n",
    "    #     pred_dict = {item['id']: item['prediction'] for item in predictions} # Match ID key\n",
    "    #     \n",
    "    #     total_anls = 0\n",
    "    #     count = 0\n",
    "    #     for qid, prediction in pred_dict.items():\n",
    "    #         if str(qid) in gt_dict: # Ensure ID matching (might need type conversion)\n",
    "    #             gt_answers = gt_dict[str(qid)]\n",
    "    #             # Calculate ANLS for this sample (max over multiple GT answers)\n",
    "    #             sample_anls = calculate_single_anls(prediction, gt_answers)\n",
    "    #             total_anls += sample_anls\n",
    "    #             count += 1\n",
    "    #             \n",
    "    #     final_anls = total_anls / count if count > 0 else 0\n",
    "    #     results = {\"anls\": final_anls}\n",
    "    #     print(f\"Calculated ANLS: {final_anls:.4f}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error during ANLS calculation: {e}\")\n",
    "    #     results = {\"anls\": 0.0} # Return dummy score\n",
    "    # -------------------------------------------------------------\n",
    "\n",
    "    print(\"Actual ANLS calculation logic not yet implemented.\")\n",
    "    dummy_anls = 0.4 # Placeholder score\n",
    "    results = {\"anls\": dummy_anls} # Placeholder result\n",
    "\n",
    "    # Log metric to W&B if active\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\"eval/anls\": results.get(\"anls\", 0.0)})\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Helper function placeholder for ANLS calculation (to be implemented in Step 5.6)\n",
    "# def calculate_single_anls(prediction: str, ground_truths: List[str]) -> float:\n",
    "#     \"\"\"Calculates the ANLS score for a single prediction against multiple ground truths.\"\"\"\n",
    "#     # TODO: Implement Levenshtein distance and ANLS formula\n",
    "#     return 0.0 # Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Script (Placeholder Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_evaluation(config_path: Union[str, Path], \n",
    "                   model_checkpoint_path: Optional[Union[str, Path]] = None, \n",
    "                   dataset_name: str = 'vqav2_test', # Example default\n",
    "                   output_filename: Optional[str] = None,\n",
    "                   **kwargs # Allow passing generation args like temp, top_p\n",
    "                   ):\n",
    "    \"\"\"Runs the evaluation pipeline: load model, generate predictions, evaluate.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the main configuration YAML.\n",
    "        model_checkpoint_path: Path to the specific model checkpoint base name/directory to load \n",
    "                               (e.g., '/path/to/output/models/stage2_llava_lora'). \n",
    "                               If None, uses the path derived from config['paths']['stage2_model_weights'].\n",
    "                               Expects associated files like '_projector_final.pth' and '_lora_adapters'/'_full.pth'.\n",
    "        dataset_name: Name of the dataset split to evaluate (e.g., 'vqav2_test', 'textvqa_val').\n",
    "                      This key should exist in `config['paths']` pointing to dataset info.\n",
    "        output_filename: Name for the prediction output file (defaults based on model/dataset).\n",
    "        **kwargs: Additional arguments passed to `generate_predictions` (e.g., temperature, top_p).\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Evaluation Run --- \")\n",
    "    print(f\"Config: {config_path}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    \n",
    "    config = load_config(config_path)\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    eval_output_dir = output_dir / 'eval_results' / dataset_name\n",
    "    eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    models_dir = output_dir / 'models' # Where models are saved\n",
    "    \n",
    "    learner = None\n",
    "    run = None # Initialize wandb run object\n",
    "    try:\n",
    "        # --- Initialize W&B --- \n",
    "        # Modified: Init W&B here to capture VRAM/Latency logs from generate_predictions\n",
    "        if config.get('logging', {}).get('wandb', {}).get('enabled', False):\n",
    "             # Create a unique run name for evaluation\n",
    "             model_id_for_run = Path(model_checkpoint_path or config['paths']['stage2_model_weights']).stem\n",
    "             run_name = f\"eval_{model_id_for_run}_{dataset_name}_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "             run = init_wandb(config, job_type=\"evaluation\", run_name=run_name)\n",
    "        \n",
    "        # 1. Determine Model Path Base Name\n",
    "        if model_checkpoint_path is None:\n",
    "            model_base_name = Path(config['paths']['stage2_model_weights']).stem\n",
    "            model_load_base = models_dir / model_base_name\n",
    "            print(f\"Using model base name from config: {model_base_name}\")\n",
    "        else:\n",
    "            # If a specific path is given, use its stem as the base name\n",
    "            model_checkpoint_path = Path(model_checkpoint_path)\n",
    "            model_base_name = model_checkpoint_path.stem \n",
    "            # Assume the path points to the base (e.g., stage2_llava_lora), not the specific file\n",
    "            model_load_base = model_checkpoint_path \n",
    "            print(f\"Using provided model base path: {model_load_base}\")\n",
    "        \n",
    "        # 2. Load Model Components Manually\n",
    "        print(\"Loading model components for evaluation...\")\n",
    "        # Pass the config to the model constructor\n",
    "        model = BaselineLLaVAModel(config=config) \n",
    "        \n",
    "        # Load projector weights\n",
    "        proj_weights_path = model_load_base.parent / (model_load_base.name + \"_projector_final.pth\")\n",
    "        if proj_weights_path.exists():\n",
    "            model.projector.load_state_dict(torch.load(proj_weights_path, map_location='cpu'))\n",
    "            print(f\"Loaded projector weights from {proj_weights_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Projector weights not found at {proj_weights_path}. Using initial weights.\")\n",
    "            \n",
    "        # Load LoRA adapters or full LLM weights\n",
    "        use_lora_config = config.get('model', {}).get('peft', {}).get('use_lora', False)\n",
    "        lora_adapter_path = model_load_base.parent / (model_load_base.name + \"_lora_adapters\")\n",
    "        # full_model_path = model_load_base.parent / (model_load_base.name + \".pth\") # Example for full save\n",
    "\n",
    "        if use_lora_config:\n",
    "            if lora_adapter_path.exists() and _peft_available and isinstance(model.language_model, PeftModel):\n",
    "                print(f\"Loading LoRA adapters from {lora_adapter_path}\")\n",
    "                # Ensure the adapter is loaded correctly. `load_adapter` might add a new one.\n",
    "                # If `get_peft_model` was called during init, we might need to ensure the base model is loaded correctly first.\n",
    "                # Safest is often to load the base model, then apply PEFT wrapper, then load adapters.\n",
    "                # Assuming BaselineLLaVAModel's init already called get_peft_model:\n",
    "                try:\n",
    "                    # Ensure the PeftModel is ready\n",
    "                    if hasattr(model.language_model, 'load_adapter'):\n",
    "                         model.language_model.load_adapter(str(lora_adapter_path), adapter_name=\"default\")\n",
    "                         print(\"LoRA adapters loaded successfully.\")\n",
    "                    else:\n",
    "                         print(\"Warning: Model's language_model does not have 'load_adapter' method. PEFT setup might be incorrect.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading LoRA adapters: {e}. Model LLM may not be correctly wrapped or adapters mismatch.\")\n",
    "            else:\n",
    "                print(f\"Warning: LoRA enabled in config but adapters not found at {lora_adapter_path} or PEFT issue.\")\n",
    "        else:\n",
    "             print(\"LoRA not used. Assuming full LLM fine-tuning.\")\n",
    "             print(\"Warning: Loading full fine-tuned LLM state is not fully implemented here yet. Model may use base weights.\")\n",
    "             # TODO: Implement loading full model state if needed (e.g., from SaveModelCallback output)\n",
    "             # if full_model_path.exists():\n",
    "             #    learner_state = torch.load(full_model_path, map_location='cpu')\n",
    "             #    model.load_state_dict(learner_state['model'])\n",
    "             #    print(f\"Loaded full model state from {full_model_path}\")\n",
    "             \n",
    "        # 3. Create Learner Shell for Convenience\n",
    "        # Use a dummy dataloader just to hold the device context etc.\n",
    "        dummy_dl = DataLoader(list(range(1)), bs=1) # Minimal dataloader\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        # Create a dummy DataLoaders object to satisfy Learner init\n",
    "        dummy_dls = DataLoaders(dummy_dl, dummy_dl) \n",
    "        dummy_dls.device = device # Set device on DataLoaders\n",
    "        learner = Learner(dls=dummy_dls, model=model, loss_func=lambda x,y: 0) # Dummy loss\n",
    "        \n",
    "        # Manually assign tokenizer if generate_predictions needs it from learner\n",
    "        global tokenizer \n",
    "        if tokenizer is None:\n",
    "             raise RuntimeError(\"Tokenizer not loaded, cannot proceed with evaluation.\")\n",
    "        learner.tokenizer = tokenizer \n",
    "        print(f\"Minimal Learner shell created. Model moved to {device}.\")\n",
    "        \n",
    "        # 4. Load Test Data\n",
    "        print(f\"Loading test data for {dataset_name}...\")\n",
    "        # Need to instantiate the correct DataBlock (e.g., Stage 2 for VQA/TextVQA)\n",
    "        # Assuming Stage 2 structure is suitable for standard eval datasets\n",
    "        if LLaVADataBlockStage2 is None:\n",
    "             raise RuntimeError(\"LLaVADataBlockStage2 is not defined. Cannot load test data.\")\n",
    "        test_dl = get_test_dataloader(config, dataset_name, dblock=LLaVADataBlockStage2)\n",
    "        if test_dl is None:\n",
    "            raise RuntimeError(f\"Failed to load test dataloader for {dataset_name}\")\n",
    "\n",
    "        # 5. Generate Predictions\n",
    "        if output_filename is None:\n",
    "            # Use the resolved model base name for the output file\n",
    "            output_filename = f\"preds_{model_base_name}_{dataset_name}.jsonl\"\n",
    "        preds_file = eval_output_dir / output_filename\n",
    "        \n",
    "        # Get generation kwargs from **kwargs\n",
    "        gen_kwargs = {k: v for k, v in kwargs.items() if k in ['max_len', 'temperature', 'top_p']}\n",
    "        \n",
    "        # generate_predictions now handles latency/VRAM logging\n",
    "        generate_predictions(learner, test_dl, preds_file, **gen_kwargs)\n",
    "\n",
    "        # 6. Run Evaluation Metrics\n",
    "        print(\"Running evaluation metrics...\")\n",
    "        # Determine Ground Truth file path based on dataset_name\n",
    "        gt_config = config['paths'].get(dataset_name)\n",
    "        if not gt_config or 'annotations' not in gt_config:\n",
    "            print(f\"Warning: Ground truth annotation path not found for '{dataset_name}' in config. Cannot run metrics.\")\n",
    "        else:\n",
    "            gt_file_path = Path(config['paths']['data_base']) / gt_config['annotations']\n",
    "            if not gt_file_path.exists():\n",
    "                 print(f\"Warning: Ground truth file not found at {gt_file_path}. Cannot run metrics.\")\n",
    "            else:\n",
    "                 # Call appropriate evaluation function based on dataset name convention\n",
    "                 if 'vqav2' in dataset_name.lower():\n",
    "                     vqa_results = evaluate_vqa(preds_file, gt_file_path)\n",
    "                     print(f\"VQAv2 Results: {vqa_results}\")\n",
    "                 elif 'textvqa' in dataset_name.lower():\n",
    "                     anls_results = evaluate_textvqa(preds_file, gt_file_path)\n",
    "                     print(f\"TextVQA ANLS Results: {anls_results}\")\n",
    "                 # --- Add more evaluation cases here --- #\n",
    "                 # elif 'docvqa' in dataset_name.lower():\n",
    "                 #     docvqa_results = evaluate_textvqa(preds_file, gt_file_path) # Often uses ANLS too\n",
    "                 #     print(f\"DocVQA ANLS Results: {docvqa_results}\")\n",
    "                 # elif 'chartqa' in dataset_name.lower():\n",
    "                 #     # chartqa_results = evaluate_chartqa(preds_file, gt_file_path)\n",
    "                 #     print(\"ChartQA evaluation not implemented yet.\")\n",
    "                 # elif 'custom_eval' in dataset_name.lower():\n",
    "                 #     # custom_results = evaluate_custom(preds_file, gt_file_path)\n",
    "                 #     print(\"Custom eval set evaluation not implemented yet.\")\n",
    "                 else:\n",
    "                     print(f\"No specific evaluation script configured for dataset: {dataset_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Set exit code for W&B if error occurs\n",
    "        if run: run.finish(exit_code=1)\n",
    "    finally:\n",
    "        # Clean up memory\n",
    "        if learner is not None and hasattr(learner, 'model') and learner.model is not None:\n",
    "            learner.model.to('cpu')\n",
    "            del learner.model\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Cleaned up model memory.\")\n",
    "        # Finish W&B run if active and not already finished due to error\n",
    "        if run and wandb.run and wandb.run.id == run.id:\n",
    "             run.finish()\n",
    "             print(\"Finished W&B run.\")\n",
    "            \n",
    "    print(f\"--- Evaluation Run Complete --- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/40_evaluation.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "llava"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}