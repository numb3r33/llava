{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> Functions for evaluating LLaVA models, including prediction generation and metric calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Union, Optional, Dict, Any, List, Type # Added Dict, Any, List, Type\n",
    "import wandb # Added wandb import\n",
    "import argparse\n",
    "import PIL.Image # Added PIL\n",
    "\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.load import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "# Add dependency for Levenshtein distance calculation\n",
    "try:\n",
    "    import Levenshtein\n",
    "except ImportError:\n",
    "    print(\"Warning: `pip install python-Levenshtein` is required for ANLS evaluation.\")\n",
    "    Levenshtein = None\n",
    "\n",
    "# --- Project Root Setup --- \n",
    "project_root = Path(os.getcwd())\n",
    "# Check if running from nbs/\n",
    "if project_root.name == 'nbs' and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "# Check if running from scripts/\n",
    "elif project_root.name == 'scripts' and (project_root.parent / 'settings.ini').exists():\n",
    "     project_root = project_root.parent\n",
    "elif not (project_root / 'settings.ini').exists():\n",
    "     # Try going up one level if settings.ini not found directly\n",
    "     if (project_root.parent / 'settings.ini').exists():\n",
    "          project_root = project_root.parent\n",
    "     else:\n",
    "          print(\"Warning: Could not automatically determine project root. Assuming current dir.\")\n",
    "          # Fallback: Assume running from project root if structure unknown\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    # print(f\"Project root already in sys.path: {project_root_str}\") # Less verbose\n",
    "    pass\n",
    "# --- End Project Root Setup --- \n",
    "    \n",
    "# Import necessary llava components\n",
    "try:\n",
    "    from llava.utils import load_config, init_wandb # Added init_wandb\n",
    "    from llava.data.loading import get_test_dataloader, LLaVADataBlockStage2, get_image_path, LLaVASample # Assumes this function exists in 10_data_loading\n",
    "    from llava.model.baseline import BaselineLLaVAModel \n",
    "    from llava.model.adaptive import AdaptiveLLaVAModel # Import adaptive model\n",
    "    from llava.data.preprocessing import tokenizer, DEFAULT_IMAGE_TOKEN, IGNORE_INDEX, IMAGE_TOKEN_INDEX_PLACEHOLDER # Import tokenizer & constants\n",
    "    # Import PEFT related things if needed for model loading check\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        _peft_available = True\n",
    "    except ImportError:\n",
    "        PeftModel = None \n",
    "        _peft_available = False\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing llava modules: {e}. Make sure nbdev_export was run.\")\n",
    "    # Define placeholders if running standalone or during initial setup\n",
    "    def load_config(path): return {}\n",
    "    def init_wandb(*args, **kwargs): pass\n",
    "    def get_test_dataloader(config, dataset_name, dblock=None): raise NotImplementedError\n",
    "    class BaselineLLaVAModel(torch.nn.Module): \n",
    "        def __init__(self, *args, **kwargs): \n",
    "            super().__init__()\n",
    "            self.dummy = torch.nn.Linear(1,1)\n",
    "            self.image_token_index_marker = -200 # Define necessary attributes\n",
    "            self.projector = torch.nn.Linear(10,10)\n",
    "            self.language_model = self # Make model act like LLM for generate\n",
    "            self.vision_tower = self # Dummy vision tower\n",
    "            self.config = {'model':{}} # Dummy config\n",
    "        def encode_image(self, *args, **kwargs): return torch.randn(1,5,10) # Dummy image features B, P, D\n",
    "        def get_input_embeddings(self): return torch.nn.Embedding(100, 10)\n",
    "        def forward(self, *args, **kwargs): return CausalLMOutputWithPast(logits=torch.randn(1, 10, 100))\n",
    "        def generate(self, *args, **kwargs): return torch.randint(0, 100, (1, kwargs.get('max_new_tokens', 10)))\n",
    "        def to(self, *args, **kwargs): return self # Avoid device errors in dummy\n",
    "        def eval(self): pass\n",
    "    class AdaptiveLLaVAModel(BaselineLLaVAModel): pass # Simple inheritance for placeholder\n",
    "    class DummyTokenizer:\n",
    "         def __init__(self): self.eos_token_id=1; self.pad_token_id=0; self.unk_token_id=2\n",
    "         def decode(self, *args, **kwargs): return \"dummy decoded text\"\n",
    "         def convert_tokens_to_ids(self, *args): return self.unk_token_id\n",
    "         def __len__(self): return 100\n",
    "    tokenizer = DummyTokenizer()\n",
    "    DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "    IGNORE_INDEX = -100\n",
    "    IMAGE_TOKEN_INDEX_PLACEHOLDER = -200\n",
    "    LLaVADataBlockStage2 = None # Placeholder\n",
    "    PeftModel = None\n",
    "    _peft_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Prediction Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a trained fastai `Learner` and a test `DataLoader`, generates predictions, decodes them into text, and saves them to a JSON file. It now also includes efficiency logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_raw_images_for_batch(dl: DataLoader, batch_indices: List[int]) -> List[Optional[PIL.Image.Image]]:\n",
    "    \"\"\"Safely retrieves the raw PIL images for a given batch of indices from the DataLoader's dataset.\n",
    "\n",
    "    Args:\n",
    "        dl: The DataLoader instance.\n",
    "        batch_indices: A list of integer indices corresponding to the items in the current batch.\n",
    "\n",
    "    Returns:\n",
    "        A list of PIL.Image.Image objects or None for each item in the batch.\n",
    "    \"\"\"\n",
    "    raw_images = []\n",
    "    if not hasattr(dl, 'dataset'):\n",
    "        print(\"Warning: DataLoader has no 'dataset' attribute. Cannot retrieve raw images.\")\n",
    "        return [None] * len(batch_indices)\n",
    "\n",
    "    # Try to get original items from dataloader if available\n",
    "    original_items = getattr(dl, 'items', None)\n",
    "    if original_items is None:\n",
    "        print(\"Warning: DataLoader has no 'items' attribute. Cannot reliably retrieve raw image paths.\")\n",
    "        return [None] * len(batch_indices)\n",
    "\n",
    "    for idx in batch_indices:\n",
    "        try:\n",
    "            if idx < len(original_items):\n",
    "                original_item = original_items[idx]\n",
    "                if isinstance(original_item, LLaVASample):\n",
    "                    image_path = get_image_path(original_item)\n",
    "                    if image_path.exists():\n",
    "                        raw_images.append(PIL.Image.open(image_path))\n",
    "                    else:\n",
    "                        print(f\"Warning: Image file not found at {image_path} for index {idx}\")\n",
    "                        raw_images.append(None)\n",
    "                else:\n",
    "                    # Attempt to infer path if item is just a path or tuple\n",
    "                    potential_path = None\n",
    "                    if isinstance(original_item, (str, Path)):\n",
    "                         potential_path = Path(original_item)\n",
    "                    elif isinstance(original_item, tuple) and len(original_item)>0 and isinstance(original_item[0], (str, Path)):\n",
    "                         potential_path = Path(original_item[0])\n",
    "                    \n",
    "                    if potential_path and potential_path.exists():\n",
    "                        raw_images.append(PIL.Image.open(potential_path))\n",
    "                    else:\n",
    "                         print(f\"Warning: Could not determine image path from item at index {idx}: {original_item}\")\n",
    "                         raw_images.append(None) # Cannot determine path from item format\n",
    "            else:\n",
    "                 print(f\"Warning: Index {idx} out of bounds for dl.items (length {len(original_items)})\")\n",
    "                 raw_images.append(None)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load raw image for dataset index {idx}: {e}\")\n",
    "            raw_images.append(None)\n",
    "    return raw_images\n",
    "\n",
    "\n",
    "def generate_predictions(learner: Learner, \n",
    "                         test_dl: DataLoader, \n",
    "                         output_file: Union[str, Path], \n",
    "                         max_len: int = 200, # Max generation length\n",
    "                         temperature: float = 0.2, # Generation temperature\n",
    "                         top_p: Optional[float] = None, # Nucleus sampling top_p\n",
    "                        ):\n",
    "    \"\"\"Generates predictions for a test dataloader and saves them to a JSON Lines file.\n",
    "    Includes efficiency logging (latency, peak VRAM).\n",
    "\n",
    "    Uses the HF generate method on the underlying language model component,\n",
    "    manually preparing the combined image and text embeddings.\n",
    "    Handles passing `raw_images` if the model is adaptive.\n",
    "\n",
    "    Args:\n",
    "        learner: Trained fastai Learner object containing the model.\n",
    "                 Expected to have `learner.model`, `learner.dls.device`,\n",
    "                 and potentially `learner.tokenizer` or `learner.dls.tokenizer`.\n",
    "        test_dl: DataLoader for the test set.\n",
    "        output_file: Path to save the JSON Lines prediction file.\n",
    "        max_len: Maximum number of new tokens to generate.\n",
    "        temperature: Sampling temperature for generation. Set to 0 for greedy decoding.\n",
    "        top_p: If set (and temperature > 0), use nucleus sampling with this top_p value.\n",
    "    \"\"\"\n",
    "    output_file = Path(output_file)\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model = learner.model\n",
    "    is_adaptive = isinstance(model, AdaptiveLLaVAModel)\n",
    "    \n",
    "    # Get tokenizer\n",
    "    tok = getattr(learner, 'tokenizer', getattr(learner.dls, 'tokenizer', tokenizer))\n",
    "    if tok is None:\n",
    "        raise AttributeError(\"Tokenizer not found. Cannot decode predictions.\")\n",
    "        \n",
    "    # Ensure model components exist\n",
    "    if not all(hasattr(model, attr) and getattr(model, attr) is not None \n",
    "               for attr in ['vision_tower', 'projector', 'language_model', 'get_input_embeddings', 'encode_image']):\n",
    "        raise AttributeError(\"Model is missing required components.\")\n",
    "        \n",
    "    model.eval() # Set model to evaluation mode\n",
    "    results = []\n",
    "    total_time = 0\n",
    "    num_samples = 0\n",
    "    device = learner.dls.device # Get device from learner\n",
    "\n",
    "    print(f\"Generating predictions for {len(test_dl.dataset)} samples...\")\n",
    "    print(f\"Saving predictions to: {output_file}\")\n",
    "    \n",
    "    # Reset CUDA memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats(device=device) \n",
    "        print(\"Reset CUDA peak memory stats before generation.\")\n",
    "\n",
    "    with torch.no_grad(), open(output_file, 'w') as f_out:\n",
    "        batch_count = 0\n",
    "        for batch_tuple in tqdm(test_dl, desc=\"Generating Predictions\"):\n",
    "            start_time = time.time() # Start timing\n",
    "            \n",
    "            # The dataloader yields tuples (or dicts if custom collate). Assume dict for now.\n",
    "            # If it yields tuples, need to adjust access.\n",
    "            batch = batch_tuple # Assuming the batch is already a dict\n",
    "            if not isinstance(batch, dict):\n",
    "                 # Fallback if it's a tuple (e.g., (images, texts))\n",
    "                 # This assumes standard fastai dataloader output without custom collate\n",
    "                 print(f\"Warning: Batch type is {type(batch)}, expected dict. Trying tuple access.\")\n",
    "                 try:\n",
    "                      # Need to manually apply the batch transform logic here if not done by DL\n",
    "                      # This is complex. It's better to ensure the DL yields the dict.\n",
    "                      # For now, assume batch *is* the dict for simplicity\n",
    "                      raise NotImplementedError(\"DataLoader did not yield a dictionary batch. Ensure LLaVABatchTransform is used.\")\n",
    "                 except (TypeError, IndexError, NotImplementedError) as e:\n",
    "                      print(f\"Error processing non-dict batch: {e}. Skipping batch.\")\n",
    "                      continue\n",
    "                 \n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device) \n",
    "            batch_size = pixel_values.shape[0]\n",
    "            \n",
    "            # --- Retrieve sample IDs and raw images --- \n",
    "            # Use fastai's internal batch indexing if available\n",
    "            start_idx = batch_count * test_dl.bs\n",
    "            end_idx = start_idx + batch_size\n",
    "            current_indices = test_dl.idxs[start_idx:end_idx] if hasattr(test_dl, 'idxs') else list(range(num_samples, num_samples + batch_size))\n",
    "            batch_ids = []\n",
    "            if hasattr(test_dl, 'items') and test_dl.items is not None:\n",
    "                 for idx in current_indices:\n",
    "                      item_id = f\"sample_{idx}\"\n",
    "                      try:\n",
    "                          original_item = test_dl.items[idx]\n",
    "                          if isinstance(original_item, LLaVASample):\n",
    "                              item_id = original_item.sample_id\n",
    "                          elif isinstance(original_item, dict) and 'id' in original_item:\n",
    "                              item_id = original_item['id']\n",
    "                      except IndexError:\n",
    "                           pass # Keep default ID if index out of bounds\n",
    "                      except Exception as e:\n",
    "                           print(f\"Warning: Error getting ID for index {idx}: {e}\")\n",
    "                      batch_ids.append(item_id)\n",
    "            else:\n",
    "                 batch_ids = [f\"sample_{idx}\" for idx in current_indices]\n",
    "                 \n",
    "            raw_images = None\n",
    "            if is_adaptive:\n",
    "                raw_images = get_raw_images_for_batch(test_dl, current_indices)\n",
    "                if len(raw_images) != batch_size:\n",
    "                    print(f\"Warning: Mismatch between raw_images ({len(raw_images)}) and batch_size ({batch_size}). Generation might fail.\")\n",
    "                    raw_images = [None] * batch_size # Pad with None to avoid errors\n",
    "            # ------------------------------------------ \n",
    "\n",
    "            # --- Generation --- \n",
    "            image_features = model.encode_image(pixel_values) # (B, P, D_clip)\n",
    "            if image_features is None: \n",
    "                print(\"Warning: Image encoding failed for batch. Skipping.\")\n",
    "                num_samples += batch_size\n",
    "                total_time += (time.time() - start_time)\n",
    "                batch_count += 1\n",
    "                continue \n",
    "            projected_features = model.projector(image_features) # (B, P, D_llm)\n",
    "            \n",
    "            outputs_list = []\n",
    "            for i in range(batch_size):\n",
    "                current_input_ids = input_ids[i:i+1] \n",
    "                current_proj_features = projected_features[i:i+1]\n",
    "                \n",
    "                marker = getattr(model, 'image_token_index_marker', IMAGE_TOKEN_INDEX_PLACEHOLDER)\n",
    "                image_token_indices = torch.where(current_input_ids[0] == marker)[0]\n",
    "                \n",
    "                if len(image_token_indices) == 0:\n",
    "                    print(f\"Warning: Image marker {marker} not found in sample {batch_ids[i]}. Skipping.\")\n",
    "                    outputs_list.append(torch.tensor([tok.eos_token_id], device=device)) \n",
    "                    continue\n",
    "                \n",
    "                image_token_start_index = image_token_indices[0].item()\n",
    "                input_ids_no_marker = current_input_ids.clone()\n",
    "                input_ids_no_marker[input_ids_no_marker == marker] = 0 \n",
    "                text_embeddings = model.get_input_embeddings()(input_ids_no_marker)\n",
    "                \n",
    "                text_emb_before = text_embeddings[:, :image_token_start_index]\n",
    "                text_emb_after = text_embeddings[:, image_token_start_index + 1:]\n",
    "                \n",
    "                prompt_embeds = torch.cat([\n",
    "                    text_emb_before,\n",
    "                    current_proj_features.to(text_embeddings.device, dtype=text_embeddings.dtype),\n",
    "                    text_emb_after\n",
    "                ], dim=1)\n",
    "                \n",
    "                llm_component = model.language_model\n",
    "                if _peft_available and isinstance(llm_component, PeftModel):\n",
    "                     llm_component = llm_component.base_model.model \n",
    "                \n",
    "                prompt_attention_mask = torch.ones(prompt_embeds.shape[:2], dtype=torch.long, device=device)\n",
    "                \n",
    "                try:\n",
    "                    gen_params = {\n",
    "                        \"inputs_embeds\": prompt_embeds,\n",
    "                        \"attention_mask\": prompt_attention_mask,\n",
    "                        \"max_new_tokens\": max_len,\n",
    "                        \"eos_token_id\": tok.eos_token_id,\n",
    "                        \"pad_token_id\": tok.pad_token_id if tok.pad_token_id is not None else tok.eos_token_id,\n",
    "                        \"do_sample\": (temperature > 0),\n",
    "                        \"num_beams\": 1\n",
    "                    }\n",
    "                    if temperature > 0:\n",
    "                        gen_params[\"temperature\"] = temperature\n",
    "                        if top_p is not None: gen_params[\"top_p\"] = top_p\n",
    "                    else: \n",
    "                        gen_params[\"temperature\"] = 1.0 \n",
    "                        gen_params[\"do_sample\"] = False\n",
    "                        \n",
    "                    # NOTE: Passing raw_images to generate is not directly supported by HF's standard generate.\n",
    "                    # If the AdaptivePatcher needs raw_image during the forward pass *called by generate*,\n",
    "                    # this requires a custom generation loop or modification of the model's generate method.\n",
    "                    # Our VariableResolutionPatcher determines strategy *before* forward, so it's currently okay.\n",
    "                    \n",
    "                    output_ids_gen = llm_component.generate(**gen_params)\n",
    "                    \n",
    "                    # Remove the prompt tokens from the generated output\n",
    "                    output_ids_gen = output_ids_gen[:, prompt_embeds.shape[1]:]\n",
    "                    outputs_list.append(output_ids_gen[0]) \n",
    "                except Exception as gen_e:\n",
    "                     print(f\"Error during generation for sample {batch_ids[i]}: {gen_e}\")\n",
    "                     outputs_list.append(torch.tensor([tok.eos_token_id], device=device)) # Fallback\n",
    "            \n",
    "            # --- Decode and Save --- \n",
    "            end_time = time.time() \n",
    "            total_time += (end_time - start_time)\n",
    "            num_samples_in_batch = batch_size\n",
    "\n",
    "            for i, gen_ids in enumerate(outputs_list):\n",
    "                decoded_text = tok.decode(gen_ids.cpu(), skip_special_tokens=True).strip()\n",
    "                result_entry = {\"id\": batch_ids[i], \"prediction\": decoded_text}\n",
    "                f_out.write(json.dumps(result_entry) + '\\n')\n",
    "                results.append(result_entry)\n",
    "            \n",
    "            num_samples += num_samples_in_batch\n",
    "            batch_count += 1\n",
    "\n",
    "    # --- Log Efficiency Metrics --- \n",
    "    avg_latency = (total_time / num_samples) * 1000 if num_samples > 0 else 0\n",
    "    print(f\"Finished generation. Saved {len(results)} predictions to {output_file}\")\n",
    "    print(f\"Average inference latency: {avg_latency:.2f} ms/sample\")\n",
    "    \n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\"eval/avg_inference_latency_ms\": avg_latency})\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        peak_vram_gb = torch.cuda.max_memory_allocated(device=device) / (1024**3) \n",
    "        print(f\"Peak Inference VRAM used: {peak_vram_gb:.2f} GB\")\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\"eval/peak_inference_vram_gb\": peak_vram_gb})\n",
    "        torch.cuda.reset_peak_memory_stats(device=device) \n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3 & 5.5: Integrate External Evaluation Scripts (Implement VQAv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for calling external evaluation scripts. The `evaluate_vqa` function is now implemented using the official VQA tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def evaluate_vqa(preds_file: Union[str, Path], \n",
    "                 gt_file: Union[str, Path], \n",
    "                 ques_file: Optional[Union[str, Path]] = None, # Path to questions file if needed by API\n",
    "                 **kwargs):\n",
    "    \"\"\"Evaluates VQAv2 predictions using the official VQA evaluation tools.\n",
    "    \n",
    "    Args:\n",
    "        preds_file: Path to the JSON Lines prediction file generated by `generate_predictions`\n",
    "                    (expected format: {'id': question_id, 'prediction': answer}).\n",
    "        gt_file: Path to the ground truth annotation file (e.g., v2_mscoco_val2014_annotations.json).\n",
    "        ques_file: Path to the questions file (e.g., v2_OpenEnded_mscoco_val2014_questions.json). \n",
    "                   Required by the standard VQA eval tools.\n",
    "        **kwargs: Additional arguments (unused currently).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing VQA scores (e.g., {'overall': score, 'yes/no': ..., 'number': ..., 'other': ...}), \n",
    "        or {'overall': 0.0} if evaluation fails.\n",
    "    \"\"\"\n",
    "    print(f\"--- VQA Evaluation --- \")\n",
    "    print(f\"Predictions file: {preds_file}\")\n",
    "    print(f\"Ground Truth Annotation file: {gt_file}\")\n",
    "    print(f\"Ground Truth Questions file: {ques_file}\")\n",
    "    results = {\"overall\": 0.0} # Default return value\n",
    "    \n",
    "    if ques_file is None:\n",
    "        print(\"Error: Path to questions file (ques_file) is required for VQA evaluation.\")\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        # --- Load VQA Tools --- \n",
    "        # Assumes the VQA evaluation tools directory ('PythonHelperTools') is in the Python path.\n",
    "        # User needs to download from https://visualqa.org/download.html\n",
    "        # and add the PythonHelperTools directory to their PYTHONPATH or sys.path\n",
    "        try:\n",
    "            from vqaTools.vqa import VQA\n",
    "            from vqaEvaluation.vqaEval import VQAEval\n",
    "        except ImportError:\n",
    "            print(\"Error: VQA evaluation tools (vqaTools/vqaEvaluation) not found in PYTHONPATH.\")\n",
    "            print(\"Please download from https://visualqa.org/download.html and add PythonHelperTools to your path.\")\n",
    "            return results\n",
    "        \n",
    "        # --- Load and Reformat Predictions --- \n",
    "        vqa_preds_formatted = []\n",
    "        print(f\"Loading and reformatting predictions from {preds_file}...\")\n",
    "        with open(preds_file, 'r') as f_pred:\n",
    "            for line in f_pred:\n",
    "                try:\n",
    "                    pred_item = json.loads(line.strip())\n",
    "                    # VQA API expects integer question_id\n",
    "                    question_id = int(pred_item['id']) \n",
    "                    answer = pred_item['prediction']\n",
    "                    vqa_preds_formatted.append({\n",
    "                        'question_id': question_id,\n",
    "                        'answer': answer\n",
    "                    })\n",
    "                except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "                    print(f\"Warning: Skipping invalid prediction line: {line.strip()}. Error: {e}\")\n",
    "        print(f\"Loaded {len(vqa_preds_formatted)} predictions for evaluation.\")\n",
    "        if not vqa_preds_formatted:\n",
    "            print(\"Error: No valid predictions found in the file.\")\n",
    "            return results\n",
    "        \n",
    "        # --- Run VQA Evaluation --- \n",
    "        print(\"Initializing VQA evaluation...\")\n",
    "        vqa_ann = VQA(gt_file, ques_file) # Load annotations and questions\n",
    "        vqa_pred = vqa_ann.loadRes(vqa_preds_formatted, ques_file) # Load predictions\n",
    "        \n",
    "        vqa_eval = VQAEval(vqa_ann, vqa_pred, n=2) # n=2 is standard for VQA\n",
    "        \n",
    "        print(\"Running evaluation...\")\n",
    "        vqa_eval.evaluate() # Perform evaluation\n",
    "        \n",
    "        print(\"Evaluation complete. Results:\")\n",
    "        vqa_eval.showEvals() # Print detailed results\n",
    "        \n",
    "        # Extract results into a dictionary\n",
    "        results = {\n",
    "            \"overall\": vqa_eval.accuracy['overall'],\n",
    "            \"yes/no\": vqa_eval.accuracy['perAnswerType']['yes/no'],\n",
    "            \"number\": vqa_eval.accuracy['perAnswerType']['number'],\n",
    "            \"other\": vqa_eval.accuracy['perAnswerType']['other'],\n",
    "        }\n",
    "        print(f\"Parsed VQA Accuracy: {results}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "         print(f\"Error: Required file not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during VQA evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Log metric to W&B if active\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\"eval/vqa_score_overall\": results.get(\"overall\", 0.0)})\n",
    "        wandb.log({\"eval/vqa_score_yes_no\": results.get(\"yes/no\", 0.0)})\n",
    "        wandb.log({\"eval/vqa_score_number\": results.get(\"number\", 0.0)})\n",
    "        wandb.log({\"eval/vqa_score_other\": results.get(\"other\", 0.0)})\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.6: Implement ANLS Calculation for TextVQA/DocVQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_single_anls(prediction: str, ground_truths: List[str], threshold=0.5) -> float:\n",
    "    \"\"\"Calculates the Average Normalized Levenshtein Similarity (ANLS) for a single prediction.\n",
    "    \n",
    "    Based on the ANLS definition for ICDAR 2019 Robust Reading Challenge on Scene Text VQA.\n",
    "    https://rrc.cvc.uab.es/?ch=11&com=tasks -> Task 3 -> Evaluation\n",
    "\n",
    "    Args:\n",
    "        prediction: The predicted answer string.\n",
    "        ground_truths: A list of ground truth answer strings.\n",
    "        threshold: The Normalized Levenshtein Distance threshold (default: 0.5).\n",
    "                   If NLD > threshold, the similarity is 0.\n",
    "\n",
    "    Returns:\n",
    "        The ANLS score for this prediction (float between 0 and 1).\n",
    "    \"\"\"\n",
    "    if Levenshtein is None:\n",
    "        print(\"Warning: Levenshtein library not available. Returning 0 for ANLS.\")\n",
    "        return 0.0\n",
    "    \n",
    "    if not ground_truths: # Handle case with no ground truths\n",
    "        return 0.0\n",
    "    if not prediction: # Handle case with empty prediction\n",
    "        # Similarity is 1 if any GT is also empty, 0 otherwise\n",
    "        return 1.0 if any(not gt for gt in ground_truths) else 0.0\n",
    "        \n",
    "    max_similarity = 0.0\n",
    "    prediction_lower = prediction.lower() # Typically case-insensitive\n",
    "\n",
    "    for gt in ground_truths:\n",
    "        gt_lower = gt.lower()\n",
    "        if not gt_lower:\n",
    "            # Similarity is 0 if prediction is non-empty and GT is empty\n",
    "            similarity = 0.0\n",
    "        else:\n",
    "            distance = Levenshtein.distance(prediction_lower, gt_lower)\n",
    "            max_len = max(len(prediction_lower), len(gt_lower))\n",
    "            if max_len == 0: # Avoid division by zero if both strings are empty after lowercasing etc.\n",
    "                 nld = 0.0\n",
    "            else:\n",
    "                 nld = distance / max_len # Normalized Levenshtein Distance\n",
    "\n",
    "            if nld <= threshold:\n",
    "                similarity = 1.0 - nld\n",
    "            else:\n",
    "                similarity = 0.0\n",
    "        \n",
    "        max_similarity = max(max_similarity, similarity)\n",
    "        \n",
    "    return max_similarity\n",
    "\n",
    "#| export\n",
    "def evaluate_textvqa(preds_file: Union[str, Path], \n",
    "                     gt_file: Union[str, Path], \n",
    "                     anls_threshold=0.5,\n",
    "                     **kwargs):\n",
    "    \"\"\"Evaluates TextVQA/DocVQA predictions using the ANLS metric.\n",
    "\n",
    "    Assumes standard TextVQA JSON format for ground truth and\n",
    "    a JSON Lines file for predictions with 'id' (question_id) and 'prediction'.\n",
    "\n",
    "    Args:\n",
    "        preds_file: Path to the JSON Lines prediction file.\n",
    "        gt_file: Path to the ground truth annotation file (e.g., TextVQA_0.5.1_val.json).\n",
    "        anls_threshold: NLD threshold for ANLS calculation (default: 0.5).\n",
    "        **kwargs: Additional arguments (unused).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing ANLS score (e.g., {'anls': score}).\n",
    "    \"\"\"\n",
    "    print(f\"--- TextVQA/DocVQA ANLS Evaluation --- \")\n",
    "    print(f\"Predictions file: {preds_file}\")\n",
    "    print(f\"Ground Truth file: {gt_file}\")\n",
    "    \n",
    "    if Levenshtein is None:\n",
    "        print(\"Error: `pip install python-Levenshtein` is required for ANLS evaluation.\")\n",
    "        return {\"anls\": 0.0}\n",
    "        \n",
    "    total_anls = 0.0\n",
    "    count = 0\n",
    "    results = {\"anls\": 0.0} # Default return value\n",
    "    \n",
    "    try:\n",
    "        # Load ground truth\n",
    "        with open(gt_file, 'r') as f_gt:\n",
    "            ground_truth_data = json.load(f_gt)\n",
    "            \n",
    "        # Format ground truth into a dictionary {question_id: [answer1, answer2, ...]}\n",
    "        # Adapting for TextVQA format which uses 'question_id' and 'answers'\n",
    "        if 'data' in ground_truth_data:\n",
    "            gt_dict = {item['question_id']: item['answers'] for item in ground_truth_data['data']}\n",
    "        else:\n",
    "             print(f\"Warning: Unexpected GT file format in {gt_file}. Expected a 'data' key.\")\n",
    "             gt_dict = {}\n",
    "             \n",
    "        # Load predictions\n",
    "        predictions = []\n",
    "        with open(preds_file, 'r') as f_pred:\n",
    "            for line in f_pred:\n",
    "                try:\n",
    "                    predictions.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping invalid JSON line in predictions file: {line.strip()}\")\n",
    "                    \n",
    "        print(f\"Loaded {len(gt_dict)} ground truth items and {len(predictions)} predictions.\")\n",
    "        \n",
    "        # Calculate ANLS for each prediction\n",
    "        for pred_item in tqdm(predictions, desc=\"Calculating ANLS\"):\n",
    "            pred_id = pred_item.get('id')\n",
    "            prediction = pred_item.get('prediction', '')\n",
    "            \n",
    "            if pred_id is None:\n",
    "                print(f\"Warning: Prediction item missing 'id': {pred_item}\")\n",
    "                continue\n",
    "                \n",
    "            # Ensure ID matching (GT keys are strings in TextVQA JSON)\n",
    "            gt_answers = gt_dict.get(str(pred_id))\n",
    "            \n",
    "            if gt_answers is None:\n",
    "                # print(f\"Warning: No ground truth found for prediction ID: {pred_id}\")\n",
    "                # Some benchmarks might have predictions for samples not in the target GT split\n",
    "                continue # Skip if no ground truth for this prediction\n",
    "            \n",
    "            sample_anls = calculate_single_anls(prediction, gt_answers, threshold=anls_threshold)\n",
    "            total_anls += sample_anls\n",
    "            count += 1\n",
    "            \n",
    "        if count > 0:\n",
    "            final_anls = total_anls / count\n",
    "            results = {\"anls\": final_anls}\n",
    "            print(f\"Evaluated {count} samples.\")\n",
    "            print(f\"Average Normalized Levenshtein Similarity (ANLS): {final_anls:.4f}\")\n",
    "        else:\n",
    "             print(\"No matching predictions found for ground truth IDs. ANLS is 0.\")\n",
    "             results = {\"anls\": 0.0}\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Required file not found: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ANLS calculation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Log metric to W&B if active\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({\"eval/anls\": results.get(\"anls\", 0.0)})\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test ANLS Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Levenshtein library not available. Returning 0 for ANLS.\n",
      "Test 1 (Exact Match): Expected 1.0, Got 0.0 - Levenshtein not installed.\n",
      "Warning: Levenshtein library not available. Returning 0 for ANLS.\n",
      "Test 2 (One deletion): Expected >0.8, Got 0.0 - Levenshtein not installed.\n",
      "Warning: Levenshtein library not available. Returning 0 for ANLS.\n",
      "Test 3 (High distance): Expected 0.0, Got 0.0 - Levenshtein not installed.\n",
      "Warning: Levenshtein library not available. Returning 0 for ANLS.\n",
      "Test 4 (Empty prediction): Expected 0.0, Got 0.0 - Levenshtein not installed.\n",
      "Warning: Levenshtein library not available. Returning 0 for ANLS.\n",
      "Test 5 (Empty GT): Expected 0.0, Got 0.0 - Levenshtein not installed.\n",
      "Warning: Levenshtein library not available. Returning 0 for ANLS.\n",
      "Test 6 (Case difference): Expected 1.0, Got 0.0 - Levenshtein not installed.\n",
      "Warning: Levenshtein library not available. Returning 0 for ANLS.\n",
      "Test 7 (Multiple GT, one match): Expected 1.0, Got 0.0 - Levenshtein not installed.\n",
      "Warning: Levenshtein library not available. Returning 0 for ANLS.\n",
      "Test 8 (NLD < threshold): Expected ~0.56, Got 0.0 - Levenshtein not installed.\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "if Levenshtein:\n",
    "    # Test cases for calculate_single_anls\n",
    "    assert abs(calculate_single_anls(\"apple\", [\"apple\"]) - 1.0) < 1e-6, \"Test 1 Failed (Exact Match)\"\n",
    "    assert abs(calculate_single_anls(\"apple\", [\"aple\"]) - (1 - 1/5)) < 1e-6, \"Test 2 Failed (One deletion)\"\n",
    "    assert abs(calculate_single_anls(\"apple\", [\"banana\"]) - 0.0) < 1e-6, \"Test 3 Failed (High distance)\"\n",
    "    assert abs(calculate_single_anls(\"\", [\"apple\"]) - 0.0) < 1e-6, \"Test 4 Failed (Empty prediction)\"\n",
    "    assert abs(calculate_single_anls(\"apple\", [\"\"]) - 0.0) < 1e-6, \"Test 5 Failed (Empty GT)\"\n",
    "    assert abs(calculate_single_anls(\"apple\", [\"APPLE\"]) - 1.0) < 1e-6, \"Test 6 Failed (Case difference)\"\n",
    "    assert abs(calculate_single_anls(\"apple\", [\"banana\", \"apple\", \"orange\"]) - 1.0) < 1e-6, \"Test 7 Failed (Multiple GT, one match)\"\n",
    "    # Test 8: distance(\"apple pie\", \"apple\") = 4, max_len=9, nld=4/9=0.444. Since 0.444 <= 0.5, similarity = 1 - 0.444 = 0.555\n",
    "    assert abs(calculate_single_anls(\"apple pie\", [\"apple\"]) - (1 - 4/9)) < 1e-6, \"Test 8 Failed (NLD < threshold)\"\n",
    "    print(\"ANLS calculation tests passed.\")\n",
    "else:\n",
    "    # Print expected vs got if Levenshtein is not installed\n",
    "    print(f\"Test 1 (Exact Match): Expected 1.0, Got {calculate_single_anls('apple', ['apple'])} - Levenshtein not installed.\")\n",
    "    print(f\"Test 2 (One deletion): Expected >0.8, Got {calculate_single_anls('apple', ['aple'])} - Levenshtein not installed.\")\n",
    "    print(f\"Test 3 (High distance): Expected 0.0, Got {calculate_single_anls('apple', ['banana'])} - Levenshtein not installed.\")\n",
    "    print(f\"Test 4 (Empty prediction): Expected 0.0, Got {calculate_single_anls('', ['apple'])} - Levenshtein not installed.\")\n",
    "    print(f\"Test 5 (Empty GT): Expected 0.0, Got {calculate_single_anls('apple', [''])} - Levenshtein not installed.\")\n",
    "    print(f\"Test 6 (Case difference): Expected 1.0, Got {calculate_single_anls('apple', ['APPLE'])} - Levenshtein not installed.\")\n",
    "    print(f\"Test 7 (Multiple GT, one match): Expected 1.0, Got {calculate_single_anls('apple', ['banana', 'apple', 'orange'])} - Levenshtein not installed.\")\n",
    "    print(f\"Test 8 (NLD < threshold): Expected ~0.56, Got {calculate_single_anls('apple pie', ['apple'])} - Levenshtein not installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.4: Custom Evaluation Set Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function specifically for evaluating the custom dataset. This might involve custom metrics or qualitative analysis later. For now, we'll implement it using ANLS as a placeholder, assuming the custom set has a compatible format (questions/answers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def evaluate_custom_set(preds_file: Union[str, Path],\n",
    "                        gt_file: Union[str, Path],\n",
    "                        **kwargs):\n",
    "    \"\"\"Evaluates predictions on the custom evaluation set.\n",
    "    \n",
    "    Placeholder implementation. Currently uses ANLS assuming a TextVQA-like format.\n",
    "    This should be adapted based on the specific metrics defined for the custom set.\n",
    "\n",
    "    Args:\n",
    "        preds_file: Path to the JSON Lines prediction file.\n",
    "        gt_file: Path to the ground truth annotation file for the custom set.\n",
    "        **kwargs: Additional arguments (e.g., ANLS threshold).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing evaluation scores.\n",
    "    \"\"\"\n",
    "    print(f\"--- Custom Evaluation Set Evaluation --- \")\n",
    "    print(f\"Predictions file: {preds_file}\")\n",
    "    print(f\"Ground Truth file: {gt_file}\")\n",
    "\n",
    "    # --- Placeholder: Use ANLS for now --- \n",
    "    # This assumes the custom eval set follows a format where ANLS is meaningful\n",
    "    # (e.g., question_id, answers in GT; id, prediction in preds)\n",
    "    # You might need to implement different logic or metrics here.\n",
    "    if Levenshtein is None:\n",
    "        print(\"Error: Levenshtein library not installed. Cannot calculate ANLS for custom set.\")\n",
    "        return {\"custom_metric_placeholder\": 0.0}\n",
    "        \n",
    "    anls_threshold = kwargs.get('anls_threshold', 0.5)\n",
    "    results = evaluate_textvqa(preds_file, gt_file, anls_threshold=anls_threshold)\n",
    "    \n",
    "    # Rename the key for clarity and log to W&B\n",
    "    custom_results = {f\"custom_anls@{anls_threshold}\": results.get(\"anls\", 0.0)}\n",
    "    print(f\"Custom Set ANLS Results: {custom_results}\")\n",
    "\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({f\"eval/custom_anls\": custom_results.get(f\"custom_anls@{anls_threshold}\", 0.0)})\n",
    "    \n",
    "    # --- Add other custom metrics calculation here --- \n",
    "    # Example: Calculate accuracy if applicable\n",
    "    # custom_results['accuracy'] = calculate_custom_accuracy(preds_file, gt_file)\n",
    "    \n",
    "    return custom_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_evaluation(config_path: Union[str, Path], \n",
    "                   model_type: str = 'baseline', # Added model_type argument\n",
    "                   model_checkpoint_path: Optional[Union[str, Path]] = None, \n",
    "                   dataset_name: str = 'vqav2_test', # Example default\n",
    "                   ablation_mode:str = None, # Added ablation_mode argument\n",
    "                   output_filename: Optional[str] = None,\n",
    "                   **kwargs # Allow passing generation args like temp, top_p\n",
    "                   ):\n",
    "    \"\"\"Runs the evaluation pipeline: load model, generate predictions, evaluate.\n",
    "    Handles both baseline and adaptive model types.\n",
    "    Calls the appropriate evaluation function based on dataset_name.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the main configuration YAML.\n",
    "        model_type: Type of model to evaluate ('baseline' or 'adaptive').\n",
    "        model_checkpoint_path: Path to the specific model checkpoint base name/directory to load \n",
    "                               (e.g., '/path/to/output/models/stage2_llava_lora'). \n",
    "                               If None, uses the path derived from config['paths']['stage2_model_weights'].\n",
    "                               Expects associated files like '_projector_final.pth' and '_lora_adapters'/'_full.pth'.\n",
    "                               Filenames will be appended with _{model_type} (e.g., _baseline_projector_final.pth).\n",
    "        dataset_name: Name of the dataset split to evaluate (e.g., 'vqav2_test', 'textvqa_val', 'custom_eval').\n",
    "                      This key should exist in `config['paths']` pointing to dataset info.\n",
    "        output_filename: Name for the prediction output file (defaults based on model/dataset).\n",
    "        **kwargs: Additional arguments passed to `generate_predictions` (e.g., temperature, top_p).\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Evaluation Run ({model_type.capitalize()} Model) --- \")\n",
    "    print(f\"Config: {config_path}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    \n",
    "    config = load_config(config_path)\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    eval_output_dir = output_dir / 'eval_results' / f\"{model_type}_{dataset_name}\" # Add model type to output dir\n",
    "    eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    models_dir = output_dir / 'models' # Where models are saved\n",
    "\n",
    "    ablation_tag = f\"_abl-{ablation_mode}\" if ablation_mode else \"\"\n",
    "    print(f\"Ablation mode specified: {ablation_mode if ablation_mode else 'None'}\")\n",
    "    \n",
    "    learner = None\n",
    "    run = None # Initialize wandb run object\n",
    "    try:\n",
    "        # --- Initialize W&B --- \n",
    "        if config.get('logging', {}).get('wandb', {}).get('enabled', False):\n",
    "             model_id_for_run = Path(model_checkpoint_path if model_checkpoint_path else config['paths']['stage2_model_weights']).stem\n",
    "             run_name = f\"eval_{model_type}{ablation_tag}_{model_id_for_run}_{dataset_name}_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "             run = init_wandb(config, job_type=\"evaluation\", run_name=run_name)\n",
    "        \n",
    "        # 1. Determine Model Path Base Name\n",
    "        if model_checkpoint_path is None:\n",
    "            base_name_from_config = config['paths'].get('stage2_model_weights', f'stage2_llava_{model_type}')\n",
    "            model_base_name = f\"{base_name_from_config}\"\n",
    "            model_load_base = models_dir / model_base_name \n",
    "            print(f\"Using model base name from config: {model_base_name}\")\n",
    "        else:\n",
    "            model_checkpoint_path = Path(model_checkpoint_path)\n",
    "            model_base_name = model_checkpoint_path.stem\n",
    "            model_load_base = model_checkpoint_path \n",
    "            print(f\"Using provided model base path: {model_load_base}\")\n",
    "        \n",
    "        # Define model-type specific filename suffix\n",
    "        model_suffix = f\"_{model_type.lower()}\"\n",
    "\n",
    "        # 2. Load Model Components Manually\n",
    "        print(f\"Loading {model_type} model components for evaluation...\")\n",
    "        ModelClass = AdaptiveLLaVAModel if model_type == 'adaptive' else BaselineLLaVAModel\n",
    "        # Ensure LoRA/Checkpointing settings from config are used during model init\n",
    "        # (though weights will be overwritten)\n",
    "        eval_config = copy.deepcopy(config)\n",
    "        if 'model' not in eval_config: eval_config['model'] = {}\n",
    "        # Disable training-specific memory optimizations for eval if needed\n",
    "        eval_config['model']['use_activation_checkpointing'] = False\n",
    "\n",
    "        model = ModelClass(config=eval_config) \n",
    "        \n",
    "        # Load projector weights\n",
    "        proj_weights_path = model_load_base.parent / (model_load_base.name + f\"{model_suffix}{ablation_tag}_projector_final.pth\")\n",
    "        if proj_weights_path.exists():\n",
    "            model.projector.load_state_dict(torch.load(proj_weights_path, map_location='cpu'))\n",
    "            print(f\"Loaded projector weights from {proj_weights_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Projector weights not found at {proj_weights_path}. Using initial weights.\")\n",
    "            \n",
    "        # Load LoRA adapters\n",
    "        use_lora_config = config.get('model', {}).get('peft', {}).get('use_lora', False)\n",
    "        lora_adapter_path = model_load_base.parent / (model_load_base.name + f\"{model_suffix}{ablation_tag}_lora_adapters\")\n",
    "\n",
    "        if use_lora_config:\n",
    "            if lora_adapter_path.exists() and _peft_available and isinstance(model.language_model, PeftModel):\n",
    "                print(f\"Loading LoRA adapters from {lora_adapter_path}\")\n",
    "                try:\n",
    "                    if hasattr(model.language_model, 'load_adapter'):\n",
    "                         model.language_model.load_adapter(str(lora_adapter_path), adapter_name=\"default\")\n",
    "                         print(\"LoRA adapters loaded successfully.\")\n",
    "                    else:\n",
    "                         print(\"Warning: Model's language_model does not have 'load_adapter' method.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading LoRA adapters: {e}.\")\n",
    "            else:\n",
    "                print(f\"Warning: LoRA enabled but adapters not found at {lora_adapter_path} or PEFT issue.\")\n",
    "        else:\n",
    "             print(\"LoRA not used.\")\n",
    "             \n",
    "        # Load Patcher weights (if adaptive)\n",
    "        if model_type == 'adaptive' and hasattr(model, 'patcher') and model.patcher is not None:\n",
    "             patcher_params = list(model.patcher.parameters())\n",
    "             # Check if patcher has parameters (even if not trained, might be loaded)\n",
    "             if patcher_params: \n",
    "                  patcher_weights_path = model_load_base.parent / (model_load_base.name + f\"{model_suffix}{ablation_tag}_patcher_final.pth\")\n",
    "                  if patcher_weights_path.exists():\n",
    "                       model.patcher.load_state_dict(torch.load(patcher_weights_path, map_location='cpu'))\n",
    "                       print(f\"Loaded patcher weights from {patcher_weights_path}\")\n",
    "                  else:\n",
    "                       print(f\"Warning: Patcher weights file not found at {patcher_weights_path}\")\n",
    "             else:\n",
    "                  print(\"Adaptive patcher exists but has no parameters.\")\n",
    "        \n",
    "        # 3. Create Learner Shell\n",
    "        dummy_dl = DataLoader(list(range(1)), bs=1)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        dummy_dls = DataLoaders(dummy_dl, dummy_dl)\n",
    "        dummy_dls.device = device \n",
    "        learner = Learner(dls=dummy_dls, model=model, loss_func=lambda x,y: 0)\n",
    "        \n",
    "        global tokenizer \n",
    "        if tokenizer is None:\n",
    "             raise RuntimeError(\"Tokenizer not loaded, cannot proceed with evaluation.\")\n",
    "        learner.tokenizer = tokenizer \n",
    "        print(f\"Minimal Learner shell created. Model moved to {device}.\")\n",
    "        \n",
    "        # 4. Load Test Data\n",
    "        print(f\"Loading test data for {dataset_name}...\")\n",
    "        if LLaVADataBlockStage2 is None:\n",
    "             raise RuntimeError(\"LLaVADataBlockStage2 is not defined. Cannot load test data.\")\n",
    "        test_dl = get_test_dataloader(config, dataset_name, dblock=LLaVADataBlockStage2)\n",
    "        if test_dl is None:\n",
    "            raise RuntimeError(f\"Failed to load test dataloader for {dataset_name}\")\n",
    "\n",
    "        # 5. Generate Predictions\n",
    "        if output_filename is None:\n",
    "            output_filename = f\"preds_{model_base_name}{model_suffix}{ablation_tag}_{dataset_name}.jsonl\"\n",
    "        preds_file = eval_output_dir / output_filename\n",
    "        \n",
    "        gen_kwargs = {k: v for k, v in kwargs.items() if k in ['max_len', 'temperature', 'top_p']}\n",
    "        \n",
    "        generate_predictions(learner, test_dl, preds_file, **gen_kwargs)\n",
    "\n",
    "        # 6. Run Evaluation Metrics\n",
    "        print(\"Running evaluation metrics...\")\n",
    "        gt_config = config['paths'].get(dataset_name)\n",
    "        if not gt_config or 'annotations' not in gt_config:\n",
    "            print(f\"Warning: Ground truth annotation path not found for '{dataset_name}' in config. Cannot run metrics.\")\n",
    "        else:\n",
    "            gt_file_path = Path(config['paths']['data_base']) / gt_config['annotations']\n",
    "            ques_file_path = None\n",
    "            if 'questions' in gt_config:\n",
    "                 ques_file_path = Path(config['paths']['data_base']) / gt_config['questions']\n",
    "            elif 'vqav2' in dataset_name.lower(): \n",
    "                 ann_path_parts = list(gt_file_path.parts)\n",
    "                 if 'annotations' in ann_path_parts[-1]:\n",
    "                      ques_filename = gt_file_path.name.replace('annotations', 'questions').replace('v2_mscoco', 'v2_OpenEnded_mscoco')\n",
    "                      ques_file_path_default = gt_file_path.parent / ques_filename\n",
    "                      if ques_file_path_default.exists():\n",
    "                           ques_file_path = ques_file_path_default\n",
    "                           print(f\"Inferred VQA questions file path: {ques_file_path}\")\n",
    "            \n",
    "            if not gt_file_path.exists():\n",
    "                 print(f\"Warning: Ground truth file not found at {gt_file_path}. Cannot run metrics.\")\n",
    "            else:\n",
    "                 # --- Select Evaluation Function Based on Dataset Name --- #\n",
    "                 if 'vqav2' in dataset_name.lower():\n",
    "                     if ques_file_path and ques_file_path.exists():\n",
    "                         vqa_results = evaluate_vqa(preds_file, gt_file_path, ques_file=ques_file_path)\n",
    "                         print(f\"VQAv2 Results: {vqa_results}\")\n",
    "                     else:\n",
    "                          print(f\"Warning: VQA questions file not found or specified ({ques_file_path}). Cannot run VQA evaluation.\")\n",
    "                 elif 'textvqa' in dataset_name.lower():\n",
    "                     anls_results = evaluate_textvqa(preds_file, gt_file_path)\n",
    "                     print(f\"TextVQA ANLS Results: {anls_results}\")\n",
    "                 elif 'custom_eval' == dataset_name.lower(): # Use exact match for custom\n",
    "                     custom_results = evaluate_custom_set(preds_file, gt_file_path)\n",
    "                     print(f\"Custom Eval Set Results: {custom_results}\")\n",
    "                 # --- Add more evaluation cases here --- #\n",
    "                 # elif 'docvqa' in dataset_name.lower():\n",
    "                 #      anls_results = evaluate_textvqa(preds_file, gt_file_path) # Reuse ANLS\n",
    "                 #      print(f\"DocVQA ANLS Results: {anls_results}\")\n",
    "                 # elif 'chartqa' in dataset_name.lower():\n",
    "                 #      # chartqa_results = evaluate_chartqa(preds_file, gt_file_path)\n",
    "                 #      print(\"ChartQA evaluation not implemented yet.\")\n",
    "                 else:\n",
    "                     print(f\"No specific evaluation script configured for dataset: {dataset_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.finish(exit_code=1)\n",
    "    finally:\n",
    "        # Clean up memory\n",
    "        if learner is not None and hasattr(learner, 'model') and learner.model is not None:\n",
    "            learner.model.to('cpu')\n",
    "            del learner.model\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Cleaned up model memory.\")\n",
    "        # Finish W&B run if active and not already finished due to error\n",
    "        if run and wandb.run and wandb.run.id == run.id:\n",
    "             run.finish()\n",
    "             print(\"Finished W&B run.\")\n",
    "            \n",
    "    print(f\"--- Evaluation Run Complete ({model_type.capitalize()} Model) --- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Command-line execution block\n",
    "if __name__ == \"__main__\" and \"get_ipython\" not in locals():\n",
    "    parser = argparse.ArgumentParser(description=\"Run LLaVA Model Evaluation\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"configs/config.yaml\", \n",
    "                        help=\"Path to the configuration YAML file (relative to project root or absolute).\")\n",
    "    parser.add_argument(\"--model_type\", type=str, default=\"baseline\", choices=['baseline', 'adaptive'],\n",
    "                        help=\"Choose model type to evaluate: 'baseline' or 'adaptive'.\")\n",
    "    parser.add_argument(\"--model_checkpoint\", type=str, default=None,\n",
    "                        help=\"Path to the model checkpoint base name/directory (e.g., output/models/stage2_llava_lora). Optional. If provided, _{model_type} will be appended for weight loading.\")\n",
    "    parser.add_argument(\"--dataset\", type=str, required=True,\n",
    "                        help=\"Name of the dataset to evaluate (key in config['paths'], e.g., vqav2_test, textvqa_val, custom_eval).\")\n",
    "    parser.add_argument(\"--output_filename\", type=str, default=None,\n",
    "                        help=\"Optional custom name for the predictions output file.\")\n",
    "    # Add generation arguments\n",
    "    parser.add_argument(\"--max_len\", type=int, default=200, help=\"Max new tokens for generation.\")\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.2, help=\"Generation temperature (0 for greedy).\")\n",
    "    parser.add_argument(\"--top_p\", type=float, default=None, help=\"Nucleus sampling top_p (e.g., 0.9).\")\n",
    "    parser.add_argument(\"--ablation\", type=str, default=None, choices=[None, 'baseline'], # Add ablation choices\n",
    "                    help=\"Optional ablation mode being evaluated (e.g., 'baseline'). Affects weight loading path.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Resolve config path relative to the determined project root\n",
    "    config_file_path = project_root / args.config\n",
    "    model_ckpt_path = Path(args.model_checkpoint) if args.model_checkpoint else None\n",
    "    if model_ckpt_path and not model_ckpt_path.is_absolute(): # Resolve relative checkpoint path w.r.t project root\n",
    "        model_ckpt_path = project_root / model_ckpt_path\n",
    "    \n",
    "    if not config_file_path.is_file():\n",
    "        print(f\"Error: Config file not found at {config_file_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Pass generation args to run_evaluation\n",
    "    gen_args = {\n",
    "        \"max_len\": args.max_len,\n",
    "        \"temperature\": args.temperature,\n",
    "        \"top_p\": args.top_p\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        run_evaluation(\n",
    "            config_path=config_file_path,\n",
    "            model_type=args.model_type, # Pass model type\n",
    "            model_checkpoint_path=model_ckpt_path,\n",
    "            dataset_name=args.dataset,\n",
    "            ablation_mode=args.ablation,\n",
    "            output_filename=args.output_filename,\n",
    "            **gen_args\n",
    "        )\n",
    "    except NotImplementedError as e:\n",
    "         print(f\"Exiting due to NotImplementedError: {e}\")\n",
    "         sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation script failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
