{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> Functions for evaluating LLaVA models, including prediction generation and metric calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.load import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root\n",
    "project_root = Path(os.getcwd())\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")\n",
    "    \n",
    "# Import necessary llava components\n",
    "try:\n",
    "    from llava.utils import load_config\n",
    "    from llava.data.loading import get_test_dataloader # Assumes this will be added to 10_data_loading\n",
    "    from llava.model.baseline import BaselineLLaVAModel # Or AdaptiveLLaVAModel later\n",
    "    from llava.data.preprocessing import tokenizer, DEFAULT_IMAGE_TOKEN, IGNORE_INDEX # Import tokenizer\n",
    "    from llava.training.stage2 import get_stage2_learner # To load trained models\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing llava modules: {e}. Make sure nbdev_export was run.\")\n",
    "    # Define placeholders if running standalone or during initial setup\n",
    "    def load_config(path): return {}\n",
    "    def get_test_dataloader(config, dataset_name): raise NotImplementedError\n",
    "    class BaselineLLaVAModel(torch.nn.Module): \n",
    "        def __init__(self, *args, **kwargs): super().__init__(); self.dummy = torch.nn.Linear(1,1)\n",
    "        def forward(self, *args, **kwargs): return {'logits': torch.randn(1, 10, 100)}\n",
    "    tokenizer = None\n",
    "    DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "    IGNORE_INDEX = -100\n",
    "    def get_stage2_learner(config): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Prediction Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a trained fastai `Learner` and a test `DataLoader`, generates predictions, decodes them into text, and saves them to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_predictions(learner: Learner, \n",
    "                         test_dl: DataLoader, \n",
    "                         output_file: Union[str, Path], \n",
    "                         max_len: int = 200, # Max generation length\n",
    "                         temperature: float = 0.2, # Generation temperature\n",
    "                         top_p: float = None, # Nucleus sampling top_p\n",
    "                        ):\n",
    "    \"\"\"Generates predictions for a test dataloader and saves them to a JSON Lines file.\n",
    "\n",
    "    Args:\n",
    "        learner: Trained fastai Learner object containing the model and tokenizer.\n",
    "        test_dl: DataLoader for the test set.\n",
    "        output_file: Path to save the JSON Lines prediction file.\n",
    "        max_len: Maximum number of new tokens to generate.\n",
    "        temperature: Sampling temperature for generation.\n",
    "        top_p: If set, use nucleus sampling with this top_p value.\n",
    "    \"\"\"\n",
    "    output_file = Path(output_file)\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model = learner.model\n",
    "    # Assume tokenizer is accessible via learner.dls.tokenizer or learner.tokenizer\n",
    "    if hasattr(learner.dls, 'tokenizer'):\n",
    "        tok = learner.dls.tokenizer\n",
    "    elif hasattr(learner, 'tokenizer'): # Or maybe stored directly in learner?\n",
    "         tok = learner.tokenizer\n",
    "    else:\n",
    "        # Fallback: Try to import the global tokenizer (less robust)\n",
    "        global tokenizer\n",
    "        if tokenizer is None:\n",
    "             raise AttributeError(\"Tokenizer not found in learner or globally. Cannot decode predictions.\")\n",
    "        tok = tokenizer\n",
    "        print(\"Warning: Using global tokenizer instance.\")\n",
    "        \n",
    "    model.eval() # Set model to evaluation mode\n",
    "    results = []\n",
    "    total_time = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    print(f\"Generating predictions for {len(test_dl.dataset)} samples...\")\n",
    "    \n",
    "    with torch.no_grad(), open(output_file, 'w') as f_out:\n",
    "        for batch in tqdm(test_dl, desc=\"Generating Predictions\"):\n",
    "            # Move batch items to appropriate device\n",
    "            # learner.dls.device should provide the correct device\n",
    "            device = learner.dls.device\n",
    "            batch = learner.dls.after_batch(batch) # Apply batch transforms if needed\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device) # This contains the prompt and -200 marker\n",
    "            # attention_mask = batch['attention_mask'].to(device)\n",
    "            # labels = batch.get('labels') # Get labels if available in test set\n",
    "            # Assume test_dl.dataset contains sample IDs or image paths\n",
    "            # We need a way to map batch index back to sample ID\n",
    "            # This depends on how the test dataset is structured\n",
    "            # For now, let's use a dummy ID\n",
    "            batch_size = pixel_values.shape[0]\n",
    "\n",
    "            start_time = time.time()\n",
    "            # --- Generation --- \n",
    "            # The model's forward pass doesn't directly generate. We need generate method.\n",
    "            # If the model is a HF model (or wrapped HF model), we can use its .generate()\n",
    "            # Assumes BaselineLLaVAModel has or wraps a model with a generate method\n",
    "            # The current BaselineLLaVAModel forward pass returns logits, not suitable for direct generation.\n",
    "            # We need to adapt the model or use a different approach for generation.\n",
    "            \n",
    "            # **Temporary Approach (assuming HF model structure):**\n",
    "            # This requires the model to be compatible with HF generate\n",
    "            # which means the forward pass needs adjustment or we call generate directly\n",
    "            # on the underlying `language_model` component, manually preparing embeddings.\n",
    "            \n",
    "            # **Revised approach: Call generate on the underlying HF LLM**\n",
    "            # 1. Get image embeddings\n",
    "            image_features = model.encode_image(pixel_values)\n",
    "            projected_features = model.projector(image_features)\n",
    "            \n",
    "            # 2. Get text embeddings for the prompt part\n",
    "            # Find where the generation should start (after the prompt)\n",
    "            # The input_ids here already have the -200 marker\n",
    "            \n",
    "            outputs_list = []\n",
    "            for i in range(batch_size):\n",
    "                # Prepare inputs for this specific sample\n",
    "                current_input_ids = input_ids[i:i+1] # Keep batch dim\n",
    "                current_proj_features = projected_features[i:i+1]\n",
    "                \n",
    "                # Find the placeholder marker\n",
    "                image_token_indices = torch.where(current_input_ids[0] == model.image_token_index_marker)[0]\n",
    "                if len(image_token_indices) == 0:\n",
    "                    print(f\"Warning: No image token marker found in sample {i}. Skipping generation.\")\n",
    "                    outputs_list.append(torch.tensor([tok.eos_token_id], device=device)) # Append EOS as fallback\n",
    "                    continue\n",
    "                    \n",
    "                image_token_start_index = image_token_indices[0].item()\n",
    "                \n",
    "                # Create prompt embeddings by replacing marker\n",
    "                input_ids_no_marker = current_input_ids.clone()\n",
    "                input_ids_no_marker[input_ids_no_marker == model.image_token_index_marker] = 0 # Replace marker for embedding lookup\n",
    "                text_embeddings = model.get_input_embeddings()(input_ids_no_marker)\n",
    "                \n",
    "                text_emb_before = text_embeddings[:, :image_token_start_index]\n",
    "                text_emb_after = text_embeddings[:, image_token_start_index + 1:]\n",
    "                \n",
    "                prompt_embeds = torch.cat([\n",
    "                    text_emb_before,\n",
    "                    current_proj_features.to(text_embeddings.device, dtype=text_embeddings.dtype),\n",
    "                    text_emb_after\n",
    "                ], dim=1)\n",
    "                \n",
    "                # Generate using the underlying LLM component\n",
    "                # Need to handle PEFT model structure\n",
    "                llm_component = model.language_model.base_model if _peft_available and isinstance(model.language_model, PeftModel) else model.language_model\n",
    "                \n",
    "                # Create attention mask for the prompt embeddings\n",
    "                # Assume all tokens in the constructed prompt_embeds are attended to\n",
    "                prompt_attention_mask = torch.ones(prompt_embeds.shape[:2], dtype=torch.long, device=device)\n",
    "                \n",
    "                # Note: The `generate` function expects input_ids OR inputs_embeds.\n",
    "                # If using inputs_embeds, attention_mask is also needed.\n",
    "                # The generate function handles the autoregressive decoding loop.\n",
    "                try:\n",
    "                    output_ids = llm_component.generate(\n",
    "                        inputs_embeds=prompt_embeds,\n",
    "                        attention_mask=prompt_attention_mask, # Mask for the prompt\n",
    "                        max_new_tokens=max_len,\n",
    "                        eos_token_id=tok.eos_token_id,\n",
    "                        pad_token_id=tok.pad_token_id,\n",
    "                        temperature=temperature,\n",
    "                        top_p=top_p,\n",
    "                        do_sample=(temperature > 0), # Enable sampling if temp > 0\n",
    "                        num_beams=1 # Use greedy or sampling, not beam search by default\n",
    "                    )\n",
    "                    # Output includes the prompt tokens, remove them\n",
    "                    output_ids = output_ids[:, prompt_embeds.shape[1]:]\n",
    "                    outputs_list.append(output_ids[0]) # Get the single sequence\n",
    "                except Exception as gen_e:\n",
    "                     print(f\"Error during generation for sample {i}: {gen_e}\")\n",
    "                     outputs_list.append(torch.tensor([tok.eos_token_id], device=device)) # Fallback\n",
    "                     \n",
    "            end_time = time.time()\n",
    "            total_time += (end_time - start_time)\n",
    "            num_samples += batch_size\n",
    "            \n",
    "            # --- Decode and Save --- \n",
    "            # Assumes test_dl.items provides identifiers (e.g., sample_id or image_path)\n",
    "            # This mapping might be complex depending on the test dataset structure.\n",
    "            # Placeholder: Use index as ID for now.\n",
    "            start_idx = num_samples - batch_size\n",
    "            item_ids = [test_dl.items[start_idx + i].sample_id \n",
    "                        if hasattr(test_dl.items[start_idx + i], 'sample_id') \n",
    "                        else f\"sample_{start_idx + i}\" \n",
    "                        for i in range(batch_size)]\n",
    "\n",
    "            for i, gen_ids in enumerate(outputs_list):\n",
    "                decoded_text = tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "                # Get ground truth if available (e.g., from dataset item)\n",
    "                # ground_truth = \"\" # Placeholder\n",
    "                # if labels is not None and i < len(labels):\n",
    "                #    gt_ids = labels[i][labels[i] != IGNORE_INDEX].tolist()\n",
    "                #    ground_truth = tok.decode(gt_ids, skip_special_tokens=True).strip()\n",
    "                \n",
    "                result_entry = {\n",
    "                    \"id\": item_ids[i], \n",
    "                    \"prediction\": decoded_text,\n",
    "                    # \"ground_truth\": ground_truth # Add if available\n",
    "                }\n",
    "                f_out.write(json.dumps(result_entry) + '\\n')\n",
    "                results.append(result_entry) # Also keep in memory if needed\n",
    "                \n",
    "    avg_latency = (total_time / num_samples) * 1000 if num_samples > 0 else 0\n",
    "    print(f\"Finished generation. Saved {len(results)} predictions to {output_file}\")\n",
    "    print(f\"Average inference latency: {avg_latency:.2f} ms/sample\")\n",
    "    \n",
    "    # Store latency for efficiency logging (Step 5.4)\n",
    "    # This could be logged to W&B or returned\n",
    "    # Example: wandb.log({\"avg_inference_latency_ms\": avg_latency}) \n",
    "\n",
    "    # Measure peak inference VRAM (Step 5.4)\n",
    "    if torch.cuda.is_available():\n",
    "        peak_vram = torch.cuda.max_memory_allocated() / (1024**3) # Convert bytes to GB\n",
    "        print(f\"Peak Inference VRAM used: {peak_vram:.2f} GB\")\n",
    "        # Example: wandb.log({\"peak_inference_vram_gb\": peak_vram})\n",
    "        torch.cuda.reset_peak_memory_stats() # Reset for next measurement\n",
    "        \n",
    "    return results # Return list of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3: Integrate External Evaluation Scripts (Placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define placeholder functions for calling external evaluation scripts. These will be implemented fully in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def evaluate_vqa(preds_file: Union[str, Path], gt_file: Union[str, Path], **kwargs):\n",
    "    \"\"\"Placeholder function to evaluate VQAv2 predictions.\n",
    "    \n",
    "    Args:\n",
    "        preds_file: Path to the JSON prediction file.\n",
    "        gt_file: Path to the ground truth annotation file.\n",
    "        **kwargs: Additional arguments for the VQA eval API.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing VQA scores (e.g., {'overall': score}).\n",
    "    \"\"\"\n",
    "    print(f\"--- VQA Evaluation (Placeholder) ---\")\n",
    "    print(f\"Predictions: {preds_file}\")\n",
    "    print(f\"Ground Truth: {gt_file}\")\n",
    "    # TODO: Implement actual call to VQA evaluation API (Step 5.5)\n",
    "    # Example: Use subprocess to run official script or import library\n",
    "    print(\"Actual VQA evaluation logic not yet implemented.\")\n",
    "    dummy_score = 0.5 # Placeholder score\n",
    "    # wandb.log({\"vqa_score_overall\": dummy_score}) # Log metric\n",
    "    return {\"overall\": dummy_score}\n",
    "\n",
    "def evaluate_textvqa(preds_file: Union[str, Path], gt_file: Union[str, Path], **kwargs):\n",
    "    \"\"\"Placeholder function to evaluate TextVQA/DocVQA predictions using ANLS.\n",
    "\n",
    "    Args:\n",
    "        preds_file: Path to the JSON prediction file.\n",
    "        gt_file: Path to the ground truth annotation file.\n",
    "        **kwargs: Additional arguments for ANLS calculation.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing ANLS score (e.g., {'anls': score}).\n",
    "    \"\"\"\n",
    "    print(f\"--- TextVQA/DocVQA ANLS Evaluation (Placeholder) ---\")\n",
    "    print(f\"Predictions: {preds_file}\")\n",
    "    print(f\"Ground Truth: {gt_file}\")\n",
    "    # TODO: Implement actual ANLS calculation (Step 5.6)\n",
    "    print(\"Actual ANLS calculation logic not yet implemented.\")\n",
    "    dummy_anls = 0.4 # Placeholder score\n",
    "    # wandb.log({\"anls\": dummy_anls}) # Log metric\n",
    "    return {\"anls\": dummy_anls}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Script (Placeholder Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_evaluation(config_path: str | Path, \n",
    "                   model_checkpoint: Optional[str | Path] = None, \n",
    "                   dataset_name: str = 'vqav2_test', # Example default\n",
    "                   output_filename: Optional[str] = None):\n",
    "    \"\"\"Runs the evaluation pipeline: load model, generate predictions, evaluate.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the main configuration YAML.\n",
    "        model_checkpoint: Path to the specific model checkpoint to load (e.g., LoRA adapters dir or full model).\n",
    "                          If None, uses the path from config['paths']['stage2_model_weights'].\n",
    "        dataset_name: Name of the dataset split to evaluate (e.g., 'vqav2_test', 'textvqa_val').\n",
    "        output_filename: Name for the prediction output file (defaults based on model/dataset).\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Evaluation Run --- \")\n",
    "    print(f\"Config: {config_path}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    \n",
    "    config = load_config(config_path)\n",
    "    output_dir = Path(config['paths']['output_dir']) / 'eval_results'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    learner = None\n",
    "    try:\n",
    "        # 1. Load Model (This needs refinement depending on how Stage 2 weights are saved)\n",
    "        print(\"Loading model for evaluation...\")\n",
    "        # Option 1: Reload using Learner setup (if full model was saved)\n",
    "        # learner = get_stage2_learner(config) # Re-runs setup, maybe inefficient\n",
    "        # checkpoint_name = Path(model_checkpoint or config['paths']['stage2_model_weights']).stem\n",
    "        # learner.load(checkpoint_name) \n",
    "        \n",
    "        # Option 2: Instantiate model and load weights manually (more flexible)\n",
    "        # This requires knowing if LoRA was used and loading adapters separately.\n",
    "        model = BaselineLLaVAModel(config) # Instantiate base structure\n",
    "        \n",
    "        # TODO: Add logic here to load specific weights (projector + LoRA/full LLM)\n",
    "        # based on `model_checkpoint` or config paths.\n",
    "        # Example (needs refinement):\n",
    "        model_base_name = Path(config['paths']['stage2_model_weights']).stem\n",
    "        proj_weights_path = output_dir.parent / 'models' / f\"{model_base_name}_projector_final.pth\"\n",
    "        lora_adapter_path = output_dir.parent / 'models' / f\"{model_base_name}_lora_adapters\"\n",
    "        \n",
    "        if proj_weights_path.exists():\n",
    "            model.projector.load_state_dict(torch.load(proj_weights_path, map_location='cpu'))\n",
    "            print(f\"Loaded projector weights from {proj_weights_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Projector weights not found at {proj_weights_path}\")\n",
    "            \n",
    "        if config['model']['peft']['use_lora'] and lora_adapter_path.exists() and _peft_available and isinstance(model.language_model, PeftModel):\n",
    "            from peft import PeftModel # Re-import locally just in case\n",
    "            # Assumes model was already created with get_peft_model in BaselineLLaVAModel init\n",
    "            # Loading adapters into the existing PeftModel\n",
    "            model.language_model.load_adapter(str(lora_adapter_path), adapter_name=\"default\")\n",
    "            print(f\"Loaded LoRA adapters from {lora_adapter_path}\")\n",
    "        elif config['model']['peft']['use_lora']:\n",
    "            print(f\"Warning: LoRA enabled but adapters not found at {lora_adapter_path} or PEFT issue.\")\n",
    "        else:\n",
    "             print(\"LoRA not used. Assuming full LLM fine-tuning (loading not implemented here yet).\")\n",
    "             # Need logic to load full saved model state if not using LoRA\n",
    "             \n",
    "        # Create a minimal learner shell for convenience (e.g., accessing device, tokenizer)\n",
    "        # Get dummy dls just to create learner instance\n",
    "        # TODO: Need a more robust way to get device/tokenizer if not using full learner load\n",
    "        dummy_dl = DataLoader(list(range(1)), bs=1) # Dummy dataloader\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        learner = Learner(dls=dummy_dl, model=model, loss_func=lambda x,y: 0) # Dummy loss\n",
    "        # Manually assign tokenizer if needed for generate_predictions\n",
    "        global tokenizer\n",
    "        learner.tokenizer = tokenizer \n",
    "        learner.dls.device = device # Set device explicitly\n",
    "        print(\"Minimal Learner shell created for evaluation.\")\n",
    "        \n",
    "        # 2. Load Test Data\n",
    "        print(f\"Loading test data for {dataset_name}...\")\n",
    "        test_dl = get_test_dataloader(config, dataset_name)\n",
    "        if test_dl is None:\n",
    "            raise RuntimeError(f\"Failed to load test dataloader for {dataset_name}\")\n",
    "        learner.dls.test = test_dl # Assign test_dl for learner convenience\n",
    "\n",
    "        # 3. Generate Predictions\n",
    "        if output_filename is None:\n",
    "            model_id = Path(model_checkpoint or config['paths']['stage2_model_weights']).stem\n",
    "            output_filename = f\"preds_{model_id}_{dataset_name}.jsonl\"\n",
    "        preds_file = output_dir / output_filename\n",
    "        \n",
    "        generate_predictions(learner, test_dl, preds_file)\n",
    "\n",
    "        # 4. Run Evaluation Scripts\n",
    "        print(\"Running evaluation metrics...\")\n",
    "        if 'vqav2' in dataset_name.lower():\n",
    "            gt_file = Path(config['paths']['data_base']) / config['paths']['vqav2_test_annotations']\n",
    "            vqa_results = evaluate_vqa(preds_file, gt_file)\n",
    "            print(f\"VQAv2 Results: {vqa_results}\")\n",
    "        elif 'textvqa' in dataset_name.lower():\n",
    "            gt_file = Path(config['paths']['data_base']) / config['paths']['textvqa_val'] # Assuming val set GT\n",
    "            anls_results = evaluate_textvqa(preds_file, gt_file)\n",
    "            print(f\"TextVQA ANLS Results: {anls_results}\")\n",
    "        # Add more evaluation cases for other datasets (DocVQA, ChartQA, Custom)\n",
    "        else:\n",
    "            print(f\"No specific evaluation script configured for dataset: {dataset_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Clean up memory\n",
    "        if learner is not None and hasattr(learner, 'model') and learner.model is not None:\n",
    "            learner.model.to('cpu')\n",
    "            del learner.model\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Cleaned up model memory.\")\n",
    "        # Finish W&B run if active (unlikely here, usually tied to training run)\n",
    "        # if wandb.run is not None: wandb.finish()\n",
    "            \n",
    "    print(f\"--- Evaluation Run Complete --- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/40_evaluation.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "llava"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}