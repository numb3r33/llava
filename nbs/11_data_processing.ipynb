{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "> Functions and definitions for preprocessing steps, including normalization stats, tokenization, and template formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "PARENT_PATH = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoImageProcessor\n",
    "from fastai.vision.augment import Normalize\n",
    "from fastai.vision.all import *\n",
    "from fastai.text.all import *\n",
    "from fastai.data.transforms import Transform\n",
    "from fastai.torch_core import TensorBase, tensor\n",
    "import torch\n",
    "from typing import List, Dict, Union, Tuple, Any\n",
    "\n",
    "from llava.utils import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2 (Continued): Image Normalization Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CLIP image processor to get the correct normalization statistics (mean and standard deviation) required for the vision encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0f7c562b9e4a3d84bfca5893621920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Load config to get model name\n",
    "CONFIG_PATH = 'configs/config.yaml'\n",
    "try:\n",
    "    config = load_config(CONFIG_PATH)\n",
    "    VISION_ENCODER_NAME = config['model']['vision_encoder_name_or_path']\n",
    "    LLM_NAME = config['model']['llm_name_or_path'] # Added for tokenizer\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Config file not found at {CONFIG_PATH}. Using default model names.\")\n",
    "    VISION_ENCODER_NAME = 'openai/clip-vit-large-patch14-336' # Fallback\n",
    "    LLM_NAME = 'lmsys/vicuna-7b-v1.5' # Fallback\n",
    "except KeyError as e:\n",
    "    print(f\"Warning: Key {e} not found in {CONFIG_PATH}. Using defaults.\")\n",
    "    VISION_ENCODER_NAME = config.get('model', {}).get('vision_encoder_name_or_path', 'openai/clip-vit-large-patch14-336')\n",
    "    LLM_NAME = config.get('model', {}).get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')\n",
    "\n",
    "# Load the CLIP image processor\n",
    "try:\n",
    "    clip_image_processor = AutoImageProcessor.from_pretrained(VISION_ENCODER_NAME)\n",
    "    print(f\"Successfully loaded CLIP image processor for: {VISION_ENCODER_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CLIP image processor for {VISION_ENCODER_NAME}: {e}\")\n",
    "    # Handle error appropriately, maybe raise or use default stats\n",
    "    clip_image_processor = None\n",
    "\n",
    "# Get normalization stats\n",
    "if clip_image_processor:\n",
    "    image_mean = clip_image_processor.image_mean\n",
    "    image_std = clip_image_processor.image_std\n",
    "else:\n",
    "    print(\"Warning: Using default ImageNet stats as fallback for normalization.\")\n",
    "    # Default fallback (ImageNet stats often used, but CLIP specific is better)\n",
    "    image_mean = [0.485, 0.456, 0.406]\n",
    "    image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create the fastai Normalize transform using CLIP stats\n",
    "clip_normalize = Normalize.from_stats(image_mean, image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Mean: [0.48145466, 0.4578275, 0.40821073]\n",
      "CLIP Std: [0.26862954, 0.26130258, 0.27577711]\n",
      "Fastai Normalize Transform: Normalize -- {'mean': tensor([[[[0.4815]],\n",
      "\n",
      "         [[0.4578]],\n",
      "\n",
      "         [[0.4082]]]], device='cuda:0'), 'std': tensor([[[[0.2686]],\n",
      "\n",
      "         [[0.2613]],\n",
      "\n",
      "         [[0.2758]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "(enc:1,dec:1)\n"
     ]
    }
   ],
   "source": [
    "# Example: Print the stats and the transform\n",
    "print(f\"CLIP Mean: {image_mean}\")\n",
    "print(f\"CLIP Std: {image_std}\")\n",
    "print(f\"Fastai Normalize Transform: {clip_normalize}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: Text Tokenization and Template Handling (Stage 1 - Plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LLM's tokenizer (Vicuna) and define the 'plain' template formatting for Stage 1 pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c13fa9bb0a46b287647b0ab9b6b520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b883f84f6e340bbbc9c29aa9f28d391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc63b66bd63240f19c241256ff8760e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\" # Placeholder token for image features\n",
    "IMAGE_TOKEN_INDEX_PLACEHOLDER = -200 # Special marker used internally in input_ids\n",
    "IGNORE_INDEX = -100 # Standard ignore index for labels in loss calculation\n",
    "\n",
    "# Load the Vicuna tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        LLM_NAME,\n",
    "        model_max_length=config.get('data', {}).get('tokenizer_model_max_length', 2048),\n",
    "        padding_side=config.get('data', {}).get('tokenizer_padding_side', 'right'),\n",
    "        use_fast=True,\n",
    "    )\n",
    "    print(f\"Successfully loaded tokenizer for: {LLM_NAME}\")\n",
    "\n",
    "    # Add the image token as a special token, if it's not already there.\n",
    "    # This ensures the tokenizer knows about it, even though we replace its embedding later.\n",
    "    # Avoids warnings and potential token splitting issues.\n",
    "    # Check if the token already exists\n",
    "    current_vocab = tokenizer.get_vocab()\n",
    "    if DEFAULT_IMAGE_TOKEN not in current_vocab:\n",
    "        print(f\"Adding special token {DEFAULT_IMAGE_TOKEN} to tokenizer.\")\n",
    "        # Add the token as a *special* token. This typically means it won't be split\n",
    "        # and can be handled distinctly. `additional_special_tokens` is a common way.\n",
    "        num_added = tokenizer.add_special_tokens({'additional_special_tokens': [DEFAULT_IMAGE_TOKEN]})\n",
    "        if num_added > 0:\n",
    "            print(f\"Added {num_added} token(s). New vocab size: {len(tokenizer)}\")\n",
    "            # NOTE: If a token is truly added (increasing vocab size),\n",
    "            # the model's embedding layer MUST be resized later.\n",
    "            # For LLaVA, we usually map it to an existing ID or rely on the -200 replacement,\n",
    "            # but adding it as a special token helps tokenizer processing.\n",
    "    # Retrieve the assigned ID\n",
    "    IMAGE_TOKEN_ID = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)\n",
    "    print(f\"Using token ID for {DEFAULT_IMAGE_TOKEN}: {IMAGE_TOKEN_ID}\")\n",
    "    # Handle case where it might still be UNK if not properly added/found\n",
    "    if IMAGE_TOKEN_ID == tokenizer.unk_token_id:\n",
    "         print(f\"Warning: {DEFAULT_IMAGE_TOKEN} resolved to UNK token ID ({tokenizer.unk_token_id}). Check tokenizer setup.\")\n",
    "\n",
    "\n",
    "    # Set pad token if missing (common for LLaMA models)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token # Option 1: Use EOS\n",
    "        # tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Option 2: Add a new pad token\n",
    "        print(f\"Using EOS token as pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "        # Note: If adding a new pad token, the model's embedding layer needs resizing later!\n",
    "    else:\n",
    "        print(f\"Tokenizer already has pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer for {LLM_NAME}: {e}\")\n",
    "    tokenizer = None\n",
    "    IMAGE_TOKEN_ID = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_plain_template(conversations: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"Formats conversations using the 'plain' template for Stage 1 pre-training.\n",
    "\n",
    "    The 'plain' template uses the format: <image>\\n{caption}\\n\n",
    "    where {caption} is the value of the first 'gpt' turn.\n",
    "\n",
    "    Args:\n",
    "        conversations: A list of conversation turns (dictionaries with 'from' and 'value').\n",
    "\n",
    "    Returns:\n",
    "        The formatted string. Returns just the image token if no 'gpt' turn is found.\n",
    "    \"\"\"\n",
    "    caption = \"\" # Default to empty caption if no 'gpt' turn found\n",
    "    for turn in conversations:\n",
    "        if turn.get('from', '').lower() == 'gpt':\n",
    "            caption = turn.get('value', '')\n",
    "            break # Use the first GPT response as the caption\n",
    "\n",
    "    # Ensure the <image> token is always first, followed by newline and caption\n",
    "    # Remove any existing <image> token from caption to avoid duplicates\n",
    "    caption = caption.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "\n",
    "    # Return formatted string, handle empty caption case\n",
    "    # Add final newline consistent with some implementations?\n",
    "    # LLaVA reference often has <image>\\n{caption}\n",
    "    # Let's stick to <image>\\n{caption} for now, stripping trailing whitespace.\n",
    "    formatted = f\"{DEFAULT_IMAGE_TOKEN}\\n{caption}\".strip() if caption else f\"{DEFAULT_IMAGE_TOKEN}\"\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### format_plain_template\n",
       "\n",
       ">      format_plain_template (conversations:List[Dict[str,str]])\n",
       "\n",
       "*Formats conversations using the 'plain' template for Stage 1 pre-training.\n",
       "\n",
       "    The 'plain' template uses the format: <image>\n",
       "{caption}\n",
       "\n",
       "    where {caption} is the value of the first 'gpt' turn.\n",
       "\n",
       "    Args:\n",
       "        conversations: A list of conversation turns (dictionaries with 'from' and 'value').\n",
       "\n",
       "    Returns:\n",
       "        The formatted string. Returns just the image token if no 'gpt' turn is found.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### format_plain_template\n",
       "\n",
       ">      format_plain_template (conversations:List[Dict[str,str]])\n",
       "\n",
       "*Formats conversations using the 'plain' template for Stage 1 pre-training.\n",
       "\n",
       "    The 'plain' template uses the format: <image>\n",
       "{caption}\n",
       "\n",
       "    where {caption} is the value of the first 'gpt' turn.\n",
       "\n",
       "    Args:\n",
       "        conversations: A list of conversation turns (dictionaries with 'from' and 'value').\n",
       "\n",
       "    Returns:\n",
       "        The formatted string. Returns just the image token if no 'gpt' turn is found.*"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(format_plain_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Template Formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 (Standard): <image>\n",
      "This is the caption.\n",
      "Test Case 2 (Image token in caption): <image>\n",
      "Caption with image token removed.\n",
      "Test Case 3 (No GPT turn): <image>\n",
      "Test Case 4 (Empty conversation): <image>\n"
     ]
    }
   ],
   "source": [
    "conv1 = [{'from': 'human', 'value': '<image>\\nDescribe.'}, {'from': 'gpt', 'value': 'This is the caption.'}]\n",
    "conv2 = [{'from': 'human', 'value': 'Describe.'}, {'from': 'gpt', 'value': '<image>Caption with image token removed.'}]\n",
    "conv3 = [{'from': 'human', 'value': '<image>\\nDescribe.'}]\n",
    "conv4 = []\n",
    "\n",
    "print(f\"Test Case 1 (Standard): {format_plain_template(conv1)}\")\n",
    "print(f\"Test Case 2 (Image token in caption): {format_plain_template(conv2)}\")\n",
    "print(f\"Test Case 3 (No GPT turn): {format_plain_template(conv3)}\")\n",
    "print(f\"Test Case 4 (Empty conversation): {format_plain_template(conv4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLaVATextTokenizer(Transform):\n",
    "    \"\"\"A fastai Transform to format and tokenize text data for LLaVA stage 1.\n",
    "\n",
    "    Applies the 'plain' template formatting and then tokenizes the text, returning only the input IDs.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, template_formatter=format_plain_template):\n",
    "        store_attr()\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided and loaded successfully.\")\n",
    "\n",
    "    def encodes(self, conversations: list) -> list:\n",
    "        \"\"\"Applies formatting and tokenization to conversation data.\n",
    "\n",
    "        Args:\n",
    "            conversations: Raw conversation list from the dataset sample.\n",
    "\n",
    "        Returns:\n",
    "            A list of input token IDs.\n",
    "        \"\"\"\n",
    "        formatted_text = self.template_formatter(conversations)\n",
    "        # Tokenize the formatted text.\n",
    "        # We don't pad here; padding is done at the batch level.\n",
    "        tokenized_output = self.tokenizer(formatted_text,\n",
    "                                         return_tensors=None, # Let batch collation handle tensor conversion + padding\n",
    "                                         add_special_tokens=True, # Add BOS/EOS if tokenizer configured to do so\n",
    "                                         truncation=False # Truncation can be done later if needed\n",
    "                                        )\n",
    "        # Return just the input_ids list for DataBlock item_tfms\n",
    "        # We use TensorBase here so fastai recognizes it as tensor-like for collation\n",
    "        # return TensorBase(tokenized_output['input_ids'])\n",
    "        # Update: Returning raw list might be simpler, collation handles tensor conversion\n",
    "        return tokenized_output['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Tokenizer Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Conversations: [{'from': 'human', 'value': '<image>\\nDescribe this.'}, {'from': 'gpt', 'value': 'It is red.'}]\n",
      "Formatted Text: <image>\n",
      "It is red.\n",
      "Tokenized Output (Input IDs): [1, 32000, 13, 3112, 338, 2654, 29889]\n",
      "Decoded Tokens: ['<s>', '<image>', '<0x0A>', 'It', '▁is', '▁red', '.']\n"
     ]
    }
   ],
   "source": [
    "if tokenizer:\n",
    "    # Example conversation\n",
    "    example_conv = [{'from': 'human', 'value': '<image>\\nDescribe this.'}, {'from': 'gpt', 'value': 'It is red.'}]\n",
    "\n",
    "    # Create the transform instance\n",
    "    llava_tokenizer_tfm = LLaVATextTokenizer(tokenizer)\n",
    "\n",
    "    # Apply the transform\n",
    "    tokenized_ids = llava_tokenizer_tfm(example_conv)\n",
    "\n",
    "    print(f\"Original Conversations: {example_conv}\")\n",
    "    # Re-format to show what was tokenized\n",
    "    formatted = format_plain_template(example_conv)\n",
    "    print(f\"Formatted Text: {formatted}\")\n",
    "    print(f\"Tokenized Output (Input IDs): {tokenized_ids}\")\n",
    "\n",
    "    # Decode for verification\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_ids)\n",
    "    print(f\"Decoded Tokens: {decoded_tokens}\")\n",
    "\n",
    "    # Ensure the output is a list of integers\n",
    "    assert isinstance(tokenized_ids, list)\n",
    "    assert all(isinstance(x, int) for x in tokenized_ids)\n",
    "else:\n",
    "    print(\"Tokenizer not loaded, skipping tokenizer transform test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Implement Custom Batch Transform / Collate Function (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transform handles batch-level operations: padding, masking, image token replacement, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLaVABatchTransform(Transform):\n",
    "    \"\"\" Custom batch transform for LLaVA stage 1.\n",
    "        Handles image normalization, text padding reconstruction, attention mask creation,\n",
    "        image token marker replacement, label creation, and label masking for the 'plain' template.\n",
    "        Operates on the default collated batch tuple (images, list_of_positional_token_tensors).\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, normalize_tfm: Normalize, image_token_id=None):\n",
    "        store_attr() # Stores tokenizer, normalize_tfm\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided.\")\n",
    "\n",
    "        if image_token_id is None:\n",
    "            self.image_token_id = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)\n",
    "            if self.image_token_id == tokenizer.unk_token_id:\n",
    "                print(f\"Warning: {DEFAULT_IMAGE_TOKEN} not found in tokenizer vocab. Using UNK ID: {self.image_token_id}\")\n",
    "        else:\n",
    "            self.image_token_id = image_token_id\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        if self.pad_token_id is None:\n",
    "            print(f\"Warning: Tokenizer has no pad_token_id. Assuming fastai default padding (often 0) or using eos_token_id ({self.tokenizer.eos_token_id}) if specified.\")\n",
    "            # We will reconstruct the mask based on the actual pad ID used by fastai/tokenizer\n",
    "            # Let's assume the tokenizer's pad_token_id IS defined now after previous setup steps\n",
    "            # If not, add error handling here.\n",
    "            if self.tokenizer.pad_token_id is None:\n",
    "                 raise ValueError(\"Tokenizer must have a pad_token_id after setup.\")\n",
    "            self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        print(f\"LLaVABatchTransform initialized. Image Token ID: {self.image_token_id}, Pad Token ID: {self.pad_token_id}\")\n",
    "\n",
    "    def encodes(self, collated_batch: tuple) -> dict:\n",
    "        \"\"\"Applies normalization, reconstructs padded tensors, applies masking.\n",
    "\n",
    "        Args:\n",
    "            collated_batch: A tuple containing:\n",
    "                (collated_image_tensors: torch.Tensor,\n",
    "                 list_of_positional_token_tensors: List[torch.Tensor])\n",
    "                as produced by default fastai collation for sequences.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the fully processed batch ready for the model.\n",
    "        \"\"\"\n",
    "        # 1. Unpack the collated batch\n",
    "        if not isinstance(collated_batch, tuple) or len(collated_batch) != 2:\n",
    "             print(f\"Warning: LLaVABatchTransform received unexpected input type: {type(collated_batch)}. Skipping.\")\n",
    "             return collated_batch\n",
    "\n",
    "        collated_images, list_of_positional_tensors = collated_batch\n",
    "\n",
    "        # Ensure correct types\n",
    "        if not isinstance(collated_images, torch.Tensor):\n",
    "             raise TypeError(f\"Expected first element of collated batch to be a Tensor, got {type(collated_images)}\")\n",
    "        if not isinstance(list_of_positional_tensors, list) or not all(isinstance(t, torch.Tensor) for t in list_of_positional_tensors):\n",
    "             raise TypeError(f\"Expected second element of collated batch to be a list of Tensors, got {type(list_of_positional_tensors)}\")\n",
    "        if not list_of_positional_tensors: # Handle empty batch case if it occurs\n",
    "            print(\"Warning: Received empty list of positional tensors.\")\n",
    "            # Return empty dict or handle appropriately\n",
    "            return {}\n",
    "\n",
    "\n",
    "        # 2. Normalize images\n",
    "        normalized_images = self.normalize_tfm(collated_images) # Use the stored normalize_tfm\n",
    "\n",
    "        # 3. Reconstruct padded input_ids tensor from the list of positional tensors\n",
    "        # Stack along a new dimension (dim=0) -> (seq_len, batch_size)\n",
    "        # Transpose to get (batch_size, seq_len)\n",
    "        try:\n",
    "            input_ids = torch.stack(list_of_positional_tensors, dim=0).T\n",
    "        except RuntimeError as e:\n",
    "             print(\"Error stacking positional tensors. Check consistency.\")\n",
    "             # Print shapes for debugging\n",
    "             for i, t in enumerate(list_of_positional_tensors): print(f\"Tensor {i} shape: {t.shape}\")\n",
    "             raise e\n",
    "\n",
    "\n",
    "        # 4. Create attention mask based on the reconstructed input_ids and pad_token_id\n",
    "        # Assumes fa_collate used the tokenizer's pad_token_id (or default 0 if none specified - check!)\n",
    "        attention_mask = (input_ids != self.pad_token_id).long()\n",
    "\n",
    "        # 5. Create labels by cloning input_ids BEFORE replacement\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # 6. Find image token ID and replace with placeholder -200 in input_ids\n",
    "        input_ids[input_ids == self.image_token_id] = IMAGE_TOKEN_INDEX_PLACEHOLDER\n",
    "\n",
    "        # 7. Apply label masking for 'plain' template\n",
    "        for i in range(labels.shape[0]):\n",
    "            image_token_indices = torch.where(labels[i] == self.image_token_id)[0]\n",
    "            if len(image_token_indices) > 0:\n",
    "                image_token_idx = image_token_indices[0].item()\n",
    "                mask_until_idx = image_token_idx + 1\n",
    "                labels[i, :mask_until_idx] = IGNORE_INDEX\n",
    "            else:\n",
    "                print(f\"Warning: Image token ID {self.image_token_id} not found in labels for sample {i}. Masking all.\")\n",
    "                labels[i, :] = IGNORE_INDEX\n",
    "\n",
    "            # Also mask padding tokens based on the attention mask we created\n",
    "            labels[i][attention_mask[i] == 0] = IGNORE_INDEX\n",
    "\n",
    "            # Specific check: Mask the BOS token if present\n",
    "            if labels.shape[1] > 0 and labels[i, 0] == self.tokenizer.bos_token_id:\n",
    "                labels[i, 0] = IGNORE_INDEX\n",
    "\n",
    "        # 8. Return the prepared batch as a dictionary\n",
    "        return {\n",
    "            'pixel_values': normalized_images,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    # decodes method remains the same as the last fix\n",
    "    def decodes(self, batch: dict) -> tuple:\n",
    "        \"\"\"Decodes a batch dictionary back into a tuple of (images, texts).\"\"\"\n",
    "        decoded_images = []\n",
    "        decoded_texts = []\n",
    "        if not isinstance(batch, dict) or 'pixel_values' not in batch or 'input_ids' not in batch:\n",
    "             print(f\"Decode expected dict with 'pixel_values' and 'input_ids', got {type(batch)}\")\n",
    "             return ([], []) # Return empty tuple of lists\n",
    "\n",
    "        imgs = batch['pixel_values']\n",
    "        ids = batch['input_ids']\n",
    "        bs = imgs.shape[0]\n",
    "\n",
    "        for i in range(bs):\n",
    "            # Decode image\n",
    "            img_decoded = self.normalize_tfm.decode(imgs[i].unsqueeze(0).cpu())[0]\n",
    "\n",
    "            # Decode text\n",
    "            ids_i = ids[i].clone()\n",
    "            ids_i[ids_i == IMAGE_TOKEN_INDEX_PLACEHOLDER] = self.image_token_id\n",
    "            # Use the *known* pad token ID for filtering\n",
    "            actual_ids = ids_i[ids_i != self.pad_token_id].tolist()\n",
    "            text_decoded = self.tokenizer.decode(actual_ids, skip_special_tokens=True)\n",
    "\n",
    "            decoded_images.append(img_decoded)\n",
    "            decoded_texts.append(TitledStr(text_decoded))\n",
    "\n",
    "        return (decoded_images, decoded_texts)\n",
    "\n",
    "# Make the transform usable in fastai pipelines\n",
    "LLaVABatchTransform.split_idx = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Batch Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0\n",
      "--- Input Samples (Individual Items) ---\n",
      "Sample 0:\n",
      "  Image shape: torch.Size([3, 336, 336])\n",
      "  Token IDs: [1, 32000, 13, 3112, 338, 2654, 29889]\n",
      "Sample 1:\n",
      "  Image shape: torch.Size([3, 336, 336])\n",
      "  Token IDs: [1, 32000, 13, 3624, 372, 7933, 29973]\n",
      "\n",
      "--- Simulated Collated Batch (Input to Transform) ---\n",
      "Images Tensor Shape: torch.Size([2, 3, 336, 336])\n",
      "List of Token Lists Length: 2\n",
      "\n",
      "--- Processed Batch (Output of Transform) ---\n",
      "Batch processing failed or returned unexpected type.\n",
      "Output: (tensor([[[[0.8321, 0.0670, 0.9668,  ..., 0.9698, 0.0592, 0.2165],\n",
      "          [0.5144, 0.4863, 0.4180,  ..., 0.1336, 0.7944, 0.5811],\n",
      "          [0.1692, 0.7337, 0.7632,  ..., 0.6975, 0.9847, 0.7347],\n",
      "          ...,\n",
      "          [0.7079, 0.1073, 0.6360,  ..., 0.4958, 0.9614, 0.1172],\n",
      "          [0.6989, 0.4944, 0.5440,  ..., 0.0147, 0.0686, 0.7959],\n",
      "          [0.6766, 0.0987, 0.3949,  ..., 0.7271, 0.8536, 0.5163]],\n",
      "\n",
      "         [[0.5814, 0.8536, 0.7608,  ..., 0.1944, 0.6119, 0.3136],\n",
      "          [0.8017, 0.7149, 0.4219,  ..., 0.2988, 0.4946, 0.5395],\n",
      "          [0.6635, 0.6874, 0.6546,  ..., 0.1521, 0.6732, 0.8580],\n",
      "          ...,\n",
      "          [0.2610, 0.3498, 0.6311,  ..., 0.6078, 0.0985, 0.3440],\n",
      "          [0.8678, 0.3714, 0.8452,  ..., 0.9147, 0.1881, 0.4653],\n",
      "          [0.4118, 0.0790, 0.6242,  ..., 0.6814, 0.1471, 0.0548]],\n",
      "\n",
      "         [[0.8752, 0.6010, 0.4684,  ..., 0.9980, 0.4082, 0.5008],\n",
      "          [0.2765, 0.3792, 0.7013,  ..., 0.6577, 0.2256, 0.2317],\n",
      "          [0.9368, 0.1431, 0.9404,  ..., 0.8666, 0.6535, 0.2149],\n",
      "          ...,\n",
      "          [0.5048, 0.0995, 0.6739,  ..., 0.1895, 0.6685, 0.0918],\n",
      "          [0.5512, 0.4239, 0.0990,  ..., 0.7030, 0.7365, 0.7176],\n",
      "          [0.3439, 0.1363, 0.2599,  ..., 0.9739, 0.5756, 0.3066]]],\n",
      "\n",
      "\n",
      "        [[[0.6220, 0.4376, 0.5274,  ..., 0.7169, 0.1199, 0.3422],\n",
      "          [0.6466, 0.6608, 0.8479,  ..., 0.5687, 0.4171, 0.7324],\n",
      "          [0.0263, 0.3531, 0.6389,  ..., 0.5211, 0.4093, 0.9001],\n",
      "          ...,\n",
      "          [0.4305, 0.4794, 0.5704,  ..., 0.9173, 0.9766, 0.2717],\n",
      "          [0.3500, 0.9278, 0.6328,  ..., 0.6696, 0.9286, 0.9425],\n",
      "          [0.2759, 0.7897, 0.5338,  ..., 0.4006, 0.4061, 0.7458]],\n",
      "\n",
      "         [[0.6747, 0.1025, 0.3016,  ..., 0.2986, 0.0524, 0.0315],\n",
      "          [0.9732, 0.6305, 0.5542,  ..., 0.6039, 0.4683, 0.5608],\n",
      "          [0.7445, 0.4980, 0.8190,  ..., 0.5695, 0.4753, 0.5217],\n",
      "          ...,\n",
      "          [0.6838, 0.1922, 0.5355,  ..., 0.0087, 0.1222, 0.0957],\n",
      "          [0.5964, 0.1291, 0.0275,  ..., 0.9914, 0.0710, 0.0491],\n",
      "          [0.4912, 0.2275, 0.1574,  ..., 0.9689, 0.0289, 0.5065]],\n",
      "\n",
      "         [[0.8940, 0.7397, 0.7459,  ..., 0.6609, 0.9847, 0.9298],\n",
      "          [0.8455, 0.4975, 0.3113,  ..., 0.8399, 0.6013, 0.5118],\n",
      "          [0.3222, 0.9836, 0.2297,  ..., 0.8481, 0.6826, 0.0299],\n",
      "          ...,\n",
      "          [0.7132, 0.1912, 0.9795,  ..., 0.2184, 0.1879, 0.4804],\n",
      "          [0.8034, 0.1228, 0.7908,  ..., 0.9434, 0.0946, 0.0772],\n",
      "          [0.0190, 0.3748, 0.4289,  ..., 0.8131, 0.7071, 0.7402]]]]), [[1, 32000, 13, 3112, 338, 2654, 29889], [1, 32000, 13, 3624, 372, 7933, 29973]])\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "if tokenizer and 'llava_tokenizer_tfm' in locals(): # Ensure tokenizer and text transform are available\n",
    "    # 1. Create dummy item samples (output of item transforms)\n",
    "    try:\n",
    "        sample1_img = torch.rand(3, 336, 336) # Dummy image tensor\n",
    "        sample1_text_ids = llava_tokenizer_tfm([{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'It is red.'}])\n",
    "        sample1 = (sample1_img, sample1_text_ids)\n",
    "\n",
    "        sample2_img = torch.rand(3, 336, 336)\n",
    "        # Example with slightly longer text\n",
    "        sample2_text_ids = llava_tokenizer_tfm([{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'Is it green?'}])\n",
    "        sample2 = (sample2_img, sample2_text_ids)\n",
    "\n",
    "        dummy_samples = [sample1, sample2]\n",
    "\n",
    "        # --- Simulate Collation ---\n",
    "        # This is what fastai's default collate function would roughly do\n",
    "        collated_images = torch.stack([s[0] for s in dummy_samples])\n",
    "        list_of_token_ids_lists = [s[1] for s in dummy_samples]\n",
    "        simulated_collated_batch = (collated_images, list_of_token_ids_lists)\n",
    "        # --------------------------\n",
    "\n",
    "        # 2. Instantiate the batch transform\n",
    "        batch_transform = LLaVABatchTransform(tokenizer, normalize_tfm=clip_normalize)\n",
    "\n",
    "        # 3. Apply the transform to the *simulated collated batch*\n",
    "        processed_batch = batch_transform(simulated_collated_batch)\n",
    "\n",
    "        # 4. Inspect the output\n",
    "        print(\"--- Input Samples (Individual Items) ---\")\n",
    "        for i, s in enumerate(dummy_samples):\n",
    "            print(f\"Sample {i}:\")\n",
    "            print(f\"  Image shape: {s[0].shape}\")\n",
    "            print(f\"  Token IDs: {s[1]}\")\n",
    "\n",
    "        print(\"\\n--- Simulated Collated Batch (Input to Transform) ---\")\n",
    "        print(f\"Images Tensor Shape: {simulated_collated_batch[0].shape}\")\n",
    "        print(f\"List of Token Lists Length: {len(simulated_collated_batch[1])}\")\n",
    "\n",
    "        print(\"\\n--- Processed Batch (Output of Transform) ---\")\n",
    "        if processed_batch and isinstance(processed_batch, dict): # Check if transform returned a dict\n",
    "            print(f\"Pixel Values Shape: {processed_batch.get('pixel_values', torch.Tensor()).shape}\")\n",
    "            print(f\"Input IDs Shape: {processed_batch.get('input_ids', torch.Tensor()).shape}\")\n",
    "            print(f\"Input IDs:\\n{processed_batch.get('input_ids')}\")\n",
    "            print(f\"Attention Mask Shape: {processed_batch.get('attention_mask', torch.Tensor()).shape}\")\n",
    "            print(f\"Attention Mask:\\n{processed_batch.get('attention_mask')}\")\n",
    "            print(f\"Labels Shape: {processed_batch.get('labels', torch.Tensor()).shape}\")\n",
    "            print(f\"Labels:\\n{processed_batch.get('labels')}\")\n",
    "\n",
    "            print(\"\\n--- Decoded Labels (showing non-masked tokens) ---\")\n",
    "            if 'labels' in processed_batch:\n",
    "                for i in range(processed_batch['labels'].shape[0]):\n",
    "                    label_ids = processed_batch['labels'][i]\n",
    "                    valid_label_ids = label_ids[label_ids != IGNORE_INDEX].tolist()\n",
    "                    decoded_labels = tokenizer.convert_ids_to_tokens(valid_label_ids)\n",
    "                    cleaned_labels = [t for t in decoded_labels if t not in tokenizer.all_special_tokens]\n",
    "                    print(f\"Sample {i} Labels: {cleaned_labels}\")\n",
    "            else:\n",
    "                print(\"Labels key missing in processed batch.\")\n",
    "\n",
    "\n",
    "            # Basic Assertions\n",
    "            assert 'pixel_values' in processed_batch\n",
    "            assert 'input_ids' in processed_batch\n",
    "            assert 'attention_mask' in processed_batch\n",
    "            assert 'labels' in processed_batch\n",
    "            assert processed_batch['input_ids'].shape == processed_batch['attention_mask'].shape == processed_batch['labels'].shape\n",
    "            # Check if image token replacement happened\n",
    "            assert torch.any(processed_batch['input_ids'] == IMAGE_TOKEN_INDEX_PLACEHOLDER)\n",
    "            # Check if labels are masked\n",
    "            assert torch.any(processed_batch['labels'] == IGNORE_INDEX)\n",
    "        else:\n",
    "            print(\"Batch processing failed or returned unexpected type.\")\n",
    "            print(f\"Output: {processed_batch}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"\\n--- ERROR DURING TEST ---\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"Tokenizer not loaded or llava_tokenizer_tfm not defined, skipping batch transform test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Update Data Handling for Stage 2 (Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will adapt text processing for the Vicuna v1 template and update label masking logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for format_v1_template function\n",
    "# Placeholder for updated LLaVABatchTransform logic for v1 template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/11_data_processing.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "Adaptive_Patching_VIT_fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
