{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "> Functions and definitions for preprocessing steps, including normalization stats, tokenization, template formatting, and batch transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoImageProcessor\n",
    "from fastai.vision.augment import Normalize\n",
    "from fastai.vision.all import *\n",
    "from fastai.text.all import *\n",
    "from fastai.data.transforms import Transform\n",
    "from fastai.torch_core import TensorBase, tensor\n",
    "import torch\n",
    "from typing import List, Dict, Union, Tuple, Any, Optional\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import copy\n",
    "\n",
    "# Attempt to import from llava utils, handle potential ImportError if running standalone\n",
    "try:\n",
    "    from llava.utils import load_config\n",
    "except ImportError:\n",
    "    print(\"Warning: llava.utils not found. load_config function might be unavailable.\")\n",
    "    def load_config(path): return {}\n",
    "\n",
    "# Import conversation handling logic (adapt from LLaVA reference or define here)\n",
    "# For now, let's define a simple structure based on Vicuna v1 description\n",
    "from llava.conversation import conv_templates, get_conv_template, SeparatorStyle # Assuming this exists based on reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\" # Placeholder token for image features\n",
    "IMAGE_TOKEN_INDEX_PLACEHOLDER = -200 # Special marker used internally in input_ids\n",
    "IGNORE_INDEX = -100 # Standard ignore index for labels in loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config and Initialize Processors/Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from configs/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n",
      "CLIP Mean: [0.48145466, 0.4578275, 0.40821073]\n",
      "CLIP Std: [0.26862954, 0.26130258, 0.27577711]\n",
      "Fastai Normalize Transform: Normalize -- {'mean': tensor([[[[0.4815]],\n",
      "\n",
      "         [[0.4578]],\n",
      "\n",
      "         [[0.4082]]]], device='cuda:0'), 'std': tensor([[[[0.2686]],\n",
      "\n",
      "         [[0.2613]],\n",
      "\n",
      "         [[0.2758]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "(enc:2,dec:2)\n",
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# --- Configuration Loading --- \n",
    "CONFIG_PATH = 'configs/config.yaml'\n",
    "config = {}\n",
    "try:\n",
    "    config = load_config(CONFIG_PATH)\n",
    "    print(f\"Loaded config from {CONFIG_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Config file not found at {CONFIG_PATH}. Using default model names.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading config from {CONFIG_PATH}: {e}. Using defaults.\")\n",
    "\n",
    "# Get model names from config or use defaults\n",
    "VISION_ENCODER_NAME = config.get('model', {}).get('vision_encoder_name_or_path', 'openai/clip-vit-large-patch14-336')\n",
    "LLM_NAME = config.get('model', {}).get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')\n",
    "TOKENIZER_MAX_LEN = config.get('data', {}).get('tokenizer_model_max_length', 2048)\n",
    "TOKENIZER_PADDING_SIDE = config.get('data', {}).get('tokenizer_padding_side', 'right')\n",
    "\n",
    "# --- Image Processor and Normalization --- \n",
    "clip_image_processor = None\n",
    "image_mean = [0.485, 0.456, 0.406] # Default ImageNet stats\n",
    "image_std = [0.229, 0.224, 0.225]\n",
    "try:\n",
    "    clip_image_processor = AutoImageProcessor.from_pretrained(VISION_ENCODER_NAME)\n",
    "    image_mean = clip_image_processor.image_mean\n",
    "    image_std = clip_image_processor.image_std\n",
    "    print(f\"Successfully loaded CLIP image processor for: {VISION_ENCODER_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading CLIP image processor for {VISION_ENCODER_NAME}: {e}. Using default ImageNet stats.\")\n",
    "\n",
    "clip_normalize = Normalize.from_stats(image_mean, image_std)\n",
    "print(f\"CLIP Mean: {image_mean}\")\n",
    "print(f\"CLIP Std: {image_std}\")\n",
    "print(f\"Fastai Normalize Transform: {clip_normalize}\")\n",
    "\n",
    "# --- Tokenizer --- \n",
    "tokenizer = None\n",
    "IMAGE_TOKEN_ID = None\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        LLM_NAME,\n",
    "        model_max_length=TOKENIZER_MAX_LEN,\n",
    "        padding_side=TOKENIZER_PADDING_SIDE,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    print(f\"Successfully loaded tokenizer for: {LLM_NAME}\")\n",
    "\n",
    "    current_vocab = tokenizer.get_vocab()\n",
    "    if DEFAULT_IMAGE_TOKEN not in current_vocab:\n",
    "        print(f\"Adding special token {DEFAULT_IMAGE_TOKEN} to tokenizer.\")\n",
    "        num_added = tokenizer.add_special_tokens({'additional_special_tokens': [DEFAULT_IMAGE_TOKEN]})\n",
    "        if num_added > 0:\n",
    "            print(f\"Added {num_added} token(s). New vocab size: {len(tokenizer)}\")\n",
    "    \n",
    "    IMAGE_TOKEN_ID = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)\n",
    "    print(f\"Using token ID for {DEFAULT_IMAGE_TOKEN}: {IMAGE_TOKEN_ID}\")\n",
    "    if IMAGE_TOKEN_ID == tokenizer.unk_token_id:\n",
    "         print(f\"Warning: {DEFAULT_IMAGE_TOKEN} resolved to UNK token ID ({tokenizer.unk_token_id}). Check tokenizer setup.\")\n",
    "         # Attempt to force it if necessary and if vocab doesn't contain it\n",
    "         # This is risky if the ID is already used\n",
    "         if DEFAULT_IMAGE_TOKEN not in current_vocab:\n",
    "              IMAGE_TOKEN_ID = len(tokenizer) - 1 # Use the newly added token ID\n",
    "              print(f\"Using explicitly added token ID: {IMAGE_TOKEN_ID}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.unk_token # Use UNK as pad if no PAD exists (like Llama-2)\n",
    "        print(f\"Set pad token to UNK token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "    else:\n",
    "        print(f\"Tokenizer already has pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Fatal Error: Could not load tokenizer for {LLM_NAME}: {e}\")\n",
    "    # Handle error appropriately in a real application\n",
    "    # For notebook execution, print warning and continue if possible\n",
    "    tokenizer = None\n",
    "    IMAGE_TOKEN_ID = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Formatting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_plain_template(conversations: List[Dict[str, str]], tokenizer: AutoTokenizer = tokenizer) -> str:\n",
    "    \"\"\"Formats conversations using the 'plain' template for Stage 1 pre-training.\n",
    "    \n",
    "    The 'plain' template uses the format: <image>\\n{caption}\\n\n",
    "    where {caption} is the value of the first 'gpt' turn.\n",
    "    Handles moving <image> token to the start if found elsewhere in the input.\n",
    "\n",
    "    Args:\n",
    "        conversations: A list of conversation turns (dictionaries with 'from' and 'value').\n",
    "        tokenizer: The tokenizer instance (needed for special tokens, though not used here).\n",
    "\n",
    "    Returns:\n",
    "        The formatted string. Returns just the image token if no 'gpt' turn is found.\n",
    "    \"\"\"\n",
    "    caption = \"\"\n",
    "    image_token_found = False\n",
    "    first_human_turn_value = None\n",
    "\n",
    "    # First pass: find caption and check for image token\n",
    "    for i, turn in enumerate(conversations):\n",
    "        value = turn.get('value', '')\n",
    "        if turn.get('from', '').lower() == 'gpt' and not caption: # Only take first caption\n",
    "            caption = value\n",
    "        if DEFAULT_IMAGE_TOKEN in value:\n",
    "            image_token_found = True\n",
    "        if turn.get('from', '').lower() == 'human' and first_human_turn_value is None:\n",
    "             first_human_turn_value = value\n",
    "\n",
    "    # Ensure <image> token is at the start, conceptually\n",
    "    # Remove <image> from caption if present\n",
    "    caption = caption.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "\n",
    "    # Construct final output: <image>\\n{caption}\n",
    "    formatted = f\"{DEFAULT_IMAGE_TOKEN}\\n{caption}\".strip() if caption else DEFAULT_IMAGE_TOKEN\n",
    "    return formatted\n",
    "\n",
    "\n",
    "def format_v1_template(conversations: List[Dict[str, str]], tokenizer: AutoTokenizer = tokenizer) -> str:\n",
    "    \"\"\"Formats conversations using the Vicuna v1 template.\n",
    "\n",
    "    Handles moving the <image> token to the beginning of the *first* human message.\n",
    "    Uses the `conv_templates['v1']` structure.\n",
    "\n",
    "    Args:\n",
    "        conversations: A list of conversation turns.\n",
    "        tokenizer: The tokenizer instance.\n",
    "\n",
    "    Returns:\n",
    "        The fully formatted prompt string according to Vicuna v1 template.\n",
    "    \"\"\"\n",
    "    if 'v1' not in conv_templates:\n",
    "         raise ValueError(\"Vicuna v1 conversation template ('v1') not found in conversation_lib.\")\n",
    "\n",
    "    # Create a deep copy to avoid modifying the template dictionary directly\n",
    "    conv = copy.deepcopy(conv_templates['v1'])\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Preprocess: Move <image> token to the start of the first human turn\n",
    "    processed_conversations = []\n",
    "    image_token_moved = False\n",
    "    for i, turn in enumerate(conversations):\n",
    "        value = turn.get('value', '')\n",
    "        from_role = turn.get('from', '').lower()\n",
    "\n",
    "        new_turn = copy.deepcopy(turn)\n",
    "\n",
    "        if DEFAULT_IMAGE_TOKEN in value:\n",
    "            if from_role == 'human' and not image_token_moved:\n",
    "                # Move to start of this turn, remove from original position\n",
    "                new_turn['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + value.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "                image_token_moved = True\n",
    "            else:\n",
    "                 # Remove image token if found elsewhere (e.g., GPT response or later human turn)\n",
    "                 new_turn['value'] = value.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "        \n",
    "        processed_conversations.append(new_turn)\n",
    "    \n",
    "    # If image token was never found, add it to the start of the first human turn if one exists\n",
    "    if not image_token_moved:\n",
    "         found_human = False\n",
    "         for i, turn in enumerate(processed_conversations):\n",
    "              if turn.get('from', '').lower() == 'human':\n",
    "                   processed_conversations[i]['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + turn.get('value', '')\n",
    "                   found_human = True\n",
    "                   break\n",
    "         # If no human turn exists, prepend <image>\\n before the system prompt or start\n",
    "         # This is unlikely for instruct data but handles edge cases.\n",
    "         # However, standard LLaVA format assumes <image> is tied to a human turn.\n",
    "         # Let's stick to adding it to the first human turn.\n",
    "         # If no human turn exists, the template formatting will likely handle it appropriately\n",
    "         # or it might indicate an issue with the input data format.\n",
    "         if not found_human:\n",
    "              print(\"Warning: No 'human' turn found to prepend <image> token to.\")\n",
    "\n",
    "\n",
    "    # Append conversations to the template\n",
    "    for turn in processed_conversations:\n",
    "        role_key = turn.get('from', '').lower()\n",
    "        if role_key in roles:\n",
    "            conv.append_message(roles[role_key], turn.get('value'))\n",
    "        else:\n",
    "            # Handle unknown roles if necessary, e.g., skip or raise error\n",
    "            print(f\"Warning: Skipping turn with unknown role '{role_key}'.\")\n",
    "            continue\n",
    "    \n",
    "    # Append the assistant prompt\n",
    "    conv.append_message(roles['gpt'], None) \n",
    "\n",
    "    return conv.get_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Template Formatting - V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test Case 1: Standard --- \n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\n",
      "Describe this image. ASSISTANT: This is a red object.</s> USER: What shape is it? ASSISTANT: It is round.</s> USER: Thanks! ASSISTANT:\n",
      "\n",
      "--- Test Case 2: Image token later --- \n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Describe this image. ASSISTANT: This is a green object.</s> USER: <image>\n",
      "What shape is it  ? ASSISTANT: It is square.</s> USER: Thanks! ASSISTANT:\n",
      "\n",
      "--- Test Case 3: Image token in GPT response (removed) --- \n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\n",
      "Describe. ASSISTANT: This is a blue object.</s> USER: Anything else? ASSISTANT: It might be  shiny.</s> USER: Ok ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "conv1 = [\n",
    "    {'from': 'human', 'value': '<image>\\nDescribe this image.'},\n",
    "    {'from': 'gpt', 'value': 'This is a red object.'},\n",
    "    {'from': 'human', 'value': 'What shape is it?'},\n",
    "    {'from': 'gpt', 'value': 'It is round.'},\n",
    "    {'from': 'human', 'value': 'Thanks!'}\n",
    "]\n",
    "conv2 = [\n",
    "    {'from': 'human', 'value': 'Describe this image.'},\n",
    "    {'from': 'gpt', 'value': 'This is a green object.'},\n",
    "    {'from': 'human', 'value': 'What shape is it <image> ?'},\n",
    "    {'from': 'gpt', 'value': 'It is square.'},\n",
    "    {'from': 'human', 'value': 'Thanks!'}\n",
    "]\n",
    "conv3 = [\n",
    "    {'from': 'human', 'value': '<image>Describe.'},\n",
    "    {'from': 'gpt', 'value': 'This is a blue object.'},\n",
    "    {'from': 'human', 'value': 'Anything else?'},\n",
    "    {'from': 'gpt', 'value': 'It might be <image> shiny.'},\n",
    "    {'from': 'human', 'value': 'Ok'}\n",
    "]\n",
    "\n",
    "if tokenizer:\n",
    "    print(\"--- Test Case 1: Standard --- \")\n",
    "    print(format_v1_template(conv1, tokenizer))\n",
    "    print(\"\\n--- Test Case 2: Image token later --- \")\n",
    "    print(format_v1_template(conv2, tokenizer))\n",
    "    print(\"\\n--- Test Case 3: Image token in GPT response (removed) --- \")\n",
    "    print(format_v1_template(conv3, tokenizer))\n",
    "else:\n",
    "    print(\"Tokenizer not loaded, skipping v1 template test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLaVATextTokenizer(Transform):\n",
    "    \"\"\"A fastai Transform to format and tokenize text data for LLaVA stages.\n",
    "    \n",
    "    Applies the specified template formatting (e.g., 'plain' or 'v1') \n",
    "    and then tokenizes the text, returning only the input IDs.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, template_formatter):\n",
    "        store_attr()\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided and loaded successfully.\")\n",
    "\n",
    "    def encodes(self, conversations: list) -> list:\n",
    "        \"\"\"Applies formatting and tokenization to conversation data.\n",
    "\n",
    "        Args:\n",
    "            conversations: Raw conversation list from the dataset sample.\n",
    "\n",
    "        Returns:\n",
    "            A list of input token IDs.\n",
    "        \"\"\"\n",
    "        formatted_text = self.template_formatter(conversations, self.tokenizer)\n",
    "        tokenized_output = self.tokenizer(formatted_text,\n",
    "                                         return_tensors=None, \n",
    "                                         add_special_tokens=True, \n",
    "                                         truncation=False \n",
    "                                        )\n",
    "        return tokenized_output['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLaVABatchTransform(Transform):\n",
    "    \"\"\" Custom batch transform for LLaVA stages.\n",
    "        Handles image normalization, text padding reconstruction, attention mask creation,\n",
    "        image token marker replacement, label creation, and template-specific label masking.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, normalize_tfm: Normalize, template: str = 'plain', image_token_id: Optional[int] = None):\n",
    "        store_attr() # Stores tokenizer, normalize_tfm, template\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided.\")\n",
    "\n",
    "        if image_token_id is None:\n",
    "            self.image_token_id = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)\n",
    "            if self.image_token_id == tokenizer.unk_token_id and DEFAULT_IMAGE_TOKEN in tokenizer.added_tokens_decoder:\n",
    "                 self.image_token_id = tokenizer.added_tokens_decoder[DEFAULT_IMAGE_TOKEN]\n",
    "            if self.image_token_id == tokenizer.unk_token_id:\n",
    "                print(f\"Warning: {DEFAULT_IMAGE_TOKEN} not found in tokenizer vocab. Using UNK ID: {self.image_token_id}\")\n",
    "        else:\n",
    "            self.image_token_id = image_token_id\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        if self.pad_token_id is None:\n",
    "            raise ValueError(\"Tokenizer must have a pad_token_id defined.\")\n",
    "            \n",
    "        self.assistant_role_token_ids = None\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "        self.bos_token_id = tokenizer.bos_token_id\n",
    "        self.sep = None\n",
    "        self.sep2 = None\n",
    "\n",
    "        if self.template == 'v1':\n",
    "            conv_v1 = conv_templates.get('v1')\n",
    "            if conv_v1:\n",
    "                assistant_role_str = conv_v1.roles[1] \n",
    "                self.assistant_role_token_ids = self.tokenizer.encode(f\"{assistant_role_str}:\", add_special_tokens=False)\n",
    "                self.sep = conv_v1.sep \n",
    "                self.sep2 = conv_v1.sep2 \n",
    "                # print(f\"V1 template assistant role tokens: {self.assistant_role_token_ids}\") # Less verbose\n",
    "            else:\n",
    "                 print(\"Warning: Vicuna v1 template not found, v1 masking might not work correctly.\")\n",
    "\n",
    "        # print(f\"LLaVABatchTransform initialized. Image Token ID: {self.image_token_id}, Pad Token ID: {self.pad_token_id}, Template: {self.template}\") # Less verbose\n",
    "\n",
    "    def encodes(self, collated_batch: tuple) -> dict:\n",
    "        if not isinstance(collated_batch, tuple) or len(collated_batch) != 2:\n",
    "             # This path should ideally not be hit if DataBlock collation works as expected\n",
    "             print(f\"Warning: LLaVABatchTransform.encodes received unexpected input type: {type(collated_batch)}. Length: {len(collated_batch) if isinstance(collated_batch, tuple) else 'N/A'}\")\n",
    "             # Attempt to unpack if it's a list/tuple of one item which is the actual 2-item tuple\n",
    "             if isinstance(collated_batch, (list, tuple)) and len(collated_batch) == 1 and isinstance(collated_batch[0], tuple) and len(collated_batch[0]) == 2:\n",
    "                 print(\"Adjusting input: using the first element as the collated_batch tuple.\")\n",
    "                 collated_batch = collated_batch[0]\n",
    "             else:\n",
    "                 raise ValueError(f\"LLaVABatchTransform.encodes expects a 2-element tuple (images, texts_list), got {type(collated_batch)}\")\n",
    "\n",
    "        collated_images, list_of_positional_tensors = collated_batch\n",
    "        \n",
    "        if not isinstance(collated_images, torch.Tensor):\n",
    "             raise TypeError(f\"Expected first element of collated batch to be a Tensor, got {type(collated_images)}\")\n",
    "        if not isinstance(list_of_positional_tensors, list) or \\\n",
    "           (list_of_positional_tensors and not all(isinstance(t, torch.Tensor) for t in list_of_positional_tensors)):\n",
    "             raise TypeError(f\"Expected second element of collated batch to be a list of Tensors, got {type(list_of_positional_tensors)}\")\n",
    "        \n",
    "        bs = collated_images.shape[0]\n",
    "        device = collated_images.device\n",
    "\n",
    "        if not list_of_positional_tensors: \n",
    "            print(\"Warning: Received empty list of positional tensors. Outputting minimal dict structure.\")\n",
    "            normalized_images = self.normalize_tfm(collated_images)\n",
    "            # Create dummy tensors for input_ids, attention_mask, labels to maintain dict structure\n",
    "            # Assuming a minimal sequence length of 1 for these dummies\n",
    "            dummy_text_tensor = torch.full((bs, 1), self.pad_token_id, dtype=torch.long, device=device)\n",
    "            dummy_labels_tensor = torch.full((bs, 1), IGNORE_INDEX, dtype=torch.long, device=device)\n",
    "            output_dict = {\n",
    "                'pixel_values': normalized_images,\n",
    "                'input_ids': dummy_text_tensor,\n",
    "                'attention_mask': torch.zeros_like(dummy_text_tensor, dtype=torch.long),\n",
    "                'labels': dummy_labels_tensor\n",
    "            }\n",
    "            # print(f\"LLaVABatchTransform.encodes (empty text) returning dict with keys: {output_dict.keys()}\")\n",
    "            return output_dict\n",
    "\n",
    "        normalized_images = self.normalize_tfm(collated_images)\n",
    "\n",
    "        try:\n",
    "            # Device for padding should be consistent with list_of_positional_tensors elements\n",
    "            # If list_of_positional_tensors is empty, this won't be hit due to above check\n",
    "            text_device = list_of_positional_tensors[0].device \n",
    "            input_ids = pad_sequence([t.to(text_device) for t in list_of_positional_tensors], \n",
    "                                       batch_first=True, \n",
    "                                       padding_value=self.pad_token_id)\n",
    "        except Exception as e:\n",
    "             print(\"Error padding positional tensors. Check consistency.\")\n",
    "             for i, t in enumerate(list_of_positional_tensors): print(f\"Tensor {i} shape: {t.shape}, device: {t.device}\")\n",
    "             raise e\n",
    "\n",
    "        attention_mask = (input_ids != self.pad_token_id).long()\n",
    "        labels = input_ids.clone()\n",
    "        input_ids_processed = input_ids.clone()\n",
    "        input_ids_processed[input_ids_processed == self.image_token_id] = IMAGE_TOKEN_INDEX_PLACEHOLDER\n",
    "\n",
    "        if self.template == 'plain':\n",
    "            self._apply_plain_masking(labels, attention_mask)\n",
    "        elif self.template == 'v1':\n",
    "            self._apply_v1_masking(labels, attention_mask)\n",
    "        else:\n",
    "            print(f\"Warning: Unknown template '{self.template}'. Defaulting to plain masking.\")\n",
    "            self._apply_plain_masking(labels, attention_mask)\n",
    "\n",
    "        output_dict = {\n",
    "            'pixel_values': normalized_images.to(device), # Ensure all outputs are on consistent device\n",
    "            'input_ids': input_ids_processed.to(device), \n",
    "            'attention_mask': attention_mask.to(device),\n",
    "            'labels': labels.to(device) \n",
    "        }\n",
    "        # DIAGNOSTIC PRINT:\n",
    "        # print(f\"LLaVABatchTransform.encodes returning dict with keys: {output_dict.keys()} and types: {[type(v) for v in output_dict.values()]}\")\n",
    "        # print(f\"LLaVABatchTransform.encodes returning type: {type(output_dict)}\") # Added to ensure it is dict\n",
    "        return output_dict\n",
    "\n",
    "    def _apply_plain_masking(self, labels, attention_mask):\n",
    "        \"\"\"Masks labels for the 'plain' template.\"\"\" \n",
    "        for i in range(labels.shape[0]):\n",
    "            image_token_indices = torch.where(labels[i] == self.image_token_id)[0]\n",
    "            mask_until_idx = -1\n",
    "            if len(image_token_indices) > 0:\n",
    "                image_token_idx = image_token_indices[0].item()\n",
    "                mask_until_idx = image_token_idx + 1 \n",
    "                if image_token_idx + 1 < len(labels[i]) and labels[i, image_token_idx + 1] == 13: # Newline token\n",
    "                     mask_until_idx += 1\n",
    "            else:\n",
    "                # This case should be rare if data is well-formed, but good to log\n",
    "                print(f\"Warning: Image token ID {self.image_token_id} not found in labels for sample {i} (Plain template). Masking up to first valid token if any.\")\n",
    "                # Fallback: if no image token, try to mask only padding and BOS. This is heuristic.\n",
    "                # Or mask all prompt, but plain template assumes <image>\\n<caption>\n",
    "                mask_until_idx = 0 # Default to only masking BOS if image token is missing.\n",
    "\n",
    "            if mask_until_idx >= 0 : # Ensure mask_until_idx is valid before slicing\n",
    "                 labels[i, :mask_until_idx] = IGNORE_INDEX\n",
    "            \n",
    "            labels[i][attention_mask[i] == 0] = IGNORE_INDEX # Mask padding\n",
    "            \n",
    "            if labels.shape[1] > 0 and labels[i, 0] == self.bos_token_id: # BOS\n",
    "                labels[i, 0] = IGNORE_INDEX\n",
    "                \n",
    "    def _find_subsequence(self, main_tensor, sub_tensor):\n",
    "        \"\"\"Finds the start indices of a sub-tensor within a main tensor.\"\"\"\n",
    "        n = main_tensor.size(0)\n",
    "        m = sub_tensor.size(0)\n",
    "        if m > n or m == 0: return []\n",
    "        indices = []\n",
    "        sub_tensor = sub_tensor.to(main_tensor.device)\n",
    "        for i in range(n - m + 1):\n",
    "            if torch.equal(main_tensor[i:i+m], sub_tensor):\n",
    "                indices.append(i)\n",
    "        return indices\n",
    "\n",
    "    def _apply_v1_masking(self, labels, attention_mask):\n",
    "        \"\"\"Masks labels for the Vicuna 'v1' template.\"\"\" \n",
    "        if self.assistant_role_token_ids is None:\n",
    "            print(\"Warning: Assistant role tokens not initialized for v1 template. Cannot perform v1 masking. Masking only padding and BOS.\")\n",
    "            for i in range(labels.shape[0]):\n",
    "                labels[i][attention_mask[i] == 0] = IGNORE_INDEX # Mask padding\n",
    "                if labels.shape[1] > 0 and labels[i, 0] == self.bos_token_id: # BOS\n",
    "                    labels[i, 0] = IGNORE_INDEX\n",
    "            return\n",
    "        \n",
    "        assistant_token_ids_tensor = torch.tensor(self.assistant_role_token_ids, dtype=torch.long, device=labels.device)\n",
    "        len_assistant_prompt = len(self.assistant_role_token_ids)\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            current_labels = labels[i]\n",
    "            # Create a mask that initially ignores all tokens\n",
    "            current_final_mask = torch.full_like(current_labels, IGNORE_INDEX) \n",
    "\n",
    "            assistant_indices = self._find_subsequence(current_labels, assistant_token_ids_tensor)\n",
    "\n",
    "            for start_idx in assistant_indices:\n",
    "                response_start_idx = start_idx + len_assistant_prompt\n",
    "                response_end_idx = -1\n",
    "                \n",
    "                # Find where the response ends (either by EOS or end of non-padded sequence)\n",
    "                eos_after_response_start = torch.where(current_labels[response_start_idx:] == self.eos_token_id)[0]\n",
    "                if len(eos_after_response_start) > 0:\n",
    "                    # EOS found, response ends at EOS token relative to response_start_idx\n",
    "                    response_end_idx = response_start_idx + eos_after_response_start[0].item() \n",
    "                else:\n",
    "                    # No EOS, response goes to the end of actual tokens in the sequence\n",
    "                    non_padded_len = attention_mask[i].sum().item()\n",
    "                    response_end_idx = non_padded_len\n",
    "                \n",
    "                # Unmask the tokens that are part of the assistant's response\n",
    "                if response_start_idx < response_end_idx:\n",
    "                    current_final_mask[response_start_idx:response_end_idx] = current_labels[response_start_idx:response_end_idx]\n",
    "            \n",
    "            # Explicitly mask the <image> token placeholder if it's still in labels\n",
    "            # (it should have been replaced by IMAGE_TOKEN_INDEX_PLACEHOLDER in input_ids, but labels are a clone before that)\n",
    "            image_token_indices = torch.where(current_labels == self.image_token_id)[0]\n",
    "            if len(image_token_indices) > 0:\n",
    "                current_final_mask[image_token_indices[0].item()] = IGNORE_INDEX # Mask the first occurrence\n",
    "            \n",
    "            # Ensure padding tokens are ignored (even if they were part of a misidentified response)\n",
    "            current_final_mask[attention_mask[i] == 0] = IGNORE_INDEX\n",
    "            \n",
    "            # Ensure BOS token is ignored\n",
    "            if current_labels.shape[0] > 0 and current_labels[0] == self.bos_token_id: # BOS\n",
    "                current_final_mask[0] = IGNORE_INDEX\n",
    "                \n",
    "            labels[i] = current_final_mask\n",
    "\n",
    "    def decodes(self, batch: dict) -> tuple:\n",
    "        \"\"\"Decodes a batch dictionary back into a tuple of (images, texts).\"\"\"\n",
    "        decoded_images = []\n",
    "        decoded_texts = []\n",
    "        if not isinstance(batch, dict) or 'pixel_values' not in batch or 'input_ids' not in batch:\n",
    "             # It's possible that during show_batch, the input `batch` might be a tuple if DataBlock has n_inp > 1\n",
    "             # This decode is primarily for fastai's show_batch.\n",
    "             # print(f\"Decode expected dict with 'pixel_values' and 'input_ids', got {type(batch)}\\nContent: {str(batch)[:500]}\")\n",
    "             # Try to handle if it's the (images, texts_tuple_from_dict) structure\n",
    "             if isinstance(batch, tuple) and len(batch) == 2 and isinstance(batch[0], torch.Tensor) and isinstance(batch[1], tuple):\n",
    "                 # This is a heuristic, assuming batch[0] are images and batch[1] contains text-related tensors\n",
    "                 # This path is less robust and depends on exact structure of batch[1]\n",
    "                 # For simplicity, we'll assume `batch` is the dictionary as `encodes` returns.\n",
    "                 # If show_batch fails, this decode method might need more sophisticated input handling.\n",
    "                 return ([], [TitledStr(\"Decode error: Unexpected batch structure\")])\n",
    "             return ([], []) \n",
    "\n",
    "        imgs = batch['pixel_values']\n",
    "        ids = batch['input_ids']\n",
    "        bs = imgs.shape[0]\n",
    "\n",
    "        for i in range(bs):\n",
    "            img_tensor_cpu = imgs[i].cpu()\n",
    "            img_decoded = self.normalize_tfm.decode(img_tensor_cpu.unsqueeze(0))[0]\n",
    "\n",
    "            ids_i = ids[i].clone().cpu() \n",
    "            ids_i[ids_i == IMAGE_TOKEN_INDEX_PLACEHOLDER] = self.image_token_id \n",
    "            \n",
    "            attn_mask_i = (ids_i != self.pad_token_id)\n",
    "            actual_ids = ids_i[attn_mask_i].tolist() # Get only non-padded tokens\n",
    "            \n",
    "            # Remove BOS and EOS for cleaner decoded text if they are not part of content\n",
    "            if actual_ids and actual_ids[0] == self.bos_token_id: actual_ids = actual_ids[1:]\n",
    "            if actual_ids and actual_ids[-1] == self.eos_token_id: actual_ids = actual_ids[:-1]\n",
    "\n",
    "            text_decoded = self.tokenizer.decode(actual_ids, skip_special_tokens=False) # Keep special tokens to see them\n",
    "            \n",
    "            # Replace the <image> token string with the placeholder for display\n",
    "            # Need to decode image_token_id without skip_special_tokens to get its exact string rep\n",
    "            image_token_str_in_vocab = self.tokenizer.decode([self.image_token_id], skip_special_tokens=False)\n",
    "            text_decoded = text_decoded.replace(image_token_str_in_vocab, DEFAULT_IMAGE_TOKEN)\n",
    "            # Also replace other special tokens if desired for cleaner display\n",
    "            text_decoded = text_decoded.replace(self.tokenizer.bos_token, \"[BOS]\")\n",
    "            text_decoded = text_decoded.replace(self.tokenizer.eos_token, \"[EOS]\")\n",
    "            text_decoded = text_decoded.replace(self.tokenizer.pad_token, \"[PAD]\")\n",
    "            text_decoded = text_decoded.replace(self.tokenizer.unk_token, \"[UNK]\")\n",
    "\n",
    "\n",
    "            decoded_images.append(img_decoded)\n",
    "            decoded_texts.append(TitledStr(text_decoded)) \n",
    "\n",
    "        return (decoded_images, decoded_texts)\n",
    "\n",
    "LLaVABatchTransform.split_idx = None # Make it work for both train/valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Batch Transform - V1 Masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Collated Input IDs (Padded) ---\n",
      "tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901, 29871, 32000,    13,  4002, 29581,  1967,\n",
      "         29889,   319,  1799,  9047, 13566, 29901,   739,   338,   263,  2654,\n",
      "          1203, 29889,     2,   319,  1799,  9047, 13566, 29901,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901, 29871, 32000,    13,  5618,  2927, 29973,\n",
      "           319,  1799,  9047, 13566, 29901,   739,   338,  7933, 29889,     2,\n",
      "          3148,  1001, 29901,  1126,  8267, 29973,   319,  1799,  9047, 13566,\n",
      "         29901,   739,   338,  4513,     2,   319,  1799,  9047, 13566, 29901]])\n",
      "V1 template assistant role tokens: [319, 1799, 9047, 13566, 29901]\n",
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0, Template: v1\n",
      "\n",
      "--- Processed Batch (Output of Transform) ---\n",
      "Raw processed batch type: <class 'dict'>\n",
      "Processed batch is a dictionary.\n",
      "Pixel Values Shape: torch.Size([2, 3, 336, 336])\n",
      "Input IDs Shape (with -200): torch.Size([2, 70])\n",
      "Input IDs:\n",
      "tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901, 29871,  -200,    13,  4002, 29581,  1967,\n",
      "         29889,   319,  1799,  9047, 13566, 29901,   739,   338,   263,  2654,\n",
      "          1203, 29889,     2,   319,  1799,  9047, 13566, 29901,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901, 29871,  -200,    13,  5618,  2927, 29973,\n",
      "           319,  1799,  9047, 13566, 29901,   739,   338,  7933, 29889,     2,\n",
      "          3148,  1001, 29901,  1126,  8267, 29973,   319,  1799,  9047, 13566,\n",
      "         29901,   739,   338,  4513,     2,   319,  1799,  9047, 13566, 29901]])\n",
      "Attention Mask Shape: torch.Size([2, 70])\n",
      "Attention Mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Labels Shape: torch.Size([2, 70])\n",
      "Labels:\n",
      "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,   739,   338,   263,  2654,\n",
      "          1203, 29889,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,   739,   338,  7933, 29889,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,   739,   338,  4513,  -100,  -100,  -100,  -100,  -100,  -100]])\n",
      "\n",
      "--- Decoded Labels (Showing Loss Calculation Targets) ---\n",
      "Sample 0 Target Tokens: ['▁It', '▁is', '▁a', '▁red', '▁object', '.']\n",
      "Sample 1 Target Tokens: ['▁It', '▁is', '▁green', '.', '▁It', '▁is', '▁round']\n",
      "\n",
      "V1 Masking Test Passed (Check decoded labels above).\n"
     ]
    }
   ],
   "source": [
    "if tokenizer:\n",
    "    try:\n",
    "        # 1. Create Sample Data (Tokenized using v1 template)\n",
    "        conv_a = [{'from': 'human', 'value': '<image>\\nDescribe image.'}, {'from': 'gpt', 'value': 'It is a red object.'}]\n",
    "        conv_b = [{'from': 'human', 'value': '<image>What color?'}, {'from': 'gpt', 'value': 'It is green.'}, {'from':'human', 'value': 'And shape?'}, {'from':'gpt', 'value':'It is round'}]\n",
    "        \n",
    "        tokenizer_tfm_v1 = LLaVATextTokenizer(tokenizer, template_formatter=format_v1_template)\n",
    "        \n",
    "        token_ids_a = tokenizer_tfm_v1(conv_a)\n",
    "        token_ids_b = tokenizer_tfm_v1(conv_b)\n",
    "        \n",
    "        # 2. Simulate Collation (Get padded tensor)\n",
    "        collated_ids_unprocessed = pad_sequence([torch.tensor(token_ids_a), torch.tensor(token_ids_b)], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=tokenizer.pad_token_id)\n",
    "        print(\"\\n--- Original Collated Input IDs (Padded) ---\")\n",
    "        print(collated_ids_unprocessed)\n",
    "        \n",
    "        # Dummy images\n",
    "        dummy_images = torch.rand(2, 3, 336, 336)\n",
    "        # The LLaVABatchTransform.encodes expects a tuple: (collated_images, list_of_positional_tensors)\n",
    "        simulated_collated_batch_for_transform = (dummy_images, [torch.tensor(token_ids_a), torch.tensor(token_ids_b)])\n",
    "\n",
    "        # 3. Instantiate and Apply Batch Transform for V1\n",
    "        batch_transform_v1 = LLaVABatchTransform(tokenizer, normalize_tfm=clip_normalize, template='v1')\n",
    "        # Call .encodes() directly for testing the core logic\n",
    "        raw_processed_batch = batch_transform_v1.encodes(simulated_collated_batch_for_transform)\n",
    "\n",
    "\n",
    "        # 4. Inspect Output\n",
    "        print(\"\\n--- Processed Batch (Output of Transform) ---\")\n",
    "        print(f\"Raw processed batch type: {type(raw_processed_batch)}\")\n",
    "        \n",
    "        processed_batch_dict = None\n",
    "        if isinstance(raw_processed_batch, dict):\n",
    "            processed_batch_dict = raw_processed_batch\n",
    "            print(\"Processed batch is a dictionary.\")\n",
    "        # Removed other tuple checks as .encodes() should directly return a dict\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected type for processed_batch from .encodes(): {type(raw_processed_batch)}. Expected dict. Content: {str(raw_processed_batch)[:500]}...\")\n",
    "\n",
    "        assert isinstance(processed_batch_dict, dict), \"processed_batch_dict should be a dictionary\"\n",
    "        \n",
    "        print(f\"Pixel Values Shape: {processed_batch_dict.get('pixel_values').shape}\")\n",
    "        print(f\"Input IDs Shape (with -200): {processed_batch_dict.get('input_ids').shape}\")\n",
    "        print(f\"Input IDs:\\n{processed_batch_dict.get('input_ids')}\")\n",
    "        print(f\"Attention Mask Shape: {processed_batch_dict.get('attention_mask').shape}\")\n",
    "        print(f\"Attention Mask:\\n{processed_batch_dict.get('attention_mask')}\")\n",
    "        print(f\"Labels Shape: {processed_batch_dict.get('labels').shape}\")\n",
    "        print(f\"Labels:\\n{processed_batch_dict.get('labels')}\")\n",
    "\n",
    "        print(\"\\n--- Decoded Labels (Showing Loss Calculation Targets) ---\")\n",
    "        if 'labels' in processed_batch_dict:\n",
    "            for i in range(processed_batch_dict['labels'].shape[0]):\n",
    "                label_ids = processed_batch_dict['labels'][i]\n",
    "                valid_label_ids = label_ids[label_ids != IGNORE_INDEX].tolist()\n",
    "                decoded_tokens_list = tokenizer.convert_ids_to_tokens(valid_label_ids)\n",
    "                print(f\"Sample {i} Target Tokens: {decoded_tokens_list}\")\n",
    "        \n",
    "        assert processed_batch_dict['labels'][0, 0] == IGNORE_INDEX\n",
    "        assert processed_batch_dict['labels'][1, 0] == IGNORE_INDEX\n",
    "        \n",
    "        original_img_pos_0 = torch.where(collated_ids_unprocessed[0] == IMAGE_TOKEN_ID)[0]\n",
    "        if len(original_img_pos_0) > 0:\n",
    "            assert processed_batch_dict['labels'][0, original_img_pos_0[0].item()] == IGNORE_INDEX\n",
    "        original_img_pos_1 = torch.where(collated_ids_unprocessed[1] == IMAGE_TOKEN_ID)[0]\n",
    "        if len(original_img_pos_1) > 0:\n",
    "            assert processed_batch_dict['labels'][1, original_img_pos_1[0].item()] == IGNORE_INDEX\n",
    "        \n",
    "        print(\"\\nV1 Masking Test Passed (Check decoded labels above).\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during V1 masking test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Tokenizer not loaded, skipping batch transform v1 test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
