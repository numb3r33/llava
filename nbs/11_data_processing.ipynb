{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "> Functions and definitions for preprocessing steps, including normalization stats, tokenization, and template formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoImageProcessor\n",
    "from fastai.vision.augment import Normalize\n",
    "from fastai.data.transforms import Transform\n",
    "from fastai.torch_core import TensorBase, tensor, TitledList\n",
    "import torch\n",
    "from typing import List, Dict, Union, Tuple, Any\n",
    "\n",
    "from Adaptive_Patching_VIT_fastai.utils import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2 (Continued): Image Normalization Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CLIP image processor to get the correct normalization statistics (mean and standard deviation) required for the vision encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Load config to get model name\n",
    "CONFIG_PATH = 'configs/config.yaml'\n",
    "try:\n",
    "    config = load_config(CONFIG_PATH)\n",
    "    VISION_ENCODER_NAME = config['model']['vision_encoder_name_or_path']\n",
    "    LLM_NAME = config['model']['llm_name_or_path'] # Added for tokenizer\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Config file not found at {CONFIG_PATH}. Using default model names.\")\n",
    "    VISION_ENCODER_NAME = 'openai/clip-vit-large-patch14-336' # Fallback\n",
    "    LLM_NAME = 'lmsys/vicuna-7b-v1.5' # Fallback\n",
    "except KeyError as e:\n",
    "    print(f\"Warning: Key {e} not found in {CONFIG_PATH}. Using defaults.\")\n",
    "    VISION_ENCODER_NAME = config.get('model', {}).get('vision_encoder_name_or_path', 'openai/clip-vit-large-patch14-336')\n",
    "    LLM_NAME = config.get('model', {}).get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')\n",
    "\n",
    "# Load the CLIP image processor\n",
    "try:\n",
    "    clip_image_processor = AutoImageProcessor.from_pretrained(VISION_ENCODER_NAME)\n",
    "    print(f\"Successfully loaded CLIP image processor for: {VISION_ENCODER_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CLIP image processor for {VISION_ENCODER_NAME}: {e}\")\n",
    "    # Handle error appropriately, maybe raise or use default stats\n",
    "    clip_image_processor = None\n",
    "\n",
    "# Get normalization stats\n",
    "if clip_image_processor:\n",
    "    image_mean = clip_image_processor.image_mean\n",
    "    image_std = clip_image_processor.image_std\n",
    "else:\n",
    "    print(\"Warning: Using default ImageNet stats as fallback for normalization.\")\n",
    "    # Default fallback (ImageNet stats often used, but CLIP specific is better)\n",
    "    image_mean = [0.485, 0.456, 0.406]\n",
    "    image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create the fastai Normalize transform using CLIP stats\n",
    "clip_normalize = Normalize.from_stats(image_mean, image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Mean: [0.48145466, 0.4578275, 0.40821073]\n",
      "CLIP Std: [0.26862954, 0.26130258, 0.27577711]\n",
      "Fastai Normalize Transform: Normalize -- Tries to normalize batch with `mean` and `std` specified on `axes`\n"
     ]
    }
   ],
   "source": [
    "# Example: Print the stats and the transform\n",
    "print(f\"CLIP Mean: {image_mean}\")\n",
    "print(f\"CLIP Std: {image_std}\")\n",
    "print(f\"Fastai Normalize Transform: {clip_normalize}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: Text Tokenization and Template Handling (Stage 1 - Plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LLM's tokenizer (Vicuna) and define the 'plain' template formatting for Stage 1 pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Using token ID for <image>: 32000\n",
      "Using EOS token as pad token: </s> (ID: 2)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\" # Placeholder token for image features\n",
    "IMAGE_TOKEN_INDEX_PLACEHOLDER = -200 # Special marker used internally in input_ids\n",
    "IGNORE_INDEX = -100 # Standard ignore index for labels in loss calculation\n",
    "\n",
    "# Load the Vicuna tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        LLM_NAME,\n",
    "        model_max_length=config.get('data', {}).get('tokenizer_model_max_length', 2048),\n",
    "        padding_side=config.get('data', {}).get('tokenizer_padding_side', 'right'),\n",
    "        use_fast=True,\n",
    "    )\n",
    "    print(f\"Successfully loaded tokenizer for: {LLM_NAME}\")\n",
    "\n",
    "    # Add the image token as a special token, if it's not already there.\n",
    "    # This ensures the tokenizer knows about it, even though we replace its embedding later.\n",
    "    # Avoids warnings and potential token splitting issues.\n",
    "    # Check if the token already exists\n",
    "    current_vocab = tokenizer.get_vocab()\n",
    "    if DEFAULT_IMAGE_TOKEN not in current_vocab:\n",
    "        print(f\"Adding special token {DEFAULT_IMAGE_TOKEN} to tokenizer.\")\n",
    "        # Add the token as a *special* token. This typically means it won't be split\n",
    "        # and can be handled distinctly. `additional_special_tokens` is a common way.\n",
    "        num_added = tokenizer.add_special_tokens({'additional_special_tokens': [DEFAULT_IMAGE_TOKEN]})\n",
    "        if num_added > 0:\n",
    "            print(f\"Added {num_added} token(s). New vocab size: {len(tokenizer)}\")\n",
    "            # NOTE: If a token is truly added (increasing vocab size),\n",
    "            # the model's embedding layer MUST be resized later.\n",
    "            # For LLaVA, we usually map it to an existing ID or rely on the -200 replacement,\n",
    "            # but adding it as a special token helps tokenizer processing.\n",
    "    # Retrieve the assigned ID\n",
    "    IMAGE_TOKEN_ID = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)\n",
    "    print(f\"Using token ID for {DEFAULT_IMAGE_TOKEN}: {IMAGE_TOKEN_ID}\")\n",
    "    # Handle case where it might still be UNK if not properly added/found\n",
    "    if IMAGE_TOKEN_ID == tokenizer.unk_token_id:\n",
    "         print(f\"Warning: {DEFAULT_IMAGE_TOKEN} resolved to UNK token ID ({tokenizer.unk_token_id}). Check tokenizer setup.\")\n",
    "\n",
    "\n",
    "    # Set pad token if missing (common for LLaMA models)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token # Option 1: Use EOS\n",
    "        # tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Option 2: Add a new pad token\n",
    "        print(f\"Using EOS token as pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "        # Note: If adding a new pad token, the model's embedding layer needs resizing later!\n",
    "    else:\n",
    "        print(f\"Tokenizer already has pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer for {LLM_NAME}: {e}\")\n",
    "    tokenizer = None\n",
    "    IMAGE_TOKEN_ID = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_plain_template(conversations: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"Formats conversations using the 'plain' template for Stage 1 pre-training.\n",
    "\n",
    "    The 'plain' template uses the format: <image>\\n{caption}\\n\n",
    "    where {caption} is the value of the first 'gpt' turn.\n",
    "\n",
    "    Args:\n",
    "        conversations: A list of conversation turns (dictionaries with 'from' and 'value').\n",
    "\n",
    "    Returns:\n",
    "        The formatted string. Returns just the image token if no 'gpt' turn is found.\n",
    "    \"\"\"\n",
    "    caption = \"\" # Default to empty caption if no 'gpt' turn found\n",
    "    for turn in conversations:\n",
    "        if turn.get('from', '').lower() == 'gpt':\n",
    "            caption = turn.get('value', '')\n",
    "            break # Use the first GPT response as the caption\n",
    "\n",
    "    # Ensure the <image> token is always first, followed by newline and caption\n",
    "    # Remove any existing <image> token from caption to avoid duplicates\n",
    "    caption = caption.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "\n",
    "    # Return formatted string, handle empty caption case\n",
    "    # Add final newline consistent with some implementations?\n",
    "    # LLaVA reference often has <image>\\n{caption}\n",
    "    # Let's stick to <image>\\n{caption} for now, stripping trailing whitespace.\n",
    "    formatted = f\"{DEFAULT_IMAGE_TOKEN}\\n{caption}\".strip() if caption else f\"{DEFAULT_IMAGE_TOKEN}\"\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "#| export\n",
       "def format_plain_template(conversations: List[Dict[str, str]]) -> str:\n",
       "    \"\"\"Formats conversations using the 'plain' template for Stage 1 pre-training.\\n\\nThe 'plain' template uses the format: <image>\\n{caption}\\n\\nwhere {caption} is the value of the first 'gpt' turn.\\n\\nArgs:\\n    conversations: A list of conversation turns (dictionaries with 'from' and 'value').\\n\\nReturns:\\n    The formatted string. Returns just the image token if no 'gpt' turn is found.\\n    \"\"\"\n",
       "    caption = \"\" # Default to empty caption if no 'gpt' turn found\n",
       "    for turn in conversations:\n",
       "        if turn.get('from', '').lower() == 'gpt':\n",
       "            caption = turn.get('value', '')\n",
       "            break # Use the first GPT response as the caption\n",
       "\n",
       "    # Ensure the <image> token is always first, followed by newline and caption\n",
       "    # Remove any existing <image> token from caption to avoid duplicates\n",
       "    caption = caption.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
       "    \n",
       "    # Return formatted string, handle empty caption case\n",
       "    # Add final newline consistent with some implementations?\n",
       "    # LLaVA reference often has <image>\\n{caption}\\n\n",
       "    # Let's stick to <image>\\n{caption} for now, stripping trailing whitespace.\n",
       "    formatted = f\"{DEFAULT_IMAGE_TOKEN}\\n{caption}\".strip() if caption else f\"{DEFAULT_IMAGE_TOKEN}\"\n",
       "    return formatted\n",
       "```"
      ],
      "text/plain": [
       "<showdoc.show_doc at 0x7f080cc895d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(format_plain_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Template Formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 (Standard): <image>\n",
      "This is the caption.\n",
      "Test Case 2 (Image token in caption): <image>\n",
      "Caption with image token removed.\n",
      "Test Case 3 (No GPT turn): <image>\n",
      "Test Case 4 (Empty conversation): <image>\n"
     ]
    }
   ],
   "source": [
    "conv1 = [{'from': 'human', 'value': '<image>\\nDescribe.'}, {'from': 'gpt', 'value': 'This is the caption.'}]\n",
    "conv2 = [{'from': 'human', 'value': 'Describe.'}, {'from': 'gpt', 'value': '<image>Caption with image token removed.'}]\n",
    "conv3 = [{'from': 'human', 'value': '<image>\\nDescribe.'}]\n",
    "conv4 = []\n",
    "\n",
    "print(f\"Test Case 1 (Standard): {format_plain_template(conv1)}\")\n",
    "print(f\"Test Case 2 (Image token in caption): {format_plain_template(conv2)}\")\n",
    "print(f\"Test Case 3 (No GPT turn): {format_plain_template(conv3)}\")\n",
    "print(f\"Test Case 4 (Empty conversation): {format_plain_template(conv4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLaVATextTokenizer(Transform):\n",
    "    \"\"\"A fastai Transform to format and tokenize text data for LLaVA stage 1.\n",
    "\n",
    "    Applies the 'plain' template formatting and then tokenizes the text, returning only the input IDs.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, template_formatter=format_plain_template):\n",
    "        store_attr()\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided and loaded successfully.\")\n",
    "\n",
    "    def encodes(self, conversations: List[Dict[str, str]]) -> List[int]:\n",
    "        \"\"\"Applies formatting and tokenization to conversation data.\n",
    "\n",
    "        Args:\n",
    "            conversations: Raw conversation list from the dataset sample.\n",
    "\n",
    "        Returns:\n",
    "            A list of input token IDs.\n",
    "        \"\"\"\n",
    "        formatted_text = self.template_formatter(conversations)\n",
    "        # Tokenize the formatted text.\n",
    "        # We don't pad here; padding is done at the batch level.\n",
    "        tokenized_output = self.tokenizer(formatted_text,\n",
    "                                         return_tensors=None, # Let batch collation handle tensor conversion + padding\n",
    "                                         add_special_tokens=True, # Add BOS/EOS if tokenizer configured to do so\n",
    "                                         truncation=False # Truncation can be done later if needed\n",
    "                                        )\n",
    "        # Return just the input_ids list for DataBlock item_tfms\n",
    "        # We use TensorBase here so fastai recognizes it as tensor-like for collation\n",
    "        # return TensorBase(tokenized_output['input_ids'])\n",
    "        # Update: Returning raw list might be simpler, collation handles tensor conversion\n",
    "        return tokenized_output['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Tokenizer Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Conversations: [{'from': 'human', 'value': '<image>\\nDescribe this.'}, {'from': 'gpt', 'value': 'It is red.'}]\n",
      "Formatted Text: <image>\n",
      "It is red.\n",
      "Tokenized Output (Input IDs): [1, 32000, 29871, 13, 490, 338, 2307, 29889, 2]\n",
      "Decoded Tokens: ['<s>', '<image>', '\\n', 'It', ' is', ' red', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "if tokenizer:\n",
    "    # Example conversation\n",
    "    example_conv = [{'from': 'human', 'value': '<image>\\nDescribe this.'}, {'from': 'gpt', 'value': 'It is red.'}]\n",
    "\n",
    "    # Create the transform instance\n",
    "    llava_tokenizer_tfm = LLaVATextTokenizer(tokenizer)\n",
    "\n",
    "    # Apply the transform\n",
    "    tokenized_ids = llava_tokenizer_tfm(example_conv)\n",
    "\n",
    "    print(f\"Original Conversations: {example_conv}\")\n",
    "    # Re-format to show what was tokenized\n",
    "    formatted = format_plain_template(example_conv)\n",
    "    print(f\"Formatted Text: {formatted}\")\n",
    "    print(f\"Tokenized Output (Input IDs): {tokenized_ids}\")\n",
    "\n",
    "    # Decode for verification\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_ids)\n",
    "    print(f\"Decoded Tokens: {decoded_tokens}\")\n",
    "\n",
    "    # Ensure the output is a list of integers\n",
    "    assert isinstance(tokenized_ids, list)\n",
    "    assert all(isinstance(x, int) for x in tokenized_ids)\n",
    "else:\n",
    "    print(\"Tokenizer not loaded, skipping tokenizer transform test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Implement Custom Batch Transform / Collate Function (Stage 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transform handles batch-level operations: padding, masking, image token replacement, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 2\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "class LLaVABatchTransform(Transform):\n",
    "    \"\"\" Custom batch transform for LLaVA stage 1.\n",
    "        Handles image normalization, text padding, attention mask creation,\n",
    "        image token marker replacement, label creation, and label masking for the 'plain' template.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, image_token_id=None):\n",
    "        store_attr()\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided.\")\n",
    "\n",
    "        if image_token_id is None:\n",
    "            self.image_token_id = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)\n",
    "            if self.image_token_id == tokenizer.unk_token_id:\n",
    "                print(f\"Warning: {DEFAULT_IMAGE_TOKEN} not found in tokenizer vocab. Using UNK ID: {self.image_token_id}\")\n",
    "        else:\n",
    "            self.image_token_id = image_token_id\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        if self.pad_token_id is None:\n",
    "            raise ValueError(\"Tokenizer must have a pad_token_id defined.\")\n",
    "\n",
    "        print(f\"LLaVABatchTransform initialized. Image Token ID: {self.image_token_id}, Pad Token ID: {self.pad_token_id}\")\n",
    "\n",
    "    def encodes(self, samples: List[Tuple[torch.Tensor, List[int]]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Collates samples into a batch, applies padding, masking, and normalization.\n",
    "\n",
    "        Args:\n",
    "            samples: A list of tuples, where each tuple contains:\n",
    "                (image_tensor: torch.Tensor, token_ids: List[int])\n",
    "                as produced by the item transforms.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the collated batch:\n",
    "            { 'pixel_values': torch.Tensor, 'input_ids': torch.Tensor,\n",
    "              'attention_mask': torch.Tensor, 'labels': torch.Tensor }\n",
    "        \"\"\"\n",
    "        if not samples:\n",
    "            return {}\n",
    "\n",
    "        # 1. Separate images and text token IDs\n",
    "        image_tensors = [s[0] for s in samples]\n",
    "        text_token_ids_list = [s[1] for s in samples]\n",
    "\n",
    "        # 2. Stack and normalize image tensors\n",
    "        # Assume images are already tensors via ToTensor in item_tfms\n",
    "        # Apply normalization (expects BCHW format)\n",
    "        # Make sure images are float first (IntToFloatTensor might be needed before this transform)\n",
    "        images_stacked = torch.stack(image_tensors)\n",
    "        # Need to ensure clip_normalize is available in the scope or passed in\n",
    "        # For now, assume it's globally available from earlier cell\n",
    "        normalized_images = clip_normalize(images_stacked)\n",
    "\n",
    "        # 3. Pad text sequences using tokenizer\n",
    "        # We provide the list of lists directly to tokenizer.pad\n",
    "        padded_texts = self.tokenizer.pad(\n",
    "            {'input_ids': text_token_ids_list},\n",
    "            padding='longest', # Pad to the longest sequence in the batch\n",
    "            return_tensors='pt', # Return PyTorch tensors\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids = padded_texts['input_ids']\n",
    "        attention_mask = padded_texts['attention_mask']\n",
    "\n",
    "        # 4. Create labels by cloning input_ids\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # 5. Find image token ID and replace with placeholder -200 in input_ids\n",
    "        # We do this *after* padding and *before* masking labels\n",
    "        input_ids[input_ids == self.image_token_id] = IMAGE_TOKEN_INDEX_PLACEHOLDER\n",
    "\n",
    "        # 6. Apply label masking for 'plain' template\n",
    "        # Mask everything up to and including the image token, and padding tokens.\n",
    "        # The caption starts *after* the image token (and the newline following it).\n",
    "        for i in range(labels.shape[0]): # Iterate through each sample in the batch\n",
    "            # Find the index of the image token (before it was replaced in input_ids)\n",
    "            # Note: We search for the *original* ID in the cloned labels tensor.\n",
    "            image_token_indices = torch.where(labels[i] == self.image_token_id)[0]\n",
    "            if len(image_token_indices) > 0:\n",
    "                image_token_idx = image_token_indices[0].item()\n",
    "                # Mask everything up to and including the image token.\n",
    "                # For 'plain' template (<image>\\n{caption}), the caption starts at index image_token_idx + 1\n",
    "                # (assuming newline is one token, which might not be true). Be careful.\n",
    "                # Let's find the first non-special token *after* the image token.\n",
    "                # We assume the template is <image>\\n{caption} or similar.\n",
    "                # The image token is expected at the beginning (or after BOS).\n",
    "                mask_until_idx = image_token_idx + 1 # Start masking after image token\n",
    "                \n",
    "                # Heuristic: Assume caption starts after image token and potential newline.\n",
    "                # We mask the image token itself and tokens before it.\n",
    "                labels[i, :mask_until_idx] = IGNORE_INDEX\n",
    "            else:\n",
    "                # If image token wasn't found (shouldn't happen with plain template),\n",
    "                # mask the entire sequence as a safety measure.\n",
    "                print(f\"Warning: Image token ID {self.image_token_id} not found in labels for sample {i}. Masking all.\")\n",
    "                labels[i, :] = IGNORE_INDEX\n",
    "\n",
    "            # Also mask padding tokens\n",
    "            labels[i][attention_mask[i] == 0] = IGNORE_INDEX\n",
    "            \n",
    "            # Specific check: Mask the BOS token if present (usually token ID 1 for Llama)\n",
    "            if labels[i, 0] == self.tokenizer.bos_token_id:\n",
    "                labels[i, 0] = IGNORE_INDEX\n",
    "\n",
    "        # 7. Return the prepared batch as a dictionary\n",
    "        return {\n",
    "            'pixel_values': normalized_images,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Make the transform usable in fastai pipelines\n",
    "LLaVABatchTransform.split_idx = None # Apply to both train and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Batch Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Input Samples ---",
      "Sample 0:",
      "  Image shape: torch.Size([3, 336, 336])",
      "  Token IDs: [1, 32000, 29871, 13, 490, 338, 2307, 29889, 2]",
      "Sample 1:",
      "  Image shape: torch.Size([3, 336, 336])",
      "  Token IDs: [1, 32000, 29871, 13, 1243, 526, 29918, 6388, 29889, 2]",
      "",
      "--- Collated Batch ---",
      "Pixel Values Shape: torch.Size([2, 3, 336, 336])",
      "Input IDs Shape: torch.Size([2, 10])",
      "Input IDs:",
      "tensor([[    1,  -200, 29871,    13,   490,   338,  2307, 29889,     2,     2],",
      "        [    1,  -200, 29871,    13,  1243,   526, 29918,  6388, 29889,     2]])",
      "Attention Mask Shape: torch.Size([2, 10])",
      "Attention Mask:",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])",
      "Labels Shape: torch.Size([2, 10])",
      "Labels:",
      "tensor([[ -100,  -100,  -100,    13,   490,   338,  2307, 29889,     2,  -100],",
      "        [ -100,  -100,  -100,    13,  1243,   526, 29918,  6388, 29889,     2]])",
      "",
      "--- Decoded Labels (showing non-masked tokens) ---",
      "Sample 0 Labels: [' It', ' is', ' red', '.', '</s>']",
      "Sample 1 Labels: [' is', ' green', '?', '</s>']",
      ""
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "if tokenizer:\n",
    "    # 1. Create dummy item samples (output of item transforms)\n",
    "    sample1_img = torch.rand(3, 336, 336) # Dummy image tensor\n",
    "    sample1_text_ids = llava_tokenizer_tfm([{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'It is red.'}])\n",
    "    sample1 = (sample1_img, sample1_text_ids)\n",
    "\n",
    "    sample2_img = torch.rand(3, 336, 336)\n",
    "    # Example with slightly longer text\n",
    "    sample2_text_ids = llava_tokenizer_tfm([{'from': 'human', 'value': '<image>'}, {'from': 'gpt', 'value': 'Is it green?'}])\n",
    "    sample2 = (sample2_img, sample2_text_ids)\n",
    "\n",
    "    dummy_samples = [sample1, sample2]\n",
    "\n",
    "    # 2. Instantiate the batch transform\n",
    "    batch_transform = LLaVABatchTransform(tokenizer)\n",
    "\n",
    "    # 3. Apply the transform to the dummy samples\n",
    "    collated_batch = batch_transform(dummy_samples)\n",
    "\n",
    "    # 4. Inspect the output\n",
    "    print(\"--- Input Samples ---\")\n",
    "    for i, s in enumerate(dummy_samples):\n",
    "        print(f\"Sample {i}:\")\n",
    "        print(f\"  Image shape: {s[0].shape}\")\n",
    "        print(f\"  Token IDs: {s[1]}\")\n",
    "        \n",
    "    print(\"\\n--- Collated Batch ---\")\n",
    "    if collated_batch:\n",
    "        print(f\"Pixel Values Shape: {collated_batch['pixel_values'].shape}\")\n",
    "        print(f\"Input IDs Shape: {collated_batch['input_ids'].shape}\")\n",
    "        print(f\"Input IDs:\\n{collated_batch['input_ids']}\")\n",
    "        print(f\"Attention Mask Shape: {collated_batch['attention_mask'].shape}\")\n",
    "        print(f\"Attention Mask:\\n{collated_batch['attention_mask']}\")\n",
    "        print(f\"Labels Shape: {collated_batch['labels'].shape}\")\n",
    "        print(f\"Labels:\\n{collated_batch['labels']}\")\n",
    "        \n",
    "        print(\"\\n--- Decoded Labels (showing non-masked tokens) ---\")\n",
    "        for i in range(collated_batch['labels'].shape[0]):\n",
    "            label_ids = collated_batch['labels'][i]\n",
    "            # Filter out ignored indices and decode\n",
    "            valid_label_ids = label_ids[label_ids != IGNORE_INDEX].tolist()\n",
    "            decoded_labels = tokenizer.convert_ids_to_tokens(valid_label_ids)\n",
    "            # Clean up special tokens for display if needed\n",
    "            cleaned_labels = [t for t in decoded_labels if t not in tokenizer.all_special_tokens]\n",
    "            print(f\"Sample {i} Labels: {cleaned_labels}\")\n",
    "\n",
    "        # Basic Assertions\n",
    "        assert 'pixel_values' in collated_batch\n",
    "        assert 'input_ids' in collated_batch\n",
    "        assert 'attention_mask' in collated_batch\n",
    "        assert 'labels' in collated_batch\n",
    "        assert collated_batch['input_ids'].shape == collated_batch['attention_mask'].shape == collated_batch['labels'].shape\n",
    "        # Check if image token replacement happened\n",
    "        assert torch.any(collated_batch['input_ids'] == IMAGE_TOKEN_INDEX_PLACEHOLDER)\n",
    "        # Check if labels are masked\n",
    "        assert torch.any(collated_batch['labels'] == IGNORE_INDEX)\n",
    "    else:\n",
    "        print(\"Batch collation failed.\")\n",
    "else:\n",
    "    print(\"Tokenizer not loaded, skipping batch transform test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Update Data Handling for Stage 2 (Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will adapt text processing for the Vicuna v1 template and update label masking logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for format_v1_template function\n",
    "# Placeholder for updated LLaVABatchTransform logic for v1 template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "metadata": {
   "path": "nbs/11_data_processing.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "Adaptive_Patching_VIT_fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}