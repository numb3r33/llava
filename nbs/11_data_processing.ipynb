{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "> Functions and definitions for preprocessing steps, including normalization stats, tokenization, template formatting, and batch transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoImageProcessor\n",
    "from fastai.vision.augment import Normalize\n",
    "from fastai.vision.all import *\n",
    "from fastai.text.all import *\n",
    "from fastai.data.transforms import Transform\n",
    "from fastai.torch_core import TensorBase, tensor\n",
    "import torch\n",
    "from typing import List, Dict, Union, Tuple, Any, Optional\n",
    "import copy\n",
    "\n",
    "# Attempt to import from llava utils, handle potential ImportError if running standalone\n",
    "try:\n",
    "    from llava.utils import load_config\n",
    "except ImportError:\n",
    "    print(\"Warning: llava.utils not found. load_config function might be unavailable.\")\n",
    "    def load_config(path): return {}\n",
    "\n",
    "# Import conversation handling logic (adapt from LLaVA reference or define here)\n",
    "# For now, let's define a simple structure based on Vicuna v1 description\n",
    "from llava.conversation import conv_templates, SeparatorStyle # Assuming this exists based on reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\" # Placeholder token for image features\n",
    "IMAGE_TOKEN_INDEX_PLACEHOLDER = -200 # Special marker used internally in input_ids\n",
    "IGNORE_INDEX = -100 # Standard ignore index for labels in loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config and Initialize Processors/Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from configs/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n",
      "CLIP Mean: [0.48145466, 0.4578275, 0.40821073]\n",
      "CLIP Std: [0.26862954, 0.26130258, 0.27577711]\n",
      "Fastai Normalize Transform: Normalize -- {'mean': tensor([[[[0.4815]],\n",
      "\n",
      "         [[0.4578]],\n",
      "\n",
      "         [[0.4082]]]]), 'std': tensor([[[[0.2686]],\n",
      "\n",
      "         [[0.2613]],\n",
      "\n",
      "         [[0.2758]]]]), 'axes': (0, 2, 3)}\n",
      "(enc:1,dec:1)\n",
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# --- Configuration Loading --- \n",
    "CONFIG_PATH = 'configs/config.yaml'\n",
    "config = {}\n",
    "try:\n",
    "    config = load_config(CONFIG_PATH)\n",
    "    print(f\"Loaded config from {CONFIG_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Config file not found at {CONFIG_PATH}. Using default model names.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading config from {CONFIG_PATH}: {e}. Using defaults.\")\n",
    "\n",
    "# Get model names from config or use defaults\n",
    "VISION_ENCODER_NAME = config.get('model', {}).get('vision_encoder_name_or_path', 'openai/clip-vit-large-patch14-336')\n",
    "LLM_NAME = config.get('model', {}).get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')\n",
    "TOKENIZER_MAX_LEN = config.get('data', {}).get('tokenizer_model_max_length', 2048)\n",
    "TOKENIZER_PADDING_SIDE = config.get('data', {}).get('tokenizer_padding_side', 'right')\n",
    "\n",
    "# --- Image Processor and Normalization --- \n",
    "clip_image_processor = None\n",
    "image_mean = [0.485, 0.456, 0.406] # Default ImageNet stats\n",
    "image_std = [0.229, 0.224, 0.225]\n",
    "try:\n",
    "    clip_image_processor = AutoImageProcessor.from_pretrained(VISION_ENCODER_NAME)\n",
    "    image_mean = clip_image_processor.image_mean\n",
    "    image_std = clip_image_processor.image_std\n",
    "    print(f\"Successfully loaded CLIP image processor for: {VISION_ENCODER_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading CLIP image processor for {VISION_ENCODER_NAME}: {e}. Using default ImageNet stats.\")\n",
    "\n",
    "clip_normalize = Normalize.from_stats(image_mean, image_std)\n",
    "print(f\"CLIP Mean: {image_mean}\")\n",
    "print(f\"CLIP Std: {image_std}\")\n",
    "print(f\"Fastai Normalize Transform: {clip_normalize}\")\n",
    "\n",
    "# --- Tokenizer --- \n",
    "tokenizer = None\n",
    "IMAGE_TOKEN_ID = None\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        LLM_NAME,\n",
    "        model_max_length=TOKENIZER_MAX_LEN,\n",
    "        padding_side=TOKENIZER_PADDING_SIDE,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    print(f\"Successfully loaded tokenizer for: {LLM_NAME}\")\n",
    "\n",
    "    current_vocab = tokenizer.get_vocab()\n",
    "    if DEFAULT_IMAGE_TOKEN not in current_vocab:\n",
    "        print(f\"Adding special token {DEFAULT_IMAGE_TOKEN} to tokenizer.\")\n",
    "        num_added = tokenizer.add_special_tokens({'additional_special_tokens': [DEFAULT_IMAGE_TOKEN]})\n",
    "        if num_added > 0:\n",
    "            print(f\"Added {num_added} token(s). New vocab size: {len(tokenizer)}\")\n",
    "    \n",
    "    IMAGE_TOKEN_ID = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)\n",
    "    print(f\"Using token ID for {DEFAULT_IMAGE_TOKEN}: {IMAGE_TOKEN_ID}\")\n",
    "    if IMAGE_TOKEN_ID == tokenizer.unk_token_id:\n",
    "         print(f\"Warning: {DEFAULT_IMAGE_TOKEN} resolved to UNK token ID ({tokenizer.unk_token_id}). Check tokenizer setup.\")\n",
    "         # Attempt to force it if necessary and if vocab doesn't contain it\n",
    "         # This is risky if the ID is already used\n",
    "         if DEFAULT_IMAGE_TOKEN not in current_vocab:\n",
    "              IMAGE_TOKEN_ID = len(tokenizer) - 1 # Use the newly added token ID\n",
    "              print(f\"Using explicitly added token ID: {IMAGE_TOKEN_ID}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.unk_token # Use UNK as pad if no PAD exists (like Llama-2)\n",
    "        print(f\"Set pad token to UNK token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "    else:\n",
    "        print(f\"Tokenizer already has pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Fatal Error: Could not load tokenizer for {LLM_NAME}: {e}\")\n",
    "    # Handle error appropriately in a real application\n",
    "    # For notebook execution, print warning and continue if possible\n",
    "    tokenizer = None\n",
    "    IMAGE_TOKEN_ID = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Formatting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_plain_template(conversations: List[Dict[str, str]], tokenizer: AutoTokenizer = tokenizer) -> str:\n",
    "    \"\"\"Formats conversations using the 'plain' template for Stage 1 pre-training.\n",
    "    \n",
    "    The 'plain' template uses the format: <image>\\n{caption}\\n\n",
    "    where {caption} is the value of the first 'gpt' turn.\n",
    "    Handles moving <image> token to the start if found elsewhere in the input.\n",
    "\n",
    "    Args:\n",
    "        conversations: A list of conversation turns (dictionaries with 'from' and 'value').\n",
    "        tokenizer: The tokenizer instance (needed for special tokens, though not used here).\n",
    "\n",
    "    Returns:\n",
    "        The formatted string. Returns just the image token if no 'gpt' turn is found.\n",
    "    \"\"\"\n",
    "    caption = \"\"\n",
    "    image_token_found = False\n",
    "    first_human_turn_value = None\n",
    "\n",
    "    # First pass: find caption and check for image token\n",
    "    for i, turn in enumerate(conversations):\n",
    "        value = turn.get('value', '')\n",
    "        if turn.get('from', '').lower() == 'gpt' and not caption: # Only take first caption\n",
    "            caption = value\n",
    "        if DEFAULT_IMAGE_TOKEN in value:\n",
    "            image_token_found = True\n",
    "        if turn.get('from', '').lower() == 'human' and first_human_turn_value is None:\n",
    "             first_human_turn_value = value\n",
    "\n",
    "    # Ensure <image> token is at the start, conceptually\n",
    "    # Remove <image> from caption if present\n",
    "    caption = caption.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "\n",
    "    # Construct final output: <image>\\n{caption}\n",
    "    formatted = f\"{DEFAULT_IMAGE_TOKEN}\\n{caption}\".strip() if caption else DEFAULT_IMAGE_TOKEN\n",
    "    return formatted\n",
    "\n",
    "\n",
    "def format_v1_template(conversations: List[Dict[str, str]], tokenizer: AutoTokenizer = tokenizer) -> str:\n",
    "    \"\"\"Formats conversations using the Vicuna v1 template.\n",
    "\n",
    "    Handles moving the <image> token to the beginning of the *first* human message.\n",
    "    Uses the `conv_templates['v1']` structure.\n",
    "\n",
    "    Args:\n",
    "        conversations: A list of conversation turns.\n",
    "        tokenizer: The tokenizer instance.\n",
    "\n",
    "    Returns:\n",
    "        The fully formatted prompt string according to Vicuna v1 template.\n",
    "    \"\"\"\n",
    "    if not hasattr(conv_templates, 'v1'):\n",
    "         raise ValueError(\"Vicuna v1 conversation template ('v1') not found in conversation_lib.\")\n",
    "\n",
    "    # Create a deep copy to avoid modifying the template dictionary directly\n",
    "    conv = copy.deepcopy(conv_templates['v1'])\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Preprocess: Move <image> token to the start of the first human turn\n",
    "    processed_conversations = []\n",
    "    image_token_moved = False\n",
    "    for i, turn in enumerate(conversations):\n",
    "        value = turn.get('value', '')\n",
    "        from_role = turn.get('from', '').lower()\n",
    "\n",
    "        new_turn = copy.deepcopy(turn)\n",
    "\n",
    "        if DEFAULT_IMAGE_TOKEN in value:\n",
    "            if from_role == 'human' and not image_token_moved:\n",
    "                # Move to start of this turn, remove from original position\n",
    "                new_turn['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + value.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "                image_token_moved = True\n",
    "            else:\n",
    "                 # Remove image token if found elsewhere (e.g., GPT response or later human turn)\n",
    "                 new_turn['value'] = value.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "        \n",
    "        processed_conversations.append(new_turn)\n",
    "    \n",
    "    # If image token was never found, add it to the start of the first human turn if one exists\n",
    "    if not image_token_moved:\n",
    "         found_human = False\n",
    "         for i, turn in enumerate(processed_conversations):\n",
    "              if turn.get('from', '').lower() == 'human':\n",
    "                   processed_conversations[i]['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + turn.get('value', '')\n",
    "                   found_human = True\n",
    "                   break\n",
    "         # If no human turn exists, prepend <image>\\n before the system prompt or start\n",
    "         # This is unlikely for instruct data but handles edge cases.\n",
    "         # However, standard LLaVA format assumes <image> is tied to a human turn.\n",
    "         # Let's stick to adding it to the first human turn.\n",
    "         # If no human turn exists, the template formatting will likely handle it appropriately\n",
    "         # or it might indicate an issue with the input data format.\n",
    "         if not found_human:\n",
    "              print(\"Warning: No 'human' turn found to prepend <image> token to.\")\n",
    "\n",
    "\n",
    "    # Append conversations to the template\n",
    "    for turn in processed_conversations:\n",
    "        role_key = turn.get('from', '').lower()\n",
    "        if role_key in roles:\n",
    "            conv.append_message(roles[role_key], turn.get('value'))\n",
    "        else:\n",
    "            # Handle unknown roles if necessary, e.g., skip or raise error\n",
    "            print(f\"Warning: Skipping turn with unknown role '{role_key}'.\")\n",
    "            continue\n",
    "    \n",
    "    # Append the assistant prompt\n",
    "    conv.append_message(roles['gpt'], None) \n",
    "\n",
    "    return conv.get_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Template Formatting - V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test Case 1: Standard --- \n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\n",
      "Describe this image. ASSISTANT: This is a red object.</s>USER: What shape is it? ASSISTANT: It is round.</s>USER: Thanks! ASSISTANT: \n",
      "\n",
      "--- Test Case 2: Image token later --- \n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\n",
      "Describe this image. ASSISTANT: This is a green object.</s>USER: What shape is it <image> ? ASSISTANT: It is square.</s>USER: Thanks! ASSISTANT: \n",
      "\n",
      "--- Test Case 3: Image token in GPT response (removed) --- \n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\n",
      "Describe. ASSISTANT: This is a blue object.</s>USER: Anything else? ASSISTANT: It might be <image> shiny.</s>USER: Ok ASSISTANT: \n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "conv1 = [\n",
    "    {'from': 'human', 'value': '<image>\\nDescribe this image.'},\n",
    "    {'from': 'gpt', 'value': 'This is a red object.'},\n",
    "    {'from': 'human', 'value': 'What shape is it?'},\n",
    "    {'from': 'gpt', 'value': 'It is round.'},\n",
    "    {'from': 'human', 'value': 'Thanks!'}\n",
    "]\n",
    "conv2 = [\n",
    "    {'from': 'human', 'value': 'Describe this image.'},\n",
    "    {'from': 'gpt', 'value': 'This is a green object.'},\n",
    "    {'from': 'human', 'value': 'What shape is it <image> ?'},\n",
    "    {'from': 'gpt', 'value': 'It is square.'},\n",
    "    {'from': 'human', 'value': 'Thanks!'}\n",
    "]\n",
    "conv3 = [\n",
    "    {'from': 'human', 'value': '<image>Describe.'},\n",
    "    {'from': 'gpt', 'value': 'This is a blue object.'},\n",
    "    {'from': 'human', 'value': 'Anything else?'},\n",
    "    {'from': 'gpt', 'value': 'It might be <image> shiny.'},\n",
    "    {'from': 'human', 'value': 'Ok'}\n",
    "]\n",
    "\n",
    "if tokenizer:\n",
    "    print(\"--- Test Case 1: Standard --- \")\n",
    "    print(format_v1_template(conv1, tokenizer))\n",
    "    print(\"\\n--- Test Case 2: Image token later --- \")\n",
    "    print(format_v1_template(conv2, tokenizer))\n",
    "    print(\"\\n--- Test Case 3: Image token in GPT response (removed) --- \")\n",
    "    print(format_v1_template(conv3, tokenizer))\n",
    "else:\n",
    "    print(\"Tokenizer not loaded, skipping v1 template test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLaVATextTokenizer(Transform):\n",
    "    \"\"\"A fastai Transform to format and tokenize text data for LLaVA stages.\n",
    "    \n",
    "    Applies the specified template formatting (e.g., 'plain' or 'v1') \n",
    "    and then tokenizes the text, returning only the input IDs.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, template_formatter):\n",
    "        store_attr()\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided and loaded successfully.\")\n",
    "\n",
    "    def encodes(self, conversations: list) -> list:\n",
    "        \"\"\"Applies formatting and tokenization to conversation data.\n",
    "\n",
    "        Args:\n",
    "            conversations: Raw conversation list from the dataset sample.\n",
    "\n",
    "        Returns:\n",
    "            A list of input token IDs.\n",
    "        \"\"\"\n",
    "        formatted_text = self.template_formatter(conversations, self.tokenizer)\n",
    "        tokenized_output = self.tokenizer(formatted_text,\n",
    "                                         return_tensors=None, \n",
    "                                         add_special_tokens=True, \n",
    "                                         truncation=False \n",
    "                                        )\n",
    "        return tokenized_output['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0, Template: plain\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "class LLaVABatchTransform(Transform):\n",
    "    \"\"\" Custom batch transform for LLaVA stages.\n",
    "        Handles image normalization, text padding reconstruction, attention mask creation,\n",
    "        image token marker replacement, label creation, and template-specific label masking.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, normalize_tfm: Normalize, template: str = 'plain', image_token_id: Optional[int] = None):\n",
    "        store_attr() # Stores tokenizer, normalize_tfm, template\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided.\")\n",
    "\n",
    "        if image_token_id is None:\n",
    "            self.image_token_id = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)\n",
    "            if self.image_token_id == tokenizer.unk_token_id and DEFAULT_IMAGE_TOKEN in tokenizer.added_tokens_decoder:\n",
    "                 self.image_token_id = tokenizer.added_tokens_decoder[DEFAULT_IMAGE_TOKEN]\n",
    "            if self.image_token_id == tokenizer.unk_token_id:\n",
    "                print(f\"Warning: {DEFAULT_IMAGE_TOKEN} not found in tokenizer vocab. Using UNK ID: {self.image_token_id}\")\n",
    "        else:\n",
    "            self.image_token_id = image_token_id\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        if self.pad_token_id is None:\n",
    "            raise ValueError(\"Tokenizer must have a pad_token_id defined.\")\n",
    "            \n",
    "        # Store template specific info if needed (e.g., separator tokens for v1)\n",
    "        self.assistant_role_token_ids = None\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "        self.bos_token_id = tokenizer.bos_token_id\n",
    "        self.sep = None\n",
    "        self.sep2 = None\n",
    "\n",
    "        if self.template == 'v1':\n",
    "            # Assuming conv_templates['v1'] exists and has roles and separators\n",
    "            conv_v1 = conv_templates.get('v1')\n",
    "            if conv_v1:\n",
    "                assistant_role_str = conv_v1.roles[1] # Typically 'ASSISTANT'\n",
    "                # Get the token IDs for the assistant role marker (e.g., 'ASSISTANT', ':')\n",
    "                # Need to handle potential space or other tokens depending on exact template\n",
    "                # Example: encode \"ASSISTANT: \"\n",
    "                self.assistant_role_token_ids = self.tokenizer.encode(f\"{assistant_role_str}:\", add_special_tokens=False)\n",
    "                self.sep = conv_v1.sep # Separator between turns (e.g., ' ')\n",
    "                self.sep2 = conv_v1.sep2 # Separator at end of conversation (e.g., '</s>')\n",
    "                print(f\"V1 template assistant role tokens: {self.assistant_role_token_ids}\")\n",
    "            else:\n",
    "                 print(\"Warning: Vicuna v1 template not found, v1 masking might not work correctly.\")\n",
    "\n",
    "        print(f\"LLaVABatchTransform initialized. Image Token ID: {self.image_token_id}, Pad Token ID: {self.pad_token_id}, Template: {self.template}\")\n",
    "\n",
    "    def encodes(self, collated_batch: tuple) -> dict:\n",
    "        \"\"\"Applies normalization, reconstructs padded tensors, applies masking based on template.\n",
    "        Args:\n",
    "            collated_batch: A tuple (collated_image_tensors, list_of_positional_token_tensors).\n",
    "        Returns:\n",
    "            A dictionary containing the fully processed batch ready for the model.\n",
    "        \"\"\"\n",
    "        if not isinstance(collated_batch, tuple) or len(collated_batch) != 2:\n",
    "             print(f\"Warning: LLaVABatchTransform received unexpected input type: {type(collated_batch)}. Skipping.\")\n",
    "             return collated_batch\n",
    "\n",
    "        collated_images, list_of_positional_tensors = collated_batch\n",
    "\n",
    "        if not isinstance(collated_images, torch.Tensor):\n",
    "             raise TypeError(f\"Expected first element of collated batch to be a Tensor, got {type(collated_images)}\")\n",
    "        if not isinstance(list_of_positional_tensors, list) or not all(isinstance(t, torch.Tensor) for t in list_of_positional_tensors):\n",
    "             raise TypeError(f\"Expected second element of collated batch to be a list of Tensors, got {type(list_of_positional_tensors)}\")\n",
    "        if not list_of_positional_tensors:\n",
    "            print(\"Warning: Received empty list of positional tensors.\")\n",
    "            return {}\n",
    "\n",
    "        # 1. Normalize images\n",
    "        normalized_images = self.normalize_tfm(collated_images)\n",
    "\n",
    "        # 2. Reconstruct padded input_ids tensor\n",
    "        try:\n",
    "            # Use pad_sequence directly for more control over padding value\n",
    "            # Ensure tensors are on the same device before padding\n",
    "            device = list_of_positional_tensors[0].device\n",
    "            input_ids = pad_sequence([t.to(device) for t in list_of_positional_tensors], \n",
    "                                       batch_first=True, \n",
    "                                       padding_value=self.pad_token_id)\n",
    "        except Exception as e:\n",
    "             print(\"Error padding positional tensors. Check consistency.\")\n",
    "             for i, t in enumerate(list_of_positional_tensors): print(f\"Tensor {i} shape: {t.shape}\")\n",
    "             raise e\n",
    "\n",
    "        # 3. Create attention mask\n",
    "        attention_mask = (input_ids != self.pad_token_id).long()\n",
    "\n",
    "        # 4. Create labels by cloning input_ids BEFORE replacement\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # 5. Find image token ID and replace with placeholder -200 in input_ids\n",
    "        input_ids_processed = input_ids.clone()\n",
    "        input_ids_processed[input_ids_processed == self.image_token_id] = IMAGE_TOKEN_INDEX_PLACEHOLDER\n",
    "\n",
    "        # 6. Apply label masking based on the template\n",
    "        if self.template == 'plain':\n",
    "            self._apply_plain_masking(labels, attention_mask)\n",
    "        elif self.template == 'v1':\n",
    "            self._apply_v1_masking(labels, attention_mask)\n",
    "        else:\n",
    "            print(f\"Warning: Unknown template '{self.template}'. Defaulting to plain masking.\")\n",
    "            self._apply_plain_masking(labels, attention_mask)\n",
    "\n",
    "        # 7. Return the prepared batch as a dictionary\n",
    "        return {\n",
    "            'pixel_values': normalized_images,\n",
    "            'input_ids': input_ids_processed, # Use the one with -200 placeholder\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels # Use the masked labels\n",
    "        }\n",
    "\n",
    "    def _apply_plain_masking(self, labels, attention_mask):\n",
    "        \"Masks labels for the 'plain' template.\" \n",
    "        for i in range(labels.shape[0]):\n",
    "            # Find the first occurrence of the image token ID\n",
    "            image_token_indices = torch.where(labels[i] == self.image_token_id)[0]\n",
    "            mask_until_idx = -1\n",
    "            if len(image_token_indices) > 0:\n",
    "                # Mask the image token itself and the newline following it\n",
    "                image_token_idx = image_token_indices[0].item()\n",
    "                mask_until_idx = image_token_idx + 1 # Index after image token\n",
    "                # Also check for newline token (13) immediately after\n",
    "                if image_token_idx + 1 < len(labels[i]) and labels[i, image_token_idx + 1] == 13:\n",
    "                     mask_until_idx += 1\n",
    "            else:\n",
    "                print(f\"Warning: Image token ID {self.image_token_id} not found in labels for sample {i} (Plain template). Masking all.\")\n",
    "                mask_until_idx = labels.shape[1] # Mask everything if token not found\n",
    "\n",
    "            # Apply mask up to the determined index (exclusive)\n",
    "            if mask_until_idx > 0:\n",
    "                 labels[i, :mask_until_idx] = IGNORE_INDEX\n",
    "            \n",
    "            # Also mask padding tokens\n",
    "            labels[i][attention_mask[i] == 0] = IGNORE_INDEX\n",
    "            \n",
    "            # Mask BOS if present\n",
    "            if labels.shape[1] > 0 and labels[i, 0] == self.bos_token_id:\n",
    "                labels[i, 0] = IGNORE_INDEX\n",
    "                \n",
    "    def _find_subsequence(self, main_tensor, sub_tensor):\n",
    "        \"Finds the start indices of a sub-tensor within a main tensor.\"\n",
    "        # Simple sliding window comparison\n",
    "        n = main_tensor.size(0)\n",
    "        m = sub_tensor.size(0)\n",
    "        if m > n or m == 0:\n",
    "            return []\n",
    "        indices = []\n",
    "        sub_tensor = sub_tensor.to(main_tensor.device)\n",
    "        for i in range(n - m + 1):\n",
    "            if torch.equal(main_tensor[i:i+m], sub_tensor):\n",
    "                indices.append(i)\n",
    "        return indices\n",
    "\n",
    "    def _apply_v1_masking(self, labels, attention_mask):\n",
    "        \"Masks labels for the Vicuna 'v1' template.\" \n",
    "        if self.assistant_role_token_ids is None:\n",
    "            print(\"Warning: Assistant role tokens not initialized for v1 template. Cannot perform v1 masking.\")\n",
    "            # Fallback: Mask everything? Or mask only padding? Let's mask only padding.\n",
    "            labels[attention_mask == 0] = IGNORE_INDEX\n",
    "            return\n",
    "        \n",
    "        assistant_token_ids_tensor = torch.tensor(self.assistant_role_token_ids, dtype=torch.long)\n",
    "        len_assistant_prompt = len(self.assistant_role_token_ids)\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            current_labels = labels[i]\n",
    "            current_mask = torch.ones_like(current_labels) * IGNORE_INDEX # Start by masking everything\n",
    "\n",
    "            # Find all start indices of the assistant prompt\n",
    "            assistant_indices = self._find_subsequence(current_labels, assistant_token_ids_tensor)\n",
    "\n",
    "            valid_response_found = False\n",
    "            for start_idx in assistant_indices:\n",
    "                # The response starts after the assistant prompt\n",
    "                response_start_idx = start_idx + len_assistant_prompt\n",
    "                \n",
    "                # Find the end of the response (EOS or next turn)\n",
    "                response_end_idx = -1\n",
    "                # Check for EOS token\n",
    "                eos_indices = torch.where(current_labels[response_start_idx:] == self.eos_token_id)[0]\n",
    "                if len(eos_indices) > 0:\n",
    "                    response_end_idx = response_start_idx + eos_indices[0].item()\n",
    "                else:\n",
    "                    # If no EOS, the response goes to the end of the sequence (before padding)\n",
    "                    response_end_idx = torch.sum(attention_mask[i]).item() # Length of non-padding tokens\n",
    "                \n",
    "                # Unmask the response tokens (exclusive of the end index)\n",
    "                if response_start_idx < response_end_idx:\n",
    "                    current_mask[response_start_idx:response_end_idx] = current_labels[response_start_idx:response_end_idx]\n",
    "                    valid_response_found = True\n",
    "            \n",
    "            # Mask image token explicitly if it wasn't masked as part of the prompt\n",
    "            image_token_indices = torch.where(current_labels == self.image_token_id)[0]\n",
    "            if len(image_token_indices) > 0:\n",
    "                current_mask[image_token_indices[0].item()] = IGNORE_INDEX\n",
    "                \n",
    "            # Mask padding tokens (redundant if starting with full mask, but safe)\n",
    "            current_mask[attention_mask[i] == 0] = IGNORE_INDEX\n",
    "            \n",
    "            # Mask BOS token\n",
    "            if current_labels.shape[0] > 0 and current_labels[0] == self.bos_token_id:\n",
    "                current_mask[0] = IGNORE_INDEX\n",
    "                \n",
    "            # Assign the calculated mask back to the labels tensor\n",
    "            labels[i] = current_mask\n",
    "\n",
    "    # decodes method remains the same as the last fix\n",
    "    def decodes(self, batch: dict) -> tuple:\n",
    "        \"\"\"Decodes a batch dictionary back into a tuple of (images, texts).\"\"\"\n",
    "        decoded_images = []\n",
    "        decoded_texts = []\n",
    "        if not isinstance(batch, dict) or 'pixel_values' not in batch or 'input_ids' not in batch:\n",
    "             print(f\"Decode expected dict with 'pixel_values' and 'input_ids', got {type(batch)}\")\n",
    "             return ([], []) # Return empty tuple of lists\n",
    "\n",
    "        imgs = batch['pixel_values']\n",
    "        ids = batch['input_ids']\n",
    "        bs = imgs.shape[0]\n",
    "\n",
    "        for i in range(bs):\n",
    "            # Decode image\n",
    "            # Ensure image tensor is on CPU before decoding\n",
    "            img_tensor_cpu = imgs[i].cpu()\n",
    "            # Add batch dimension for decoder\n",
    "            img_decoded = self.normalize_tfm.decode(img_tensor_cpu.unsqueeze(0))[0]\n",
    "\n",
    "            # Decode text\n",
    "            ids_i = ids[i].clone().cpu() # Move to CPU\n",
    "            # Replace placeholder back with image token ID for decoding\n",
    "            ids_i[ids_i == IMAGE_TOKEN_INDEX_PLACEHOLDER] = self.image_token_id \n",
    "            \n",
    "            # Create attention mask based on pad token ID for decoding\n",
    "            attn_mask_i = (ids_i != self.pad_token_id)\n",
    "            actual_ids = ids_i[attn_mask_i].tolist()\n",
    "            \n",
    "            # Decode, skipping special tokens like BOS/EOS/PAD/UNK\n",
    "            text_decoded = self.tokenizer.decode(actual_ids, skip_special_tokens=True)\n",
    "            # Replace the image token string with the placeholder for display\n",
    "            text_decoded = text_decoded.replace(self.tokenizer.decode([self.image_token_id], skip_special_tokens=False), DEFAULT_IMAGE_TOKEN)\n",
    "\n",
    "            decoded_images.append(img_decoded)\n",
    "            decoded_texts.append(TitledStr(text_decoded)) # TitledStr for better display in fastai\n",
    "\n",
    "        return (decoded_images, decoded_texts)\n",
    "\n",
    "# Make the transform usable in fastai pipelines\n",
    "LLaVABatchTransform.split_idx = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Batch Transform - V1 Masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1 template assistant role tokens: [1792, 29889]\n",
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0, Template: v1\n",
      "\n",
      "--- Original Collated Input IDs (Padded) ---\n",
      "tensor([[    1,   748, 29871,  ...,     0,     0,     0],\n",
      "        [    1,   748, 29871,  ...,   338,  1016,     2]])\n",
      "\n",
      "--- Processed Batch (Output of Transform) ---\n",
      "Pixel Values Shape: torch.Size([2, 3, 336, 336])\n",
      "Input IDs Shape (with -200): torch.Size([2, 40])\n",
      "Input IDs:\n",
      "tensor([[    1,   748, 29871, 13563,   322,   278, ...,   9341,     2,\n",
      "             0,     0,     0],\n",
      "        [    1,   748, 29871, 13563,   322,   278, ...,   1016,     2,\n",
      "             0,     0,     0]])\n",
      "Attention Mask Shape: torch.Size([2, 40])\n",
      "Attention Mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Labels Shape: torch.Size([2, 40])\n",
      "Labels:\n",
      "tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100,   311,   338,   264,  3104,  1188,  29889,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100,   306, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100,  372,   338, 10614,  29973, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100,  338, 1016,   -100, -100]])\n",
      "\n",
      "--- Decoded Labels (Showing Loss Calculation Targets) ---\n",
      "Sample 0 Target Tokens: [' is', ' a', ' red', ' object', '.', '</s>']\n",
      "Sample 1 Target Tokens: [' is', ' green', '.', '</s>', ' is', ' round']\n",
      "\n",
      "V1 Masking Test Passed (Check decoded labels above).\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "if tokenizer:\n",
    "    try:\n",
    "        # 1. Create Sample Data (Tokenized using v1 template)\n",
    "        conv_a = [{'from': 'human', 'value': '<image>\\nDescribe image.'}, {'from': 'gpt', 'value': 'It is a red object.'}]\n",
    "        conv_b = [{'from': 'human', 'value': '<image>What color?'}, {'from': 'gpt', 'value': 'It is green.'}, {'from':'human', 'value': 'And shape?'}, {'from':'gpt', 'value':'It is round'}]\n",
    "        \n",
    "        tokenizer_tfm_v1 = LLaVATextTokenizer(tokenizer, template_formatter=format_v1_template)\n",
    "        \n",
    "        token_ids_a = tokenizer_tfm_v1(conv_a)\n",
    "        token_ids_b = tokenizer_tfm_v1(conv_b)\n",
    "        \n",
    "        # 2. Simulate Collation (Get padded tensor)\n",
    "        collated_ids_unprocessed = pad_sequence([torch.tensor(token_ids_a), torch.tensor(token_ids_b)], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=tokenizer.pad_token_id)\n",
    "        print(\"\\n--- Original Collated Input IDs (Padded) ---\")\n",
    "        print(collated_ids_unprocessed)\n",
    "        \n",
    "        # Dummy images\n",
    "        dummy_images = torch.rand(2, 3, 336, 336)\n",
    "        simulated_collated_batch = (dummy_images, [torch.tensor(token_ids_a), torch.tensor(token_ids_b)]) # Input to transform uses list\n",
    "\n",
    "        # 3. Instantiate and Apply Batch Transform for V1\n",
    "        batch_transform_v1 = LLaVABatchTransform(tokenizer, normalize_tfm=clip_normalize, template='v1')\n",
    "        processed_batch = batch_transform_v1(simulated_collated_batch)\n",
    "\n",
    "        # 4. Inspect Output\n",
    "        print(\"\\n--- Processed Batch (Output of Transform) ---\")\n",
    "        assert isinstance(processed_batch, dict)\n",
    "        print(f\"Pixel Values Shape: {processed_batch.get('pixel_values').shape}\")\n",
    "        print(f\"Input IDs Shape (with -200): {processed_batch.get('input_ids').shape}\")\n",
    "        print(f\"Input IDs:\\n{processed_batch.get('input_ids')}\")\n",
    "        print(f\"Attention Mask Shape: {processed_batch.get('attention_mask').shape}\")\n",
    "        print(f\"Attention Mask:\\n{processed_batch.get('attention_mask')}\")\n",
    "        print(f\"Labels Shape: {processed_batch.get('labels').shape}\")\n",
    "        print(f\"Labels:\\n{processed_batch.get('labels')}\")\n",
    "\n",
    "        print(\"\\n--- Decoded Labels (Showing Loss Calculation Targets) ---\")\n",
    "        if 'labels' in processed_batch:\n",
    "            for i in range(processed_batch['labels'].shape[0]):\n",
    "                label_ids = processed_batch['labels'][i]\n",
    "                # Filter out ignore_index for decoding verification\n",
    "                valid_label_ids = label_ids[label_ids != IGNORE_INDEX].tolist()\n",
    "                decoded_labels = tokenizer.decode(valid_label_ids)\n",
    "                # Split tokens for clearer view if needed\n",
    "                decoded_tokens_list = tokenizer.convert_ids_to_tokens(valid_label_ids)\n",
    "                \n",
    "                print(f\"Sample {i} Target Tokens: {decoded_tokens_list}\")\n",
    "        \n",
    "        # Add specific assertions for v1 masking if possible\n",
    "        # Example: Check if the first token (BOS) is masked\n",
    "        assert processed_batch['labels'][0, 0] == IGNORE_INDEX\n",
    "        assert processed_batch['labels'][1, 0] == IGNORE_INDEX\n",
    "        # Example: Check if image token position is masked\n",
    "        # Find where input_ids was image_token_id before replacement\n",
    "        original_img_pos_0 = torch.where(collated_ids_unprocessed[0] == IMAGE_TOKEN_ID)[0]\n",
    "        if len(original_img_pos_0) > 0:\n",
    "            assert processed_batch['labels'][0, original_img_pos_0[0].item()] == IGNORE_INDEX\n",
    "        original_img_pos_1 = torch.where(collated_ids_unprocessed[1] == IMAGE_TOKEN_ID)[0]\n",
    "        if len(original_img_pos_1) > 0:\n",
    "            assert processed_batch['labels'][1, original_img_pos_1[0].item()] == IGNORE_INDEX\n",
    "        \n",
    "        print(\"\\nV1 Masking Test Passed (Check decoded labels above).\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during V1 masking test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Tokenizer not loaded, skipping batch transform v1 test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/11_data_processing.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "llava"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}