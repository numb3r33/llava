{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "> Functions and definitions for preprocessing steps, including normalization stats, tokenization, and template formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformers import AutoProcessor, AutoTokenizer, AutoImageProcessor\n",
    "from fastai.vision.augment import Normalize\n",
    "from fastai.data.transforms import Transform\n",
    "import torch\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "from Adaptive_Patching_VIT_fastai.utils import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2 (Continued): Image Normalization Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CLIP image processor to get the correct normalization statistics (mean and standard deviation) required for the vision encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Load config to get model name\n",
    "CONFIG_PATH = 'configs/config.yaml'\n",
    "try:\n",
    "    config = load_config(CONFIG_PATH)\n",
    "    VISION_ENCODER_NAME = config['model']['vision_encoder_name_or_path']\n",
    "    LLM_NAME = config['model']['llm_name_or_path'] # Added for tokenizer\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Config file not found at {CONFIG_PATH}. Using default model names.\")\n",
    "    VISION_ENCODER_NAME = 'openai/clip-vit-large-patch14-336' # Fallback\n",
    "    LLM_NAME = 'lmsys/vicuna-7b-v1.5' # Fallback\n",
    "except KeyError as e:\n",
    "    print(f\"Warning: Key {e} not found in {CONFIG_PATH}. Using defaults.\")\n",
    "    VISION_ENCODER_NAME = config.get('model', {}).get('vision_encoder_name_or_path', 'openai/clip-vit-large-patch14-336')\n",
    "    LLM_NAME = config.get('model', {}).get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')\n",
    "\n",
    "# Load the CLIP image processor\n",
    "try:\n",
    "    clip_image_processor = AutoImageProcessor.from_pretrained(VISION_ENCODER_NAME)\n",
    "    print(f\"Successfully loaded CLIP image processor for: {VISION_ENCODER_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CLIP image processor for {VISION_ENCODER_NAME}: {e}\")\n",
    "    # Handle error appropriately, maybe raise or use default stats\n",
    "    clip_image_processor = None\n",
    "\n",
    "# Get normalization stats\n",
    "if clip_image_processor:\n",
    "    image_mean = clip_image_processor.image_mean\n",
    "    image_std = clip_image_processor.image_std\n",
    "else:\n",
    "    print(\"Warning: Using default ImageNet stats as fallback for normalization.\")\n",
    "    # Default fallback (ImageNet stats often used, but CLIP specific is better)\n",
    "    image_mean = [0.485, 0.456, 0.406]\n",
    "    image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create the fastai Normalize transform using CLIP stats\n",
    "clip_normalize = Normalize.from_stats(image_mean, image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Mean: [0.48145466, 0.4578275, 0.40821073]\n",
      "CLIP Std: [0.26862954, 0.26130258, 0.27577711]\n",
      "Fastai Normalize Transform: Normalize -- Tries to normalize batch with `mean` and `std` specified on `axes`\n"
     ]
    }
   ],
   "source": [
    "# Example: Print the stats and the transform\n",
    "print(f\"CLIP Mean: {image_mean}\")\n",
    "print(f\"CLIP Std: {image_std}\")\n",
    "print(f\"Fastai Normalize Transform: {clip_normalize}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: Text Tokenization and Template Handling (Stage 1 - Plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LLM's tokenizer (Vicuna) and define the 'plain' template formatting for Stage 1 pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\" # Placeholder token for image features\n",
    "\n",
    "# Load the Vicuna tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        LLM_NAME,\n",
    "        model_max_length=config.get('data', {}).get('tokenizer_model_max_length', 2048),\n",
    "        padding_side=config.get('data', {}).get('tokenizer_padding_side', 'right'),\n",
    "        use_fast=True,\n",
    "    )\n",
    "    print(f\"Successfully loaded tokenizer for: {LLM_NAME}\")\n",
    "    \n",
    "    # Set pad token if missing (common for LLaMA models)\n",
    "    if tokenizer.pad_token is None:\n",
    "        # tokenizer.pad_token = tokenizer.eos_token # Option 1: Use EOS\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Option 2: Add a new pad token\n",
    "        print(f\"Added pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "        # Note: If adding a new token, the model's embedding layer needs resizing later!\n",
    "        # For LLaVA/Vicuna, typically pad_token is implicitly handled or eos is used, check conventions.\n",
    "        # LLaVA often uses 0 (unk) or eos. Let's stick to eos if pad is None initially.\n",
    "        if tokenizer.pad_token is None:\n",
    "             tokenizer.pad_token = tokenizer.eos_token\n",
    "             print(f\"Using EOS token as pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer for {LLM_NAME}: {e}\")\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'NoneType'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_plain_template(conversations: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"Formats conversations using the 'plain' template for Stage 1 pre-training.\n",
    "\n",
    "    The 'plain' template uses the format: <image>\\n{caption}\n",
    "    where {caption} is the value of the first 'gpt' turn.\n",
    "\n",
    "    Args:\n",
    "        conversations: A list of conversation turns (dictionaries with 'from' and 'value').\n",
    "\n",
    "    Returns:\n",
    "        The formatted string. Returns just the image token if no 'gpt' turn is found.\n",
    "    \"\"\"\n",
    "    caption = \"\" # Default to empty caption if no 'gpt' turn found\n",
    "    for turn in conversations:\n",
    "        if turn.get('from', '').lower() == 'gpt':\n",
    "            caption = turn.get('value', '')\n",
    "            break # Use the first GPT response as the caption\n",
    "\n",
    "    # Ensure the <image> token is always first, followed by newline and caption\n",
    "    # Remove any existing <image> token from caption to avoid duplicates\n",
    "    caption = caption.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
    "    \n",
    "    # Return formatted string, handle empty caption case\n",
    "    return f\"{DEFAULT_IMAGE_TOKEN}\\n{caption}\".strip() if caption else f\"{DEFAULT_IMAGE_TOKEN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "#| export\n",
       "def format_plain_template(conversations: List[Dict[str, str]]) -> str:\n",
       "    \"\"\"Formats conversations using the 'plain' template for Stage 1 pre-training.\n",
       "\n",
       "    The 'plain' template uses the format: <image>\\n{caption}\n",
       "    where {caption} is the value of the first 'gpt' turn.\n",
       "\n",
       "    Args:\n",
       "        conversations: A list of conversation turns (dictionaries with 'from' and 'value').\n",
       "\n",
       "    Returns:\n",
       "        The formatted string. Returns just the image token if no 'gpt' turn is found.\n",
       "    \"\"\"\n",
       "    caption = \"\" # Default to empty caption if no 'gpt' turn found\n",
       "    for turn in conversations:\n",
       "        if turn.get('from', '').lower() == 'gpt':\n",
       "            caption = turn.get('value', '')\n",
       "            break # Use the first GPT response as the caption\n",
       "\n",
       "    # Ensure the <image> token is always first, followed by newline and caption\n",
       "    # Remove any existing <image> token from caption to avoid duplicates\n",
       "    caption = caption.replace(DEFAULT_IMAGE_TOKEN, '').strip()\n",
       "    \n",
       "    # Return formatted string, handle empty caption case\n",
       "    return f\"{DEFAULT_IMAGE_TOKEN}\\n{caption}\".strip() if caption else f\"{DEFAULT_IMAGE_TOKEN}\"\n",
       "```"
      ],
      "text/plain": [
       "<showdoc.show_doc at 0x7fa6141f7d30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(format_plain_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Template Formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 (Standard): <image>\n",
      "This is the caption.\n",
      "Test Case 2 (Image token in caption): <image>\n",
      "Caption with image token removed.\n",
      "Test Case 3 (No GPT turn): <image>\n",
      "Test Case 4 (Empty conversation): <image>\n"
     ]
    }
   ],
   "source": [
    "conv1 = [{'from': 'human', 'value': '<image>\n",
    "Describe.'}, {'from': 'gpt', 'value': 'This is the caption.'}]\n",
    "conv2 = [{'from': 'human', 'value': 'Describe.'}, {'from': 'gpt', 'value': '<image>Caption with image token removed.'}]\n",
    "conv3 = [{'from': 'human', 'value': '<image>\n",
    "Describe.'}]\n",
    "conv4 = []\n",
    "\n",
    "print(f\"Test Case 1 (Standard): {format_plain_template(conv1)}\")\n",
    "print(f\"Test Case 2 (Image token in caption): {format_plain_template(conv2)}\")\n",
    "print(f\"Test Case 3 (No GPT turn): {format_plain_template(conv3)}\")\n",
    "print(f\"Test Case 4 (Empty conversation): {format_plain_template(conv4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLaVATextTokenizer(Transform):\n",
    "    \"\"\"A fastai Transform to format and tokenize text data for LLaVA stage 1.\n",
    "    \n",
    "    Applies the 'plain' template formatting and then tokenizes the text.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, template_formatter=format_plain_template):\n",
    "        store_attr()\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided and loaded successfully.\")\n",
    "        \n",
    "    def encodes(self, conversations: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Applies formatting and tokenization to conversation data.\n",
    "\n",
    "        Args:\n",
    "            conversations: Raw conversation list from the dataset sample.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing 'input_ids' and potentially 'attention_mask' \n",
    "            as PyTorch tensors. The exact output depends on the tokenizer.\n",
    "        \"\"\"\n",
    "        formatted_text = self.template_formatter(conversations)\n",
    "        # Tokenize the formatted text. `return_tensors='pt'` is usually handled by \n",
    "        # DataBlock/DataLoader, but we ensure it returns tensors if used standalone.\n",
    "        # We don't pad here; padding is done at the batch level.\n",
    "        tokenized_output = self.tokenizer(formatted_text, \n",
    "                                         return_tensors=None, # Let batch collation handle tensor conversion + padding\n",
    "                                         add_special_tokens=True, # Add BOS/EOS if tokenizer configured to do so\n",
    "                                         truncation=False # Truncation can be done later if needed\n",
    "                                        )\n",
    "        # Return just the input_ids list for DataBlock item_tfms\n",
    "        # Attention mask will be created during batch collation\n",
    "        return {'input_ids': tokenized_output['input_ids']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Tokenizer Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Conversations: [{'from': 'human', 'value': '<image>\\nDescribe this.'}, {'from': 'gpt', 'value': 'It is red.'}]\n",
      "Formatted Text: <image>\n",
      "It is red.\n",
      "Tokenized Output: {'input_ids': [1, 32000, 29871, 13, 490, 338, 2307, 29889, 2]}\n",
      "Decoded Tokens: ['<s>', '<image>', '\\n', 'It', ' is', ' red', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "if tokenizer:\n",
    "    # Example conversation\n",
    "    example_conv = [{'from': 'human', 'value': '<image>\\nDescribe this.'}, {'from': 'gpt', 'value': 'It is red.'}]\n",
    "\n",
    "    # Create the transform instance\n",
    "    llava_tokenizer_tfm = LLaVATextTokenizer(tokenizer)\n",
    "\n",
    "    # Apply the transform\n",
    "    tokenized_result = llava_tokenizer_tfm(example_conv)\n",
    "    \n",
    "    print(f\"Original Conversations: {example_conv}\")\n",
    "    # Re-format to show what was tokenized\n",
    "    formatted = format_plain_template(example_conv)\n",
    "    print(f\"Formatted Text: {formatted}\") \n",
    "    print(f\"Tokenized Output: {tokenized_result}\")\n",
    "    \n",
    "    # Decode for verification\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_result['input_ids'])\n",
    "    print(f\"Decoded Tokens: {decoded_tokens}\")\n",
    "    \n",
    "    # Ensure the output is a dictionary with 'input_ids'\n",
    "    assert isinstance(tokenized_result, dict)\n",
    "    assert 'input_ids' in tokenized_result\n",
    "    assert isinstance(tokenized_result['input_ids'], list) # Before batching, should be list\n",
    "else:\n",
    "    print(\"Tokenizer not loaded, skipping tokenizer transform test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Implement Custom Batch Transform / Collate Function (Stage 1 - Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will be implemented later. It will include padding, attention mask creation, image token marker replacement (-200), and label masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for LLaVABatchTransform class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Update Data Handling for Stage 2 (Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will adapt text processing for the Vicuna v1 template and update label masking logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for format_v1_template function\n",
    "# Placeholder for updated LLaVABatchTransform logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "metadata": {
   "path": "nbs/11_data_preprocessing.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "Adaptive_Patching_VIT_fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
