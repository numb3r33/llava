{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Core\n",
    "\n",
    "> Core components for the training pipeline, including loss functions and potentially shared callbacks or utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.core import DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the core elements needed for training, starting with the custom loss function required for LLaVA-style training where certain tokens (like prompts and padding) are ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function (To be implemented in Step 3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for LLaVALoss implementation\n",
    "#| export\n",
    "class LLaVALoss(nn.Module):\n",
    "    \"\"\" Custom CrossEntropyLoss that ignores indices where labels are -100. \"\"\"\n",
    "    def __init__(self, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.loss_fct = nn.CrossEntropyLoss(ignore_index=self.ignore_index)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # Shift logits and labels for next token prediction\n",
    "        # output shape: (batch_size, sequence_length, vocab_size)\n",
    "        # target shape: (batch_size, sequence_length)\n",
    "        shift_logits = output[..., :-1, :].contiguous()\n",
    "        shift_labels = target[..., 1:].contiguous()\n",
    "\n",
    "        # Flatten the tokens\n",
    "        # Shift_logits flattened shape: (batch_size * (sequence_length - 1), vocab_size)\n",
    "        # Shift_labels flattened shape: (batch_size * (sequence_length - 1))\n",
    "        loss = self.loss_fct(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                             shift_labels.view(-1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities (e.g., Learner Setup - To be implemented later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for learner setup functions (e.g., get_stage1_learner)\n",
    "# These will utilize WandbCallback imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "metadata": {
   "path": "nbs/30_training_core.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "Adaptive_Patching_VIT_fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}