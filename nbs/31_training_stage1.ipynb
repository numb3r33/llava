{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 Training: Projector Pre-training\n",
    "\n",
    "> Sets up and runs the first stage of LLaVA training, focusing on pre-training the projector module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc # For memory cleanup\n",
    "import argparse # For command-line execution\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n",
      "Project root already in sys.path: /workspace/llava\n",
      "Loaded config from configs/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n",
      "CLIP Mean: [0.48145466, 0.4578275, 0.40821073]\n",
      "CLIP Std: [0.26862954, 0.26130258, 0.27577711]\n",
      "Fastai Normalize Transform: Normalize -- {'mean': tensor([[[[0.4815]],\n",
      "\n",
      "         [[0.4578]],\n",
      "\n",
      "         [[0.4082]]]], device='cuda:0'), 'std': tensor([[[[0.2686]],\n",
      "\n",
      "         [[0.2613]],\n",
      "\n",
      "         [[0.2758]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "(enc:2,dec:2)\n",
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n",
      "LLaVADataBlockStage1 defined.\n",
      "LLaVADataBlockStage2 defined.\n",
      "Project root already in sys.path: /workspace/llava\n",
      "Project root already in sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from fastai.learner import Learner\n",
    "from fastai.vision.all import * # For splitter\n",
    "\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.schedule import fit_one_cycle\n",
    "from fastai.callback.training import GradientAccumulation # Import GradientAccumulation\n",
    "from fastai.callback.fp16 import MixedPrecision # Import MixedPrecision\n",
    "from fastai.data.core import DataLoaders\n",
    "from functools import partial\n",
    "import wandb # Import wandb directly for cleanup\n",
    "import json # For dummy data creation\n",
    "import PIL.Image # For dummy data creation\n",
    "\n",
    "from llava.utils import load_config, init_wandb\n",
    "from llava.data.loading import get_stage1_dataloaders\n",
    "from llava.model.baseline import BaselineLLaVAModel\n",
    "from llava.training.core import LLaVALoss, LLaVAMixedPrecision, extract_loss_from_output, SafeGradientAccumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 Splitter Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a custom splitter function for the fastai `Learner`. In Stage 1, we only want to train the `projector` module, keeping the `vision_tower` and `language_model` frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llava_stage1_splitter(model: BaselineLLaVAModel):\n",
    "    \"\"\"Splits the `BaselineLLaVAModel` parameters for Stage 1 training.\n",
    "    \n",
    "    Only the parameters of the `projector` module are marked as trainable for the optimizer.\n",
    "    Vision tower is frozen. LLM's LoRA adapters (if any) remain trainable but won't be\n",
    "    passed to the optimizer in Stage 1. Base LLM weights are frozen (by PEFT or explicitly).\n",
    "    \n",
    "    Args:\n",
    "        model: An instance of `BaselineLLaVAModel`.\n",
    "        \n",
    "    Returns:\n",
    "        A list containing a single parameter group for the projector.\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'projector') or model.projector is None:\n",
    "        raise AttributeError(\"Model does not have a 'projector' attribute or it is None.\")\n",
    "        \n",
    "    print(\"Applying Stage 1 splitter: Ensuring only projector parameters are passed to optimizer.\")\n",
    "    \n",
    "    trainable_params = []\n",
    "    # Ensure projector parameters are trainable\n",
    "    if hasattr(model, 'projector') and model.projector is not None:\n",
    "        print(\"  - Setting projector parameters to require_grad=True.\")\n",
    "        for p in model.projector.parameters():\n",
    "            p.requires_grad = True\n",
    "        trainable_params.extend(list(model.projector.parameters()))\n",
    "    \n",
    "    # Ensure vision tower is frozen\n",
    "    if hasattr(model, 'vision_tower') and model.vision_tower is not None:\n",
    "        print(\"  - Setting vision_tower parameters to require_grad=False.\")\n",
    "        for p in model.vision_tower.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    # For the language model in Stage 1:\n",
    "    # - If PEFT/LoRA is used, get_peft_model has already frozen base weights and made adapters trainable.\n",
    "    #   We do NOT want to turn off grads for LoRA adapters here.\n",
    "    # - If no PEFT/LoRA, the base LLM should be frozen. This is handled in model.__init__\n",
    "    #   by the `self.language_model.requires_grad_(False)` call if not QLoRA/LoRA.\n",
    "    # So, the splitter's main job for Stage 1 regarding LLM is *not* to modify its requires_grad state\n",
    "    # but to ensure only projector params are given to the optimizer.\n",
    "    # The `model.__init__` handles initial freezing.\n",
    "\n",
    "    if not trainable_params:\n",
    "         raise ValueError(\"Splitter function resulted in no trainable parameters for Stage 1 (projector).\")\n",
    "    \n",
    "    print(f\"  - Stage 1 Splitter will provide {len(trainable_params)} projector parameters to the optimizer.\")\n",
    "    # The optimizer will only receive these parameters.\n",
    "    return trainable_params # This was already correct, the issue was modifying LLM params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### llava_stage1_splitter\n",
       "\n",
       ">      llava_stage1_splitter (model:llava.model.baseline.BaselineLLaVAModel)\n",
       "\n",
       "*Splits the `BaselineLLaVAModel` parameters for Stage 1 training.\n",
       "    \n",
       "    Only the parameters of the `projector` module are marked as trainable.\n",
       "    The `vision_tower` and `language_model` parameters will remain frozen.\n",
       "    \n",
       "    Args:\n",
       "        model: An instance of `BaselineLLaVAModel`.\n",
       "        \n",
       "    Returns:\n",
       "        A list containing a single parameter group for the projector.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### llava_stage1_splitter\n",
       "\n",
       ">      llava_stage1_splitter (model:llava.model.baseline.BaselineLLaVAModel)\n",
       "\n",
       "*Splits the `BaselineLLaVAModel` parameters for Stage 1 training.\n",
       "    \n",
       "    Only the parameters of the `projector` module are marked as trainable.\n",
       "    The `vision_tower` and `language_model` parameters will remain frozen.\n",
       "    \n",
       "    Args:\n",
       "        model: An instance of `BaselineLLaVAModel`.\n",
       "        \n",
       "    Returns:\n",
       "        A list containing a single parameter group for the projector.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(llava_stage1_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2 & 3.3: Setup Learner Configuration (Stage 1) with Optimization Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets up the `Learner` object for Stage 1, including the model, data, loss function, optimizer, splitter, and standard callbacks (W&B, SaveModel). It now also includes the `GradientAccumulation` and `MixedPrecision` callbacks based on configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stage1_learner(config: dict) -> Learner:\n",
    "    \"\"\"Configures and returns a fastai Learner for Stage 1 projector pre-training.\n",
    "       Includes optimization callbacks (GradientAccumulation, MixedPrecision) based on config.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A configured fastai Learner instance for Stage 1.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If DataLoaders or Model instantiation fails.\n",
    "        FileNotFoundError: If specified data paths in config are incorrect.\n",
    "        AttributeError: If the model is missing expected components (e.g., projector).\n",
    "    \"\"\"\n",
    "    print(\"--- Setting up Stage 1 Learner ---\")\n",
    "    \n",
    "    # 1. Load DataLoaders\n",
    "    print(\"Loading Stage 1 DataLoaders...\")\n",
    "    try:\n",
    "        dls = get_stage1_dataloaders(config)\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        print(f\"Error loading DataLoaders: {e}\")\n",
    "        raise RuntimeError(\"Failed to create Stage 1 DataLoaders. Check config paths and data availability.\") from e\n",
    "    if not dls:\n",
    "        raise RuntimeError(\"Stage 1 DataLoaders object is None.\")\n",
    "    print(f\"DataLoaders loaded. Train samples: {len(dls.train_ds)}, Valid samples: {len(dls.valid_ds)}\")\n",
    "\n",
    "    # 2. Instantiate Model\n",
    "    print(\"Instantiating BaselineLLaVAModel...\")\n",
    "    try:\n",
    "        model = BaselineLLaVAModel(config)\n",
    "        # Ensure model components loaded successfully\n",
    "        if model.vision_tower is None or model.language_model is None or model.projector is None:\n",
    "             raise RuntimeError(\"BaselineLLaVAModel initialization failed: one or more components are None.\")\n",
    "        print(\"Model instantiated successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error instantiating BaselineLLaVAModel: {e}\")\n",
    "        raise RuntimeError(\"Failed to instantiate baseline model.\") from e\n",
    "\n",
    "    # 3. Define Loss Function\n",
    "    # loss_func = LLaVALoss()\n",
    "    loss_func = extract_loss_from_output # Use the extractor function\n",
    "    print(f\"Loss function: {loss_func.__name__}\")\n",
    "    print(f\"Loss function: {type(loss_func).__name__}\")\n",
    "\n",
    "    # 4. Define Optimizer\n",
    "    # AdamW is generally preferred for transformer models\n",
    "    lr = config.get('training', {}).get('learning_rate_stage1', 1e-4)\n",
    "    wd = config.get('training', {}).get('weight_decay', 0.0)\n",
    "    \n",
    "    \n",
    "    opt_func = partial(Adam, lr=lr, wd=wd, eps=1e-8) # Added eps for numerical stability\n",
    "    print(f\"Optimizer: Adam (lr={lr}, wd={wd})\")\n",
    "\n",
    "    # opt_func = AdamW # Pass the optimizer class directly\n",
    "    # print(f\"Optimizer: AdamW (lr will be set by Learner/fit_one_cycle, default wd={wd})\") # \n",
    "    \n",
    "    # 5. Define Splitter\n",
    "    splitter = llava_stage1_splitter\n",
    "    print(f\"Parameter splitter: {splitter.__name__}\")\n",
    "\n",
    "    # 6. Define Callbacks\n",
    "    cbs = []\n",
    "    # Weights & Biases Logging\n",
    "    if config.get('logging', {}).get('wandb', {}).get('enabled', False):\n",
    "        # Initialize W&B run here before creating WandbCallback\n",
    "        project_name = config.get('logging', {}).get('wandb', {}).get('project', 'llava-adaptive-patching')\n",
    "        entity = config.get('logging', {}).get('wandb', {}).get('entity') # Optional\n",
    "        run_name_prefix = config.get('logging', {}).get('wandb', {}).get('run_name_prefix', 'stage1')\n",
    "        \n",
    "        # Create a unique run name (init_wandb will create one if not passed, but we can pre-define)\n",
    "        run_name = f\"{run_name_prefix}_{Path(config['paths']['stage1_projector_weights']).stem}_{wandb.util.generate_id()}\"\n",
    "        \n",
    "        # Init W&B Run\n",
    "        init_wandb(config, job_type=\"stage1-training\", run_name=run_name)\n",
    "        \n",
    "        # Add W&B Callback\n",
    "        cbs.append(WandbCallback(log_preds=False, log_model=False)) # Don't log model via W&B callback, use SaveModelCallback\n",
    "        print(\"Added WandbCallback.\")\n",
    "        \n",
    "    # Model Saving (Only Projector Weights)\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    output_dir.mkdir(parents=True, exist_ok=True) # Ensure output directory exists\n",
    "    projector_weights_fname = Path(config['paths']['stage1_projector_weights']).stem # Get filename without extension\n",
    "    save_cb = SaveModelCallback(\n",
    "        monitor='valid_loss', \n",
    "        min_delta=0.001, # Avoid saving too often for tiny improvements\n",
    "        fname=projector_weights_fname, # Saves as f'{fname}.pth' in learner.path/models/\n",
    "        every_epoch=False, # Save only best based on monitor\n",
    "        with_opt=False, # Don't save optimizer state for projector\n",
    "        reset_on_fit=True # Ensures it checks from the start of training\n",
    "    )\n",
    "    cbs.append(save_cb)\n",
    "    print(f\"Added SaveModelCallback (saves best projector weights based on valid_loss to {output_dir/'models'/f'{projector_weights_fname}.pth'})\")\n",
    "\n",
    "    # --- Add Optimization Callbacks (Step 3.3 Implementation) --- \n",
    "    grad_accum_steps = config.get('training', {}).get('gradient_accumulation_steps', 1)\n",
    "    if grad_accum_steps > 1:\n",
    "        cbs.append(SafeGradientAccumulation(grad_accum_steps))\n",
    "        print(f\"Added SafeGradientAccumulation callback with {grad_accum_steps} steps.\")\n",
    "        \n",
    "        # cbs.append(GradientAccumulation(grad_accum_steps))\n",
    "        # print(f\"Added GradientAccumulation callback with {grad_accum_steps} steps.\")\n",
    "    \n",
    "    use_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)\n",
    "    qlora_enabled = config.get('model', {}).get('quantization', {}).get('load_in_4bit', False)\n",
    "    if use_mixed_precision and not qlora_enabled: # Only add if explicitly enabled AND QLoRA is OFF\n",
    "        cbs.append(LLaVAMixedPrecision()) # Or your current custom MixedPrecision\n",
    "        print(\"Added MixedPrecision callback (QLoRA is disabled).\")\n",
    "    elif use_mixed_precision and qlora_enabled:\n",
    "        print(\"QLoRA is enabled, MixedPrecision callback will be skipped as QLoRA handles its own precision.\")\n",
    "    \n",
    "    # if use_mixed_precision:\n",
    "    #     cbs.append(LLaVAMixedPrecision())\n",
    "    #     print(\"Added MixedPrecision callback.\")\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    # 7. Create Learner\n",
    "    try:\n",
    "        learner = Learner(\n",
    "            dls=dls,\n",
    "            model=model,\n",
    "            loss_func=loss_func,\n",
    "            opt_func=opt_func,\n",
    "            splitter=splitter,\n",
    "            cbs=cbs,\n",
    "            path=output_dir, # Set Learner path for saving models\n",
    "            train_bn=False, # Avoid issues with frozen batch norm layers in LLM/Vision Tower\n",
    "            # wd=wd\n",
    "        )\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Learner: {e}\")\n",
    "        # Clean up wandb run if initialized\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish(exit_code=1)\n",
    "            print(\"Finished W&B run due to error during Learner creation.\")\n",
    "        raise RuntimeError(\"Failed to create the Learner object.\") from e\n",
    "    \n",
    "    print(\"--- Stage 1 Learner Setup Complete ---\")\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_stage1_learner\n",
       "\n",
       ">      get_stage1_learner (config:dict)\n",
       "\n",
       "*Configures and returns a fastai Learner for Stage 1 projector pre-training.\n",
       "   Includes optimization callbacks (GradientAccumulation, MixedPrecision) based on config.\n",
       "\n",
       "Args:\n",
       "    config: The main configuration dictionary.\n",
       "\n",
       "Returns:\n",
       "    A configured fastai Learner instance for Stage 1.\n",
       "\n",
       "Raises:\n",
       "    RuntimeError: If DataLoaders or Model instantiation fails.\n",
       "    FileNotFoundError: If specified data paths in config are incorrect.\n",
       "    AttributeError: If the model is missing expected components (e.g., projector).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_stage1_learner\n",
       "\n",
       ">      get_stage1_learner (config:dict)\n",
       "\n",
       "*Configures and returns a fastai Learner for Stage 1 projector pre-training.\n",
       "   Includes optimization callbacks (GradientAccumulation, MixedPrecision) based on config.\n",
       "\n",
       "Args:\n",
       "    config: The main configuration dictionary.\n",
       "\n",
       "Returns:\n",
       "    A configured fastai Learner instance for Stage 1.\n",
       "\n",
       "Raises:\n",
       "    RuntimeError: If DataLoaders or Model instantiation fails.\n",
       "    FileNotFoundError: If specified data paths in config are incorrect.\n",
       "    AttributeError: If the model is missing expected components (e.g., projector).*"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_stage1_learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Learner Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "--- Setting up Stage 1 Learner ---\n",
      "Loading Stage 1 DataLoaders...\n",
      "Creating Stage 1 DataLoaders with batch size: 2, num_workers: 16\n",
      "Loading Training Stage '1' items from: /workspace/llava/data/llava_pretrain/llava_pretrain.jsonl\n",
      "Assuming image paths relative to: /workspace/llava/data/llava_pretrain/images\n",
      "Found 595375 samples for Training Stage '1'.\n",
      "DataLoaders created successfully.\n",
      "DataLoaders loaded. Train samples: 589422, Valid samples: 5953\n",
      "Instantiating BaselineLLaVAModel...\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n",
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n",
      "QLoRA enabled: Loading LLM in 4-bit with compute dtype torch.float16.\n",
      "  Setting device_map to {'': 0} for QLoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc6abb06e2b490185a68b5c2186de27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Applying LoRA...\n",
      "LoRA applied. Config: r=8, alpha=16, dropout=0.05, modules=['q_proj', 'v_proj']\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n",
      "Resizing LLM token embeddings from 32000 to 32001 (tokenizer size)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM token embeddings resized.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Moving vision_tower to cuda:0\n",
      "Moving projector to cuda:0\n",
      "Language model's first parameter is on device: cuda:0\n",
      "Model instantiated successfully.\n",
      "LLaVALoss initialized, ignoring index: -100\n",
      "Loss function: LLaVALoss\n",
      "Optimizer: AdamW (lr=1e-4, wd=0.0)\n",
      "Parameter splitter: llava_stage1_splitter\n",
      "Added SaveModelCallback (saves best projector weights based on valid_loss to /workspace/llava/output/models/stage1_projector.pth)\n",
      "Added GradientAccumulation callback with 4 steps.\n",
      "Added MixedPrecision callback.\n",
      "--- Stage 1 Learner Setup Complete ---\n",
      "Stage 1 Learner created successfully.\n",
      "Applying Stage 1 splitter: Training only the projector.\n",
      "Checked parameter freezing: 20979712 projector params trainable, 3808122880 other non-projector params frozen.\n",
      "Callback checks passed.\n",
      "\n",
      "Learner summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n",
      "/venv/main/lib/python3.10/site-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learner setup test passed.\n",
      "Cleaned up stage1_learner and model\n"
     ]
    }
   ],
   "source": [
    "#| test \n",
    "import gc\n",
    "\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.training import GradientAccumulation\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "\n",
    "# Add PEFT import block for _peft_available and PeftModel, similar to 20_model_baseline.ipynb\n",
    "try:\n",
    "    from peft import PeftModel # PeftModel is used for isinstance check\n",
    "    _peft_available = True\n",
    "except ImportError:\n",
    "    print(\"Warning: peft library not found in test cell. LoRA-related checks might be affected.\")\n",
    "    PeftModel = None \n",
    "    _peft_available = False\n",
    "\n",
    "\n",
    "try:\n",
    "    # Load config\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # --- Test Setup --- \n",
    "    # Create dummy data files and directories if they don't exist\n",
    "    data_base = Path(config['paths']['data_base'])\n",
    "    stage1_json_rel = Path(config['paths']['stage1_data'])\n",
    "    stage1_img_rel = Path(config['paths']['stage1_images'])\n",
    "    stage1_json_path = data_base / stage1_json_rel\n",
    "    stage1_img_path = data_base / stage1_img_rel\n",
    "    \n",
    "    stage1_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    stage1_img_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not stage1_json_path.exists() or stage1_json_path.stat().st_size < 10: # Check if exists and not empty\n",
    "        print(f\"Creating dummy Stage 1 JSON: {stage1_json_path}\")\n",
    "        # For parse_llava_jsonl which now expects JSON array or JSONL:\n",
    "        # We'll create JSONL for this dummy data as it's simpler line-by-line.\n",
    "        dummy_json_lines_content = [\n",
    "            json.dumps({\"id\": \"s1_001\", \"image\": \"dummy_img1.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\"}, {\"from\": \"gpt\", \"value\": \"Dummy caption 1.\"}]}),\n",
    "            json.dumps({\"id\": \"s1_002\", \"image\": \"dummy_img2.png\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>Describe\"}, {\"from\": \"gpt\", \"value\": \"Dummy caption 2.\"}]}),\n",
    "            json.dumps({\"id\": \"s1_003\", \"image\": \"dummy_img1.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\"}, {\"from\": \"gpt\", \"value\": \"Dummy cap 3.\"}]}),\n",
    "            json.dumps({\"id\": \"s1_004\", \"image\": \"dummy_img2.png\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\"}, {\"from\": \"gpt\", \"value\": \"Dummy cap 4.\"}]})\n",
    "        ]\n",
    "        with open(stage1_json_path, 'w') as f:\n",
    "            for line in dummy_json_lines_content:\n",
    "                f.write(line + '\\n') # Write as JSON Lines\n",
    "    \n",
    "    # Create dummy images (if they don't exist)\n",
    "    try:\n",
    "        img1_path = stage1_img_path / 'dummy_img1.jpg'\n",
    "        img2_path = stage1_img_path / 'dummy_img2.png'\n",
    "        if not img1_path.exists():\n",
    "            PIL.Image.new('RGB', (60, 30), color = 'red').save(img1_path)\n",
    "            print(f\"Created dummy image: {img1_path}\")\n",
    "        if not img2_path.exists():\n",
    "            PIL.Image.new('RGB', (60, 30), color = 'green').save(img2_path)\n",
    "            print(f\"Created dummy image: {img2_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create dummy image files: {e}\")\n",
    "\n",
    "    # Ensure output dir exists\n",
    "    Path(config['paths']['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "    (Path(config['paths']['output_dir']) / 'models').mkdir(parents=True, exist_ok=True) # for SaveModelCallback\n",
    "    # -----------------\n",
    "    \n",
    "    # Check if W&B is enabled and potentially skip if entity isn't set\n",
    "    wandb_enabled_original = config.get('logging', {}).get('wandb', {}).get('enabled', False) # Store original\n",
    "    wandb_entity = config.get('logging', {}).get('wandb', {}).get('entity')\n",
    "    if wandb_enabled_original and (wandb_entity is None or 'your_wandb_entity' in str(wandb_entity)): # Use str for safety\n",
    "        print(\"Warning: W&B is enabled but entity is not set or is default. Disabling W&B for this test.\")\n",
    "        if 'logging' not in config: config['logging'] = {}\n",
    "        if 'wandb' not in config['logging']: config['logging']['wandb'] = {}\n",
    "        config['logging']['wandb']['enabled'] = False\n",
    "\n",
    "\n",
    "    # Create learner\n",
    "    stage1_learner = get_stage1_learner(config)\n",
    "    print(\"Stage 1 Learner created successfully.\")\n",
    "\n",
    "    # Perform basic checks on the learner\n",
    "    assert isinstance(stage1_learner, Learner)\n",
    "    assert isinstance(stage1_learner.dls, DataLoaders)\n",
    "    assert isinstance(stage1_learner.model, BaselineLLaVAModel)\n",
    "    assert isinstance(stage1_learner.loss_func, LLaVALoss)\n",
    "    \n",
    "    # Initialize the optimizer to check param_groups\n",
    "    stage1_learner.create_opt()\n",
    "    assert stage1_learner.opt is not None, \"Optimizer was not created.\"\n",
    "    assert len(stage1_learner.opt.param_groups) == 1 # Only one group (projector) should be trainable\n",
    "    \n",
    "    # Check if only projector parameters are in the trainable group\n",
    "    projector_params_set = set(stage1_learner.model.projector.parameters())\n",
    "    opt_params_set = set(stage1_learner.opt.param_groups[0]['params'])\n",
    "    assert opt_params_set == projector_params_set, \"Optimizer parameter group does not match projector parameters.\"\n",
    "    \n",
    "    # Check parameter freezing status\n",
    "    proj_params_count = 0\n",
    "    frozen_params_count = 0\n",
    "    for name, param in stage1_learner.model.named_parameters():\n",
    "        if 'projector' in name:\n",
    "            assert param.requires_grad == True, f\"Projector parameter {name} is frozen but should be trainable.\"\n",
    "            proj_params_count += param.numel()\n",
    "        elif 'vision_tower' in name or 'language_model' in name:\n",
    "             # If LoRA is applied to language_model, some sub-params might be trainable\n",
    "             # This check assumes base LLM and vision tower are frozen for Stage 1\n",
    "             is_lora_param = False\n",
    "             if _peft_available and PeftModel is not None: # Check if PeftModel is defined\n",
    "                 if 'language_model' in name and isinstance(stage1_learner.model.language_model, PeftModel):\n",
    "                     # A more robust way to check if a param belongs to LoRA is by checking its name or if it requires_grad\n",
    "                     # For Stage 1, the entire LLM (even if PeftModel wrapped) should have its base weights frozen.\n",
    "                     # LoRA adapters themselves require_grad=True, but we are checking requires_grad == False for non-projector parts.\n",
    "                     # This means if LoRA is active (it shouldn't be for Stage 1 logic), this assertion might be tricky.\n",
    "                     # However, `llava_stage1_splitter` explicitly sets requires_grad=False for language_model and vision_tower.\n",
    "                     pass # Let the main assertion handle it.\n",
    "\n",
    "             # This assertion should hold because llava_stage1_splitter freezes vision_tower and language_model\n",
    "             assert param.requires_grad == False, f\"Parameter {name} should be frozen but is not. Its requires_grad is {param.requires_grad}.\"\n",
    "             frozen_params_count += param.numel()\n",
    "\n",
    "\n",
    "    print(f\"Checked parameter freezing: {proj_params_count} projector params trainable, {frozen_params_count} other non-projector params frozen.\")\n",
    "\n",
    "    # Check callbacks\n",
    "    has_save_cb = any(isinstance(cb, SaveModelCallback) for cb in stage1_learner.cbs)\n",
    "    assert has_save_cb, \"SaveModelCallback not found in learner callbacks.\"\n",
    "    \n",
    "    expect_grad_accum = config.get('training', {}).get('gradient_accumulation_steps', 1) > 1\n",
    "    has_grad_accum = any(isinstance(cb, GradientAccumulation) for cb in stage1_learner.cbs)\n",
    "    assert has_grad_accum == expect_grad_accum, f\"GradientAccumulation presence mismatch (Expected: {expect_grad_accum}, Found: {has_grad_accum})\"\n",
    "\n",
    "    expect_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)\n",
    "    has_mixed_precision = any(isinstance(cb, MixedPrecision) for cb in stage1_learner.cbs)\n",
    "    assert has_mixed_precision == expect_mixed_precision, f\"MixedPrecision presence mismatch (Expected: {expect_mixed_precision}, Found: {has_mixed_precision})\"\n",
    "\n",
    "    expect_wandb_cb = config.get('logging', {}).get('wandb', {}).get('enabled', False)\n",
    "    has_wandb_cb = any(isinstance(cb, WandbCallback) for cb in stage1_learner.cbs)\n",
    "    assert has_wandb_cb == expect_wandb_cb, f\"WandbCallback presence mismatch (Expected: {expect_wandb_cb}, Found: {has_wandb_cb})\"\n",
    "    \n",
    "    print(\"Callback checks passed.\")\n",
    "    \n",
    "    print(\"\\nLearner summary:\")\n",
    "    stage1_learner.summary() # This will also create opt if not already created\n",
    "    \n",
    "    print(\"\\nLearner setup test passed.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Skipping test: FileNotFoundError - {e}\")\n",
    "    print(\"Ensure config, data, and model paths are correct and accessible.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Skipping test: ImportError - {e}\")\n",
    "    print(\"Ensure all required libraries (transformers, fastai, etc.) are installed.\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred during learner setup test: {e}\")\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Restore original W&B config if modified for test\n",
    "    if 'wandb_enabled_original' in locals() and 'logging' in config and 'wandb' in config['logging']:\n",
    "        config['logging']['wandb']['enabled'] = wandb_enabled_original\n",
    "\n",
    "    if 'stage1_learner' in locals() and stage1_learner is not None:\n",
    "        if hasattr(stage1_learner, 'model') and stage1_learner.model is not None:\n",
    "            if hasattr(stage1_learner.model, 'vision_tower') and stage1_learner.model.vision_tower is not None:\n",
    "                stage1_learner.model.vision_tower.to('cpu')\n",
    "            if hasattr(stage1_learner.model, 'language_model') and stage1_learner.model.language_model is not None:\n",
    "                stage1_learner.model.language_model.to('cpu')\n",
    "            if hasattr(stage1_learner.model, 'projector') and stage1_learner.model.projector is not None:\n",
    "                stage1_learner.model.projector.to('cpu')\n",
    "            del stage1_learner.model\n",
    "        \n",
    "        if hasattr(stage1_learner, 'opt') and stage1_learner.opt is not None:\n",
    "            stage1_learner.opt.zero_grad() \n",
    "            del stage1_learner.opt\n",
    "            stage1_learner.opt = None\n",
    "\n",
    "        if hasattr(stage1_learner, 'cbs'):\n",
    "            stage1_learner.cbs = [] \n",
    "\n",
    "        del stage1_learner\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Cleaned up stage1_learner and model\")\n",
    "    \n",
    "    if wandb.run is not None:\n",
    "        try:\n",
    "             if wandb.run.id: \n",
    "                  wandb.finish()\n",
    "                  print(\"Finished W&B run.\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error finishing W&B run: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Implement Stage 1 Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the `train_stage1` function that orchestrates the actual training loop using the configured learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_stage1(config_path: str | Path):\n",
    "    \"\"\"Loads config, sets up Stage 1 learner, and runs training.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the YAML configuration file.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Stage 1 Training --- \")\n",
    "    print(f\"Loading configuration from: {config_path}\")\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    learner = None # Initialize learner to None for finally block\n",
    "    try:\n",
    "        # --- Get Learner (including optimization callbacks) --- \n",
    "        learner = get_stage1_learner(config)\n",
    "        \n",
    "        # --- Start Training --- \n",
    "        epochs = config.get('training', {}).get('num_epochs_stage1', 1)\n",
    "        lr = config.get('training', {}).get('learning_rate_stage1', 1e-4)\n",
    "        print(f\"Starting training for {epochs} epochs with max_lr={lr}...\")\n",
    "        \n",
    "        # Use fit_one_cycle (common practice)\n",
    "        # You could also use learner.fit(epochs, lr=lr) or other fine-tuning methods\n",
    "        learner.fit(epochs, lr=lr)\n",
    "        \n",
    "        print(\"Training finished.\")\n",
    "        \n",
    "        # --- Save final projector weights explicitly --- \n",
    "        # SaveModelCallback saves the *best* model during training.\n",
    "        # It might be useful to save the *final* projector state as well.\n",
    "        output_dir = Path(config['paths']['output_dir'])\n",
    "        final_projector_filename = Path(config['paths']['stage1_projector_weights']).stem + \"_final.pth\"\n",
    "        final_save_path = output_dir / 'models' / final_projector_filename \n",
    "        print(f\"Saving final projector state to: {final_save_path}\")\n",
    "        # Save only the projector's state_dict\n",
    "        torch.save(learner.model.projector.state_dict(), final_save_path)\n",
    "        print(\"Final projector weights saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Stage 1 training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Potentially re-raise or handle cleanup\n",
    "        raise e\n",
    "    finally:\n",
    "        # Clean up memory\n",
    "        if learner is not None and hasattr(learner, 'model') and learner.model is not None:\n",
    "            if hasattr(learner.model, 'vision_tower') and learner.model.vision_tower is not None:\n",
    "                learner.model.vision_tower.to('cpu')\n",
    "            if hasattr(learner.model, 'language_model') and learner.model.language_model is not None:\n",
    "                learner.model.language_model.to('cpu')\n",
    "            if hasattr(learner.model, 'projector') and learner.model.projector is not None:\n",
    "                learner.model.projector.to('cpu')\n",
    "            del learner.model\n",
    "            learner.destroy() # Release learner resources\n",
    "            del learner\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Cleaned up learner and model memory.\")\n",
    "            \n",
    "        # Ensure W&B run is finished if it was started\n",
    "        if wandb.run is not None:\n",
    "            try:\n",
    "                if wandb.run.id: # Check if run is still active\n",
    "                    wandb.finish()\n",
    "                    print(\"Finished W&B run.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error finishing W&B run: {e}\")\n",
    "            \n",
    "    print(f\"--- Stage 1 Training Complete --- \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage (Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Add command-line execution block\n",
    "if __name__ == \"__main__\" and \"get_ipython\" not in locals():\n",
    "    parser = argparse.ArgumentParser(description=\"Run LLaVA Stage 1 Training\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"../configs/config.yaml\", \n",
    "                        help=\"Path to the configuration YAML file.\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    config_path = Path(args.config)\n",
    "    if not config_path.is_file():\n",
    "        print(f\"Error: Config file not found at {config_path}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Ensure the config path is absolute or relative to the script's execution dir\n",
    "    # If running from the project root, '../configs/config.yaml' works.\n",
    "    # If running from nbs/, 'configs/config.yaml' might be needed depending on cwd.\n",
    "    # Using absolute paths or paths relative to a known root is safer.\n",
    "    # Assuming execution from project root or script location within project:\n",
    "    if not config_path.exists():\n",
    "         # Try resolving relative to the script file itself if it doesn't exist relative to CWD\n",
    "         script_dir = Path(__file__).parent.resolve()\n",
    "         config_path = (script_dir / args.config).resolve()\n",
    "         if not config_path.exists():\n",
    "              print(f\"Error: Config file not found at specified path or relative to script: {args.config}\")\n",
    "              sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        train_stage1(config_path=config_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Stage 1 training failed: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Stage 1 Training --- \n",
      "Loading configuration from: ../configs/config.yaml\n",
      "--- Setting up Stage 1 Learner ---\n",
      "Loading Stage 1 DataLoaders...\n",
      "Creating Stage 1 DataLoaders with batch size: 2, num_workers: 16\n",
      "Loading Training Stage '1' items from: /workspace/llava/data/llava_pretrain/llava_pretrain.jsonl\n",
      "Assuming image paths relative to: /workspace/llava/data/llava_pretrain/images\n",
      "Found 595375 samples for Training Stage '1'.\n",
      "DataLoaders created successfully.\n",
      "DataLoaders loaded. Train samples: 589422, Valid samples: 5953\n",
      "Instantiating BaselineLLaVAModel...\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n",
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n",
      "QLoRA enabled: Loading LLM in 4-bit with compute dtype torch.float16.\n",
      "  Setting device_map to {'': 0} for QLoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da14a69539454c729e7c099574deeb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Applying LoRA...\n",
      "LoRA applied. Config: r=8, alpha=16, dropout=0.05, modules=['q_proj', 'v_proj']\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n",
      "Resizing LLM token embeddings from 32000 to 32001 (tokenizer size)...\n",
      "LLM token embeddings resized.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Moving vision_tower to cuda:0\n",
      "Moving projector to cuda:0\n",
      "Language model's first parameter is on device: cuda:0\n",
      "Model instantiated successfully.\n",
      "Loss function: extract_loss_from_output\n",
      "Loss function: function\n",
      "Optimizer: Adam (lr=1e-4, wd=0.0)\n",
      "Parameter splitter: llava_stage1_splitter\n",
      "Added SaveModelCallback (saves best projector weights based on valid_loss to /workspace/llava/output/models/stage1_projector.pth)\n",
      "Added SafeGradientAccumulation callback with 16 steps.\n",
      "--- Stage 1 Learner Setup Complete ---\n",
      "Starting training for 1 epochs with max_lr=1e-4...\n",
      "Applying Stage 1 splitter: Ensuring only projector parameters are passed to optimizer.\n",
      "  - Setting projector parameters to require_grad=True.\n",
      "  - Setting vision_tower parameters to require_grad=False.\n",
      "  - Stage 1 Splitter will provide 4 projector parameters to the optimizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='294711' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/294711 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/workspace/llava/llava/model/baseline.py:380: UserWarning: forward() received pixel_values but not input_ids after parsing. Creating dummy text inputs (likely for learner.summary()).\n",
      "  warnings.warn(\"forward() received pixel_values but not input_ids after parsing. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during Stage 1 training: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 23.88 MiB is free. Process 950506 has 180.00 MiB memory in use. Process 1266111 has 180.00 MiB memory in use. Process 1434080 has 7.27 GiB memory in use. Of the allocated memory 6.96 GiB is allocated by PyTorch, and 122.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_7741/314466185.py\", line 24, in train_stage1\n",
      "    learner.fit(epochs, lr=lr)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 272, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 207, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 261, in _do_fit\n",
      "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 207, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 255, in _do_epoch\n",
      "    self._do_epoch_train()\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 247, in _do_epoch_train\n",
      "    self._with_events(self.all_batches, 'train', CancelTrainException)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 207, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 213, in all_batches\n",
      "    for o in enumerate(self.dl): self.one_batch(*o)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 243, in one_batch\n",
      "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 207, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"/venv/main/lib/python3.10/site-packages/fastai/learner.py\", line 224, in _do_one_batch\n",
      "    self.pred = self.model(*self.xb)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/workspace/llava/llava/model/baseline.py\", line 549, in forward\n",
      "    outputs: CausalLMOutputWithPast = self.language_model(\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/peft/peft_model.py\", line 818, in forward\n",
      "    return self.get_base_model()(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 821, in forward\n",
      "    outputs: BaseModelOutputWithPast = self.model(\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 571, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 318, in forward\n",
      "    hidden_states, self_attn_weights = self.self_attn(\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 286, in forward\n",
      "    attn_output = self.o_proj(attn_output)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
      "    return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n",
      "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/venv/main/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n",
      "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
      "  File \"/venv/main/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 1359, in dequantize_4bit\n",
      "    out = torch.empty(quant_state.shape, dtype=quant_state.dtype, device=A.device)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 23.88 MiB is free. Process 950506 has 180.00 MiB memory in use. Process 1266111 has 180.00 MiB memory in use. Process 1434080 has 7.27 GiB memory in use. Of the allocated memory 6.96 GiB is allocated by PyTorch, and 122.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "destroy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m, in \u001b[0;36mtrain_stage1\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Potentially re-raise or handle cleanup\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Clean up memory\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36mtrain_stage1\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Use fit_one_cycle (common practice)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# You could also use learner.fit(epochs, lr=lr) or other fine-tuning methods\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:272\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:261\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:255\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:213\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:243\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastai/learner.py:224\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_do_one_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/llava/llava/model/baseline.py:549\u001b[0m, in \u001b[0;36mBaselineLLaVAModel.forward\u001b[0;34m(self, pixel_values, input_ids, attention_mask, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# Debug print before final model call\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# print(f\"DEBUG: Calling self.language_model. Forward pass inputs:\")\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# print(f\"  - inputs_embeds shape: {padded_input_embeds.shape}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# --- Pass to LLM ---\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m outputs: CausalLMOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadded_input_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadded_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadded_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass potentially None labels\u001b[39;49;00m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    554\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# --- Add Detailed Debug Prints ---\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/peft/peft_model.py:818\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:318\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:286\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 286\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:484\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    482\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:533\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:462\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/bitsandbytes/functional.py:1359\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1359\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m n \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.67 GiB of which 23.88 MiB is free. Process 950506 has 180.00 MiB memory in use. Process 1266111 has 180.00 MiB memory in use. Process 1434080 has 7.27 GiB memory in use. Of the allocated memory 6.96 GiB is allocated by PyTorch, and 122.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| hide\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Example of how to run this from within the notebook (for testing purposes)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Requires dummy data to be set up as in the get_stage1_learner test cell\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_stage1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../configs/config.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use '../' if running from nbs/\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 55\u001b[0m, in \u001b[0;36mtrain_stage1\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m     learner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mprojector\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m learner\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m---> 55\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdestroy\u001b[49m() \u001b[38;5;66;03m# Release learner resources\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m learner\n\u001b[1;32m     57\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/fastcore/basics.py:537\u001b[0m, in \u001b[0;36mGetAttr.__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    535\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(attr,k)\n\u001b[0;32m--> 537\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(k)\n",
      "\u001b[0;31mAttributeError\u001b[0m: destroy"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Example of how to run this from within the notebook (for testing purposes)\n",
    "# Requires dummy data to be set up as in the get_stage1_learner test cell\n",
    "train_stage1('../configs/config.yaml') # Use '../' if running from nbs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
