{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 Training: Projector Pre-training\n",
    "\n",
    "> Sets up and runs the first stage of LLaVA training, focusing on pre-training the projector module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc # For memory cleanup\n",
    "import argparse # For command-line execution\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n",
      "Loaded config from configs/config.yaml\n",
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n",
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n",
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0\n",
      "LLaVADataBlockStage1 defined.\n",
      "LLaVALoss initialized, ignoring index: -100\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "from fastai.learner import Learner\n",
    "from fastai.optimizer import AdamW\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.schedule import fit_one_cycle\n",
    "from fastai.callback.save import SaveModelCallback\n",
    "from fastai.callback.training import GradientAccumulation # Import GradientAccumulation\n",
    "from fastai.callback.fp16 import MixedPrecision # Import MixedPrecision\n",
    "from fastai.vision.all import params # For splitter\n",
    "from fastai.data.core import DataLoaders\n",
    "from functools import partial\n",
    "import wandb # Import wandb directly for cleanup\n",
    "import json # For dummy data creation\n",
    "import PIL.Image # For dummy data creation\n",
    "\n",
    "from llava.utils import load_config, init_wandb\n",
    "from llava.data.loading import get_stage1_dataloaders\n",
    "from llava.model.baseline import BaselineLLaVAModel\n",
    "from llava.training.core import LLaVALoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 Splitter Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a custom splitter function for the fastai `Learner`. In Stage 1, we only want to train the `projector` module, keeping the `vision_tower` and `language_model` frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llava_stage1_splitter(model: BaselineLLaVAModel):\n",
    "    \"\"\"Splits the `BaselineLLaVAModel` parameters for Stage 1 training.\n",
    "    \n",
    "    Only the parameters of the `projector` module are marked as trainable.\n",
    "    The `vision_tower` and `language_model` parameters will remain frozen.\n",
    "    \n",
    "    Args:\n",
    "        model: An instance of `BaselineLLaVAModel`.\n",
    "        \n",
    "    Returns:\n",
    "        A list containing a single parameter group for the projector.\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'projector') or model.projector is None:\n",
    "        raise AttributeError(\"Model does not have a 'projector' attribute or it is None.\")\n",
    "        \n",
    "    print(\"Applying Stage 1 splitter: Training only the projector.\")\n",
    "    # Fastai's `params` function selects parameters from the given module(s)\n",
    "    # Only parameters returned by the splitter are trained.\n",
    "    trainable_params = list(model.projector.parameters())\n",
    "    \n",
    "    # Verify that projector parameters require grad\n",
    "    # They should by default unless explicitly frozen, but good to check\n",
    "    for p in trainable_params:\n",
    "         p.requires_grad = True # Ensure they are trainable\n",
    "            \n",
    "    # Ensure other parts are frozen (should be done during model init, but double-check)\n",
    "    if hasattr(model, 'vision_tower') and model.vision_tower is not None:\n",
    "        for p in model.vision_tower.parameters():\n",
    "            p.requires_grad = False\n",
    "    if hasattr(model, 'language_model') and model.language_model is not None:\n",
    "        # Note: If PEFT is somehow applied here (it shouldn't be for stage 1),\n",
    "        # this would wrongly freeze LoRA adapters. This assumes base LLM is frozen.\n",
    "        for p in model.language_model.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    # Return a list containing one group of trainable parameters (the projector's)\n",
    "    return [trainable_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### llava_stage1_splitter\n",
       "\n",
       ">      llava_stage1_splitter (model:llava.model.baseline.BaselineLLaVAModel)\n",
       "\n",
       "*Splits the `BaselineLLaVAModel` parameters for Stage 1 training.\n",
       "    \n",
       "    Only the parameters of the `projector` module are marked as trainable.\n",
       "    The `vision_tower` and `language_model` parameters will remain frozen.\n",
       "    \n",
       "    Args:\n",
       "        model: An instance of `BaselineLLaVAModel`.\n",
       "        \n",
       "    Returns:\n",
       "        A list containing a single parameter group for the projector.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### llava_stage1_splitter\n",
       "\n",
       ">      llava_stage1_splitter (model:llava.model.baseline.BaselineLLaVAModel)\n",
       "\n",
       "*Splits the `BaselineLLaVAModel` parameters for Stage 1 training.\n",
       "    \n",
       "    Only the parameters of the `projector` module are marked as trainable.\n",
       "    The `vision_tower` and `language_model` parameters will remain frozen.\n",
       "    \n",
       "    Args:\n",
       "        model: An instance of `BaselineLLaVAModel`.\n",
       "        \n",
       "    Returns:\n",
       "        A list containing a single parameter group for the projector.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(llava_stage1_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2 & 3.3: Setup Learner Configuration (Stage 1) with Optimization Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets up the `Learner` object for Stage 1, including the model, data, loss function, optimizer, splitter, and standard callbacks (W&B, SaveModel). It now also includes the `GradientAccumulation` and `MixedPrecision` callbacks based on configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stage1_learner(config: dict) -> Learner:\n",
    "    \"\"\"Configures and returns a fastai Learner for Stage 1 projector pre-training.\n",
    "       Includes optimization callbacks (GradientAccumulation, MixedPrecision) based on config.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A configured fastai Learner instance for Stage 1.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If DataLoaders or Model instantiation fails.\n",
    "        FileNotFoundError: If specified data paths in config are incorrect.\n",
    "        AttributeError: If the model is missing expected components (e.g., projector).\n",
    "    \"\"\"\n",
    "    print(\"--- Setting up Stage 1 Learner ---\")\n",
    "    \n",
    "    # 1. Load DataLoaders\n",
    "    print(\"Loading Stage 1 DataLoaders...\")\n",
    "    try:\n",
    "        dls = get_stage1_dataloaders(config)\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        print(f\"Error loading DataLoaders: {e}\")\n",
    "        raise RuntimeError(\"Failed to create Stage 1 DataLoaders. Check config paths and data availability.\") from e\n",
    "    if not dls:\n",
    "        raise RuntimeError(\"Stage 1 DataLoaders object is None.\")\n",
    "    print(f\"DataLoaders loaded. Train samples: {len(dls.train_ds)}, Valid samples: {len(dls.valid_ds)}\")\n",
    "\n",
    "    # 2. Instantiate Model\n",
    "    print(\"Instantiating BaselineLLaVAModel...\")\n",
    "    try:\n",
    "        model = BaselineLLaVAModel(config)\n",
    "        # Ensure model components loaded successfully\n",
    "        if model.vision_tower is None or model.language_model is None or model.projector is None:\n",
    "             raise RuntimeError(\"BaselineLLaVAModel initialization failed: one or more components are None.\")\n",
    "        print(\"Model instantiated successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error instantiating BaselineLLaVAModel: {e}\")\n",
    "        raise RuntimeError(\"Failed to instantiate baseline model.\") from e\n",
    "\n",
    "    # 3. Define Loss Function\n",
    "    loss_func = LLaVALoss()\n",
    "    print(f\"Loss function: {type(loss_func).__name__}\")\n",
    "\n",
    "    # 4. Define Optimizer\n",
    "    # AdamW is generally preferred for transformer models\n",
    "    lr = config.get('training', {}).get('learning_rate_stage1', 1e-4)\n",
    "    wd = config.get('training', {}).get('weight_decay', 0.0)\n",
    "    opt_func = partial(AdamW, lr=lr, wd=wd, eps=1e-8) # Added eps for numerical stability\n",
    "    print(f\"Optimizer: AdamW (lr={lr}, wd={wd})\")\n",
    "\n",
    "    # 5. Define Splitter\n",
    "    splitter = llava_stage1_splitter\n",
    "    print(f\"Parameter splitter: {splitter.__name__}\")\n",
    "\n",
    "    # 6. Define Callbacks\n",
    "    cbs = []\n",
    "    # Weights & Biases Logging\n",
    "    if config.get('logging', {}).get('wandb', {}).get('enabled', False):\n",
    "        # Initialize W&B run here before creating WandbCallback\n",
    "        project_name = config.get('logging', {}).get('wandb', {}).get('project', 'llava-adaptive-patching')\n",
    "        entity = config.get('logging', {}).get('wandb', {}).get('entity') # Optional\n",
    "        run_name_prefix = config.get('logging', {}).get('wandb', {}).get('run_name_prefix', 'stage1')\n",
    "        \n",
    "        # Create a unique run name (init_wandb will create one if not passed, but we can pre-define)\n",
    "        run_name = f\"{run_name_prefix}_{Path(config['paths']['stage1_projector_weights']).stem}_{wandb.util.generate_id()}\"\n",
    "        \n",
    "        # Init W&B Run\n",
    "        init_wandb(config, job_type=\"stage1-training\", run_name=run_name)\n",
    "        \n",
    "        # Add W&B Callback\n",
    "        cbs.append(WandbCallback(log_preds=False, log_model=False)) # Don't log model via W&B callback, use SaveModelCallback\n",
    "        print(\"Added WandbCallback.\")\n",
    "        \n",
    "    # Model Saving (Only Projector Weights)\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    output_dir.mkdir(parents=True, exist_ok=True) # Ensure output directory exists\n",
    "    projector_weights_fname = Path(config['paths']['stage1_projector_weights']).stem # Get filename without extension\n",
    "    save_cb = SaveModelCallback(\n",
    "        monitor='valid_loss', \n",
    "        min_delta=0.001, # Avoid saving too often for tiny improvements\n",
    "        fname=projector_weights_fname, # Saves as f'{fname}.pth' in learner.path/models/\n",
    "        every_epoch=False, # Save only best based on monitor\n",
    "        with_opt=False, # Don't save optimizer state for projector\n",
    "        reset_on_fit=True # Ensures it checks from the start of training\n",
    "    )\n",
    "    cbs.append(save_cb)\n",
    "    print(f\"Added SaveModelCallback (saves best projector weights based on valid_loss to {output_dir/'models'/f'{projector_weights_fname}.pth'})\")\n",
    "\n",
    "    # --- Add Optimization Callbacks (Step 3.3 Implementation) --- \n",
    "    grad_accum_steps = config.get('training', {}).get('gradient_accumulation_steps', 1)\n",
    "    if grad_accum_steps > 1:\n",
    "        cbs.append(GradientAccumulation(grad_accum_steps))\n",
    "        print(f\"Added GradientAccumulation callback with {grad_accum_steps} steps.\")\n",
    "    \n",
    "    use_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)\n",
    "    if use_mixed_precision:\n",
    "        cbs.append(MixedPrecision())\n",
    "        print(\"Added MixedPrecision callback.\")\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    # 7. Create Learner\n",
    "    try:\n",
    "        learner = Learner(\n",
    "            dls=dls,\n",
    "            model=model,\n",
    "            loss_func=loss_func,\n",
    "            opt_func=opt_func,\n",
    "            splitter=splitter,\n",
    "            cbs=cbs,\n",
    "            path=output_dir, # Set Learner path for saving models\n",
    "            train_bn=False # Avoid issues with frozen batch norm layers in LLM/Vision Tower\n",
    "        )\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Learner: {e}\")\n",
    "        # Clean up wandb run if initialized\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish(exit_code=1)\n",
    "            print(\"Finished W&B run due to error during Learner creation.\")\n",
    "        raise RuntimeError(\"Failed to create the Learner object.\") from e\n",
    "    \n",
    "    print(\"--- Stage 1 Learner Setup Complete ---\")\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_stage1_learner\n",
       "\n",
       ">      get_stage1_learner (config:dict)\n",
       "\n",
       "*Configures and returns a fastai Learner for Stage 1 projector pre-training.\n",
       "       Includes optimization callbacks (GradientAccumulation, MixedPrecision) based on config.\n",
       "\n",
       "    Args:\n",
       "        config: The main configuration dictionary.\n",
       "\n",
       "    Returns:\n",
       "        A configured fastai Learner instance for Stage 1.\n",
       "\n",
       "    Raises:\n",
       "        RuntimeError: If DataLoaders or Model instantiation fails.\n",
       "        FileNotFoundError: If specified data paths in config are incorrect.\n",
       "        AttributeError: If the model is missing expected components (e.g., projector).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_stage1_learner\n",
       "\n",
       ">      get_stage1_learner (config:dict)\n",
       "\n",
       "*Configures and returns a fastai Learner for Stage 1 projector pre-training.\n",
       "       Includes optimization callbacks (GradientAccumulation, MixedPrecision) based on config.\n",
       "\n",
       "    Args:\n",
       "        config: The main configuration dictionary.\n",
       "\n",
       "    Returns:\n",
       "        A configured fastai Learner instance for Stage 1.\n",
       "\n",
       "    Raises:\n",
       "        RuntimeError: If DataLoaders or Model instantiation fails.\n",
       "        FileNotFoundError: If specified data paths in config are incorrect.\n",
       "        AttributeError: If the model is missing expected components (e.g., projector).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_stage1_learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Learner Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Creating dummy Stage 1 JSON: /workspace/llava/data/llava_pretrain/llava_pretrain.jsonl\n",
      "Creating dummy image: /workspace/llava/data/llava_pretrain/images/dummy_img1.jpg\n",
      "Creating dummy image: /workspace/llava/data/llava_pretrain/images/dummy_img2.png\n",
      "--- Setting up Stage 1 Learner ---\n",
      "Loading Stage 1 DataLoaders...\n",
      "Creating Stage 1 DataLoaders with batch size: 8, num_workers: 4\n",
      "Loading Stage 1 items from: /workspace/llava/data/llava_pretrain/llava_pretrain.jsonl\n",
      "Assuming images relative to: /workspace/llava/data/llava_pretrain/images\n",
      "Found 4 samples for Stage 1.\n",
      "DataLoaders created successfully.\n",
      "DataLoaders loaded. Train samples: 3, Valid samples: 1\n",
      "Instantiating BaselineLLaVAModel...\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1272b8a703c9443ab9c9d312066b9df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Language Model weights frozen (will be partially unfrozen by PEFT if enabled).\n",
      "Resizing LLM token embeddings from 32000 to 32001 (tokenizer size)...\n",
      "LLM token embeddings resized.\n",
      "Model instantiated successfully.\n",
      "Loss function: LLaVALoss\n",
      "Optimizer: AdamW (lr=0.0001, wd=0.0)\n",
      "Parameter splitter: llava_stage1_splitter\n",
      "Warning: W&B is enabled but entity is not set or is default. Disabling W&B for this test.\n",
      "Added SaveModelCallback (saves best projector weights based on valid_loss to /workspace/llava/output/models/stage1_projector.pth)\n",
      "Added GradientAccumulation callback with 4 steps.\n",
      "Added MixedPrecision callback.\n",
      "Applying Stage 1 splitter: Training only the projector.\n",
      "--- Stage 1 Learner Setup Complete ---\n",
      "Stage 1 Learner created successfully.\n",
      "Checked parameter freezing: 33554432 projector params trainable, 6891018240 other params frozen.\n",
      "Callback checks passed.\n",
      "\n",
      "Learner summary:\n",
      "Applying Stage 1 splitter: Training only the projector.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:125: UserWarning: Detected NVIDIA GPUs with compute capability less than 7.0. It is recommended to use FP32 Promotes type casting for old GPUs. You can set the `promotes_type` argument of the frontend API (`torch.autocast`) to FP32, or set `TORCH_AMP_PROMOTE_TYPE=FP32` environment variable to enable it. This warning will only appear once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learner setup test passed.\n",
      "Cleaned up stage1_learner and model\n"
     ]
    }
   ],
   "source": [
    "#| test \n",
    "import gc\n",
    "from fastai.callback.save import SaveModelCallback # Need to import for isinstance check\n",
    "from fastai.callback.wandb import WandbCallback # Need to import for isinstance check\n",
    "from fastai.callback.training import GradientAccumulation # Import GradientAccumulation\n",
    "from fastai.callback.fp16 import MixedPrecision # Import MixedPrecision\n",
    "\n",
    "\n",
    "try:\n",
    "    # Load config\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # --- Test Setup --- \n",
    "    # Create dummy data files and directories if they don't exist\n",
    "    data_base = Path(config['paths']['data_base'])\n",
    "    stage1_json_rel = Path(config['paths']['stage1_data'])\n",
    "    stage1_img_rel = Path(config['paths']['stage1_images'])\n",
    "    stage1_json_path = data_base / stage1_json_rel\n",
    "    stage1_img_path = data_base / stage1_img_rel\n",
    "    \n",
    "    stage1_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    stage1_img_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not stage1_json_path.exists() or stage1_json_path.stat().st_size < 10: # Check if exists and not empty\n",
    "        print(f\"Creating dummy Stage 1 JSON: {stage1_json_path}\")\n",
    "        dummy_json_content = [\n",
    "            {\"id\": \"s1_001\", \"image\": \"dummy_img1.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\"}, {\"from\": \"gpt\", \"value\": \"Dummy caption 1.\"}]},\n",
    "            {\"id\": \"s1_002\", \"image\": \"dummy_img2.png\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>Describe\"}, {\"from\": \"gpt\", \"value\": \"Dummy caption 2.\"}]},\n",
    "             # Add more samples for train/valid split\n",
    "            {\"id\": \"s1_003\", \"image\": \"dummy_img1.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\"}, {\"from\": \"gpt\", \"value\": \"Dummy cap 3.\"}]},\n",
    "            {\"id\": \"s1_004\", \"image\": \"dummy_img2.png\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\"}, {\"from\": \"gpt\", \"value\": \"Dummy cap 4.\"}]}\n",
    "        ]\n",
    "        with open(stage1_json_path, 'w') as f:\n",
    "            # Write as a JSON array, not JSON Lines, matching parse_llava_json expectation\n",
    "            json.dump(dummy_json_content, f)\n",
    "    \n",
    "    # Create dummy images (if they don't exist)\n",
    "    try:\n",
    "        img1_path = stage1_img_path / 'dummy_img1.jpg'\n",
    "        img2_path = stage1_img_path / 'dummy_img2.png'\n",
    "        if not img1_path.exists():\n",
    "            PIL.Image.new('RGB', (60, 30), color = 'red').save(img1_path)\n",
    "            print(f\"Created dummy image: {img1_path}\")\n",
    "        if not img2_path.exists():\n",
    "            PIL.Image.new('RGB', (60, 30), color = 'green').save(img2_path)\n",
    "            print(f\"Created dummy image: {img2_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create dummy image files: {e}\")\n",
    "\n",
    "    # Ensure output dir exists\n",
    "    Path(config['paths']['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "    # -----------------\n",
    "    \n",
    "    # Check if W&B is enabled and potentially skip if entity isn't set\n",
    "    wandb_enabled = config.get('logging', {}).get('wandb', {}).get('enabled', False)\n",
    "    wandb_entity = config.get('logging', {}).get('wandb', {}).get('entity')\n",
    "    if wandb_enabled and (wandb_entity is None or 'your_wandb_entity' in wandb_entity):\n",
    "        print(\"Warning: W&B is enabled but entity is not set or is default. Disabling W&B for this test.\")\n",
    "        config['logging']['wandb']['enabled'] = False\n",
    "\n",
    "    # Create learner\n",
    "    stage1_learner = get_stage1_learner(config)\n",
    "    print(\"Stage 1 Learner created successfully.\")\n",
    "\n",
    "    # Perform basic checks on the learner\n",
    "    assert isinstance(stage1_learner, Learner)\n",
    "    assert isinstance(stage1_learner.dls, DataLoaders)\n",
    "    assert isinstance(stage1_learner.model, BaselineLLaVAModel)\n",
    "    assert isinstance(stage1_learner.loss_func, LLaVALoss)\n",
    "    assert len(stage1_learner.opt.param_groups) == 1 # Only one group (projector) should be trainable\n",
    "    \n",
    "    # Check if only projector parameters are in the trainable group\n",
    "    projector_params_set = set(stage1_learner.model.projector.parameters())\n",
    "    opt_params_set = set(stage1_learner.opt.param_groups[0]['params'])\n",
    "    assert opt_params_set == projector_params_set, \"Optimizer parameter group does not match projector parameters.\"\n",
    "    \n",
    "    # Check parameter freezing status\n",
    "    proj_params_count = 0\n",
    "    frozen_params_count = 0\n",
    "    for name, param in stage1_learner.model.named_parameters():\n",
    "        if 'projector' in name:\n",
    "            assert param.requires_grad == True, f\"Projector parameter {name} is frozen but should be trainable.\"\n",
    "            proj_params_count += param.numel()\n",
    "        elif 'vision_tower' in name or 'language_model' in name:\n",
    "             assert param.requires_grad == False, f\"Parameter {name} should be frozen but is not.\"\n",
    "             frozen_params_count += param.numel()\n",
    "    print(f\"Checked parameter freezing: {proj_params_count} projector params trainable, {frozen_params_count} other params frozen.\")\n",
    "\n",
    "    # Check callbacks\n",
    "    has_save_cb = any(isinstance(cb, SaveModelCallback) for cb in stage1_learner.cbs)\n",
    "    assert has_save_cb, \"SaveModelCallback not found in learner callbacks.\"\n",
    "    \n",
    "    # Check for optimization callbacks based on config\n",
    "    expect_grad_accum = config.get('training', {}).get('gradient_accumulation_steps', 1) > 1\n",
    "    has_grad_accum = any(isinstance(cb, GradientAccumulation) for cb in stage1_learner.cbs)\n",
    "    assert has_grad_accum == expect_grad_accum, f\"GradientAccumulation presence mismatch (Expected: {expect_grad_accum}, Found: {has_grad_accum})\"\n",
    "\n",
    "    expect_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)\n",
    "    has_mixed_precision = any(isinstance(cb, MixedPrecision) for cb in stage1_learner.cbs)\n",
    "    assert has_mixed_precision == expect_mixed_precision, f\"MixedPrecision presence mismatch (Expected: {expect_mixed_precision}, Found: {has_mixed_precision})\"\n",
    "\n",
    "    # Check WandbCallback presence based on updated config\n",
    "    expect_wandb_cb = config.get('logging', {}).get('wandb', {}).get('enabled', False)\n",
    "    has_wandb_cb = any(isinstance(cb, WandbCallback) for cb in stage1_learner.cbs)\n",
    "    assert has_wandb_cb == expect_wandb_cb, f\"WandbCallback presence mismatch (Expected: {expect_wandb_cb}, Found: {has_wandb_cb})\"\n",
    "    \n",
    "    print(\"Callback checks passed.\")\n",
    "    \n",
    "    print(\"\\nLearner summary:\")\n",
    "    stage1_learner.summary()\n",
    "    \n",
    "    print(\"\\nLearner setup test passed.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Skipping test: FileNotFoundError - {e}\")\n",
    "    print(\"Ensure config, data, and model paths are correct and accessible.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Skipping test: ImportError - {e}\")\n",
    "    print(\"Ensure all required libraries (transformers, fastai, etc.) are installed.\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred during learner setup test: {e}\")\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Clean up memory\n",
    "    if 'stage1_learner' in locals() and stage1_learner is not None:\n",
    "        if hasattr(stage1_learner, 'model') and stage1_learner.model is not None:\n",
    "            # Move components to CPU before deleting\n",
    "            if hasattr(stage1_learner.model, 'vision_tower') and stage1_learner.model.vision_tower is not None:\n",
    "                stage1_learner.model.vision_tower.to('cpu')\n",
    "            if hasattr(stage1_learner.model, 'language_model') and stage1_learner.model.language_model is not None:\n",
    "                stage1_learner.model.language_model.to('cpu')\n",
    "            if hasattr(stage1_learner.model, 'projector') and stage1_learner.model.projector is not None:\n",
    "                stage1_learner.model.projector.to('cpu')\n",
    "            del stage1_learner.model\n",
    "        stage1_learner.destroy() # Clean up learner properly\n",
    "        del stage1_learner\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Cleaned up stage1_learner and model\")\n",
    "    # Terminate wandb run if it was initialized and not already finished\n",
    "    if wandb.run is not None:\n",
    "        # Check if the run is still active before finishing\n",
    "        try:\n",
    "             if wandb.run.id:\n",
    "                  wandb.finish()\n",
    "                  print(\"Finished W&B run.\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error finishing W&B run: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Implement Stage 1 Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the `train_stage1` function that orchestrates the actual training loop using the configured learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_stage1(config_path: str | Path):\n",
    "    \"\"\"Loads config, sets up Stage 1 learner, and runs training.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the YAML configuration file.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Stage 1 Training --- \")\n",
    "    print(f\"Loading configuration from: {config_path}\")\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    learner = None # Initialize learner to None for finally block\n",
    "    try:\n",
    "        # --- Get Learner (including optimization callbacks) --- \n",
    "        learner = get_stage1_learner(config)\n",
    "        \n",
    "        # --- Start Training --- \n",
    "        epochs = config.get('training', {}).get('num_epochs_stage1', 1)\n",
    "        lr = config.get('training', {}).get('learning_rate_stage1', 1e-4)\n",
    "        print(f\"Starting training for {epochs} epochs with max_lr={lr}...\")\n",
    "        \n",
    "        # Use fit_one_cycle (common practice)\n",
    "        # You could also use learner.fit(epochs, lr=lr) or other fine-tuning methods\n",
    "        learner.fit_one_cycle(epochs, lr_max=lr)\n",
    "        \n",
    "        print(\"Training finished.\")\n",
    "        \n",
    "        # --- Save final projector weights explicitly --- \n",
    "        # SaveModelCallback saves the *best* model during training.\n",
    "        # It might be useful to save the *final* projector state as well.\n",
    "        output_dir = Path(config['paths']['output_dir'])\n",
    "        final_projector_filename = Path(config['paths']['stage1_projector_weights']).stem + \"_final.pth\"\n",
    "        final_save_path = output_dir / 'models' / final_projector_filename \n",
    "        print(f\"Saving final projector state to: {final_save_path}\")\n",
    "        # Save only the projector's state_dict\n",
    "        torch.save(learner.model.projector.state_dict(), final_save_path)\n",
    "        print(\"Final projector weights saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Stage 1 training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Potentially re-raise or handle cleanup\n",
    "        raise e\n",
    "    finally:\n",
    "        # Clean up memory\n",
    "        if learner is not None and hasattr(learner, 'model') and learner.model is not None:\n",
    "            if hasattr(learner.model, 'vision_tower') and learner.model.vision_tower is not None:\n",
    "                learner.model.vision_tower.to('cpu')\n",
    "            if hasattr(learner.model, 'language_model') and learner.model.language_model is not None:\n",
    "                learner.model.language_model.to('cpu')\n",
    "            if hasattr(learner.model, 'projector') and learner.model.projector is not None:\n",
    "                learner.model.projector.to('cpu')\n",
    "            del learner.model\n",
    "            learner.destroy() # Release learner resources\n",
    "            del learner\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Cleaned up learner and model memory.\")\n",
    "            \n",
    "        # Ensure W&B run is finished if it was started\n",
    "        if wandb.run is not None:\n",
    "            try:\n",
    "                if wandb.run.id: # Check if run is still active\n",
    "                    wandb.finish()\n",
    "                    print(\"Finished W&B run.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error finishing W&B run: {e}\")\n",
    "            \n",
    "    print(f\"--- Stage 1 Training Complete --- \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage (Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Add command-line execution block\n",
    "if __name__ == \"__main__\" and \"get_ipython\" not in locals():\n",
    "    parser = argparse.ArgumentParser(description=\"Run LLaVA Stage 1 Training\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"../configs/config.yaml\", \n",
    "                        help=\"Path to the configuration YAML file.\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    config_path = Path(args.config)\n",
    "    if not config_path.is_file():\n",
    "        print(f\"Error: Config file not found at {config_path}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Ensure the config path is absolute or relative to the script's execution dir\n",
    "    # If running from the project root, '../configs/config.yaml' works.\n",
    "    # If running from nbs/, 'configs/config.yaml' might be needed depending on cwd.\n",
    "    # Using absolute paths or paths relative to a known root is safer.\n",
    "    # Assuming execution from project root or script location within project:\n",
    "    if not config_path.exists():\n",
    "         # Try resolving relative to the script file itself if it doesn't exist relative to CWD\n",
    "         script_dir = Path(__file__).parent.resolve()\n",
    "         config_path = (script_dir / args.config).resolve()\n",
    "         if not config_path.exists():\n",
    "              print(f\"Error: Config file not found at specified path or relative to script: {args.config}\")\n",
    "              sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        train_stage1(config_path=config_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Stage 1 training failed: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Example of how to run this from within the notebook (for testing purposes)\n",
    "# Requires dummy data to be set up as in the get_stage1_learner test cell\n",
    "# train_stage1('../configs/config.yaml') # Use '../' if running from nbs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
