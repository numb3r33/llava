{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 Training: Projector Pre-training\n",
    "\n",
    "> Sets up and runs the first stage of LLaVA training, focusing on pre-training the projector module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from fastai.learner import Learner\n",
    "from fastai.optimizer import AdamW\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.schedule import fit_one_cycle\n",
    "from fastai.callback.save import SaveModelCallback\n",
    "from fastai.vision.all import params # For splitter\n",
    "from fastai.data.core import DataLoaders\n",
    "\n",
    "from llava.utils import load_config, init_wandb\n",
    "from llava.data.loading import get_stage1_dataloaders\n",
    "from llava.model.baseline import BaselineLLaVAModel\n",
    "from llava.training.core import LLaVALoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 Splitter Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a custom splitter function for the fastai `Learner`. In Stage 1, we only want to train the `projector` module, keeping the `vision_tower` and `language_model` frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llava_stage1_splitter(model: BaselineLLaVAModel):\n",
    "    \"\"\"Splits the `BaselineLLaVAModel` parameters for Stage 1 training.\n",
    "    \n",
    "    Only the parameters of the `projector` module are marked as trainable.\n",
    "    The `vision_tower` and `language_model` parameters will remain frozen.\n",
    "    \n",
    "    Args:\n",
    "        model: An instance of `BaselineLLaVAModel`.\n",
    "        \n",
    "    Returns:\n",
    "        A list containing a single parameter group for the projector.\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'projector') or model.projector is None:\n",
    "        raise AttributeError(\"Model does not have a 'projector' attribute or it is None.\")\n",
    "        \n",
    "    print(\"Applying Stage 1 splitter: Training only the projector.\")\n",
    "    # Fastai's `params` function selects parameters from the given module(s)\n",
    "    # Only parameters returned by the splitter are trained.\n",
    "    trainable_params = list(model.projector.parameters())\n",
    "    \n",
    "    # Verify that projector parameters require grad\n",
    "    # They should by default unless explicitly frozen, but good to check\n",
    "    for p in trainable_params:\n",
    "         p.requires_grad = True # Ensure they are trainable\n",
    "            \n",
    "    # Ensure other parts are frozen (should be done during model init, but double-check)\n",
    "    if hasattr(model, 'vision_tower') and model.vision_tower is not None:\n",
    "        for p in model.vision_tower.parameters():\n",
    "            p.requires_grad = False\n",
    "    if hasattr(model, 'language_model') and model.language_model is not None:\n",
    "        # Note: If PEFT is somehow applied here (it shouldn't be for stage 1),\n",
    "        # this would wrongly freeze LoRA adapters. This assumes base LLM is frozen.\n",
    "        for p in model.language_model.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    return [trainable_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Setup Learner Configuration (Stage 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stage1_learner(config: dict) -> Learner:\n",
    "    \"\"\"Configures and returns a fastai Learner for Stage 1 projector pre-training.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A configured fastai Learner instance for Stage 1.\n",
    "    \"\"\"\n",
    "    print(\"--- Setting up Stage 1 Learner ---\")\n",
    "    # 1. Load DataLoaders\n",
    "    print(\"Loading Stage 1 DataLoaders...\")\n",
    "    dls = get_stage1_dataloaders(config)\n",
    "    if not dls:\n",
    "        raise RuntimeError(\"Failed to create Stage 1 DataLoaders.\")\n",
    "    print(f\"DataLoaders loaded. Train samples: {len(dls.train_ds)}, Valid samples: {len(dls.valid_ds)}\")\n",
    "\n",
    "    # 2. Instantiate Model\n",
    "    print(\"Instantiating BaselineLLaVAModel...\")\n",
    "    model = BaselineLLaVAModel(config)\n",
    "    print(\"Model instantiated.\")\n",
    "\n",
    "    # 3. Define Loss Function\n",
    "    loss_func = LLaVALoss()\n",
    "    print(f\"Loss function: {type(loss_func).__name__}\")\n",
    "\n",
    "    # 4. Define Optimizer\n",
    "    # AdamW is generally preferred for transformer models\n",
    "    lr = config.get('training', {}).get('learning_rate_stage1', 1e-4)\n",
    "    wd = config.get('training', {}).get('weight_decay', 0.0)\n",
    "    opt_func = partial(AdamW, lr=lr, wd=wd, eps=1e-8)\n",
    "    print(f\"Optimizer: AdamW (lr={lr}, wd={wd})\")\n",
    "\n",
    "    # 5. Define Splitter\n",
    "    splitter = llava_stage1_splitter\n",
    "    print(f\"Parameter splitter: {splitter.__name__}\")\n",
    "\n",
    "    # 6. Define Callbacks\n",
    "    cbs = []\n",
    "    # Weights & Biases Logging\n",
    "    if config.get('logging', {}).get('wandb', {}).get('enabled', False):\n",
    "        # Initialize W&B run here before creating WandbCallback\n",
    "        run_name = f\"stage1_{config.get('project_name', 'llava')}_{Path(config['paths']['stage1_projector_weights']).stem}\"\n",
    "        init_wandb(config, job_type=\"stage1-training\", run_name=run_name)\n",
    "        cbs.append(WandbCallback(log_preds=False, log_model=False)) # Don't log model via W&B callback, use SaveModelCallback\n",
    "        print(\"Added WandbCallback.\")\n",
    "        \n",
    "    # Model Saving\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    projector_weights_fname = Path(config['paths']['stage1_projector_weights']).stem # Get filename without extension\n",
    "    save_cb = SaveModelCallback(\n",
    "        monitor='valid_loss', \n",
    "        min_delta=0.001, # Avoid saving too often for tiny improvements\n",
    "        fname=projector_weights_fname, # Saves as f'{fname}.pth'\n",
    "        every_epoch=True, # Save at the end of every epoch regardless of improvement\n",
    "        with_opt=False, # Don't save optimizer state for projector\n",
    "        reset_on_fit=True # Ensures it checks from the start of training\n",
    "    )\n",
    "    cbs.append(save_cb)\n",
    "    print(f\"Added SaveModelCallback (saves projector weights to {output_dir / f'{projector_weights_fname}.pth'})\")\n",
    "\n",
    "    # --- Add Optimization Callbacks (Placeholder for Step 3.3) ---\n",
    "    # Example placeholders, will be implemented in the next step\n",
    "    # grad_accum_steps = config.get('training', {}).get('gradient_accumulation_steps', 1)\n",
    "    # if grad_accum_steps > 1:\n",
    "    #     from fastai.callback.training import GradientAccumulation\n",
    "    #     cbs.append(GradientAccumulation(grad_accum_steps))\n",
    "    #     print(f\"Added GradientAccumulation callback (steps={grad_accum_steps})\")\n",
    "    \n",
    "    # use_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)\n",
    "    # if use_mixed_precision:\n",
    "    #     from fastai.callback.fp16 import MixedPrecision\n",
    "    #     cbs.append(MixedPrecision())\n",
    "    #     print(\"Added MixedPrecision callback.\")\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "    # 7. Create Learner\n",
    "    learner = Learner(\n",
    "        dls=dls,\n",
    "        model=model,\n",
    "        loss_func=loss_func,\n",
    "        opt_func=opt_func,\n",
    "        splitter=splitter,\n",
    "        cbs=cbs,\n",
    "        path=output_dir # Set Learner path for saving models\n",
    "    )\n",
    "    \n",
    "    print(\"--- Stage 1 Learner Setup Complete ---\")\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Learner Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "--- Setting up Stage 1 Learner ---\n",
      "Loading Stage 1 DataLoaders...\n",
      "Creating Stage 1 DataLoaders with batch size: 8, num_workers: 4\n",
      "Loading Stage 1 items from: /workspace/llava/data/llava_pretrain/llava_pretrain.jsonl\n",
      "Assuming images relative to: /workspace/llava/data/llava_pretrain/images\n",
      "Found 595375 samples for Stage 1.\n",
      "DataLoaders created successfully.\n",
      "DataLoaders loaded. Train samples: 589422, Valid samples: 5953\n",
      "Instantiating BaselineLLaVAModel...\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4451ab773b4f45912055e69ef43310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Language Model weights frozen (will be partially unfrozen by PEFT if enabled).\n",
      "Resizing LLM token embeddings from 32000 to 32001 (tokenizer size)...\n",
      "LLM token embeddings resized.\n",
      "Model instantiated.\n",
      "LLaVALoss initialized, ignoring index: -100\n",
      "Loss function: LLaVALoss\n",
      "Optimizer: AdamW (lr=0.0001, wd=0.0)\n",
      "Parameter splitter: llava_stage1_splitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnumb3r33\u001b[0m (team: \u001b[33mnumb3r33\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run initialized: stage1_llava_stage1_projector (Project: adaptive_patching_vit, Entity: your_wandb_entity)\n",
      "Track run at: https://wandb.ai/your_wandb_entity/adaptive_patching_vit/runs/19p7s2b4\n",
      "Added WandbCallback.\n",
      "Added SaveModelCallback (saves projector weights to /workspace/llava/output/stage1_projector.pth)\n",
      "Applying Stage 1 splitter: Training only the projector.\n",
      "--- Stage 1 Learner Setup Complete ---\n",
      "Stage 1 Learner created successfully.\n",
      "Learner summary:\n",
      "BaselineLLaVAModel(\n",
      "  (projector): LLaVAProjector(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): GELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (language_model): LlamaForCausalLM(\n",
      "    (model): LlamaModel(\n",
      "      (embed_tokens): Embedding(32001, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x LlamaDecoderLayer(\n",
      "          (self_attn): LlamaSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm()\n",
      "          (post_attention_layernorm): LlamaRMSNorm()\n",
      "        )\n",
      "      )\n",
      "      (norm): LlamaRMSNorm()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 18878464\n",
      "Non-trainable parameters: 7222987776\n",
      "Total parameters: 7241866240\n",
      "\n",
      "Learner setup test passed.\n",
      "Cleaned up stage1_learner and model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wandb:                                                                                \n",
       "wandb: Run `wandb offline` to turn off syncing."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "#| test \n",
    "import gc\n",
    "try:\n",
    "    # Load config\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # Check if W&B is enabled and potentially skip if entity isn't set\n",
    "    wandb_enabled = config.get('logging', {}).get('wandb', {}).get('enabled', False)\n",
    "    wandb_entity = config.get('logging', {}).get('wandb', {}).get('entity')\n",
    "    if wandb_enabled and (wandb_entity is None or 'your_wandb_entity' in wandb_entity):\n",
    "        print(\"Warning: W&B is enabled but entity is not set or is default. Disabling W&B for this test.\")\n",
    "        config['logging']['wandb']['enabled'] = False\n",
    "\n",
    "    # Create learner\n",
    "    stage1_learner = get_stage1_learner(config)\n",
    "    print(\"Stage 1 Learner created successfully.\")\n",
    "\n",
    "    # Perform basic checks on the learner\n",
    "    assert isinstance(stage1_learner, Learner)\n",
    "    assert isinstance(stage1_learner.dls, DataLoaders)\n",
    "    assert isinstance(stage1_learner.model, BaselineLLaVAModel)\n",
    "    assert isinstance(stage1_learner.loss_func, LLaVALoss)\n",
    "    assert len(stage1_learner.opt.param_groups) == 1 # Only one group (projector) should be trainable\n",
    "    \n",
    "    # Check if only projector parameters are in the trainable group\n",
    "    projector_params_set = set(stage1_learner.model.projector.parameters())\n",
    "    opt_params_set = set(stage1_learner.opt.param_groups[0]['params'])\n",
    "    assert opt_params_set == projector_params_set, \"Optimizer parameter group does not match projector parameters.\"\n",
    "    \n",
    "    # Check if other parts are frozen (requires_grad=False)\n",
    "    for name, param in stage1_learner.model.named_parameters():\n",
    "        if 'projector' in name:\n",
    "            assert param.requires_grad == True, f\"Projector parameter {name} is frozen but should be trainable.\"\n",
    "        elif 'vision_tower' in name or 'language_model' in name:\n",
    "             assert param.requires_grad == False, f\"Parameter {name} should be frozen but is not.\"\n",
    "                \n",
    "    print(\"\\nLearner summary:\")\n",
    "    stage1_learner.summary()\n",
    "    \n",
    "    print(\"\\nLearner setup test passed.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Skipping test: FileNotFoundError - {e}\")\n",
    "    print(\"Ensure config, data, and model paths are correct.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Skipping test: ImportError - {e}\")\n",
    "    print(\"Ensure all required libraries (transformers, fastai, etc.) are installed.\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred during learner setup test: {e}\")\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Clean up memory\n",
    "    if 'stage1_learner' in locals() and stage1_learner is not None and stage1_learner.model is not None:\n",
    "        if hasattr(stage1_learner.model, 'vision_tower') and stage1_learner.model.vision_tower is not None:\n",
    "            stage1_learner.model.vision_tower.to('cpu')\n",
    "        if hasattr(stage1_learner.model, 'language_model') and stage1_learner.model.language_model is not None:\n",
    "            stage1_learner.model.language_model.to('cpu')\n",
    "        if hasattr(stage1_learner.model, 'projector') and stage1_learner.model.projector is not None:\n",
    "            stage1_learner.model.projector.to('cpu')\n",
    "        del stage1_learner.model\n",
    "        del stage1_learner\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Cleaned up stage1_learner and model\")\n",
    "    # Terminate wandb run if it was initialized\n",
    "    if wandb.run is not None:\n",
    "        wandb.finish()\n",
    "        print(\"Finished W&B run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Implement Stage 1 Training Script (Placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual training loop execution will be added here in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for train_stage1 function\n",
    "# This function will load config, get learner, and call learner.fit_one_cycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/31_training_stage1.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "Adaptive_Patching_VIT_fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
