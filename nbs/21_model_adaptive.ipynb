{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Model Components\n",
    "\n",
    "> Defines adaptive components for the LLaVA model, starting with the Adaptive Patcher interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.adaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    # print(f\"Project root already in sys.path: {project_root_str}\") # Less verbose\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n",
      "Loaded config from ../configs/config.yaml\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pad_sequence # For padding variable length sequences\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast # For type hints\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# Import base model and utilities\n",
    "from llava.model.baseline import BaselineLLaVAModel\n",
    "from llava.utils import load_config\n",
    "from llava.data.preprocessing import tokenizer, IMAGE_TOKEN_INDEX_PLACEHOLDER, IGNORE_INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.1: Define Adaptive Patcher Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This base class defines the interface for any adaptive patching strategy. Subclasses will implement specific logic (e.g., variable resolution, attention-based patching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AdaptivePatcher(nn.Module):\n",
    "    \"\"\"Base interface for adaptive image patching modules.\n",
    "\n",
    "    Subclasses should implement the `forward` method to dynamically process\n",
    "    an input image (or its features) based on content or context (like text instructions)\n",
    "    and return a structured representation of image features for the projector.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the Adaptive Patcher.\n",
    "\n",
    "        Args:\n",
    "            config: Dictionary containing configuration relevant to the patcher strategy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Potentially load sub-modules or parameters based on config['strategy']\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None, # Existing features (e.g., from base processing)\n",
    "        text_features: Optional[torch.Tensor] = None, # For text-guided strategies\n",
    "        raw_image: Optional[Image.Image] = None, # Original image for properties like aspect ratio\n",
    "        **kwargs\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[Dict[str, Any]]]:\n",
    "        \"\"\"Processes the input image adaptively or determines processing strategy.\n",
    "\n",
    "        The exact inputs used and outputs returned depend on the specific strategy.\n",
    "        For example, a strategy predictor might only need pooled features,\n",
    "        while a variable resolution strategy primarily needs the raw image aspect ratio.\n",
    "        A text-guided strategy needs text_features and potentially patch features.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Optional preprocessed image tensor (e.g., B x C x H x W).\n",
    "                           Could represent the full image or specific patches.\n",
    "            text_features: Optional tensor containing embeddings of the instruction text.\n",
    "                           Needed for text-guided patching strategies.\n",
    "            raw_image: Optional PIL image, potentially needed for calculating aspect ratio\n",
    "                       or other properties not easily derived from pixel_values alone.\n",
    "            **kwargs: Additional keyword arguments specific to the patching strategy.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - Optional[torch.Tensor]: Processed/selected image features, if the patcher\n",
    "                                    directly outputs features. Shape might vary.\n",
    "                                    Can be None if the patcher only outputs metadata.\n",
    "            - Optional[Dict[str, Any]]: Metadata about the patching process or decision.\n",
    "                                      (e.g., strategy used, number of patches, selected grid).\n",
    "                                      Can be None if no metadata is generated.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This base method must be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement the forward method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AdaptivePatcher\n",
       "\n",
       ">      AdaptivePatcher (config:Dict[str,Any])\n",
       "\n",
       "*Base interface for adaptive image patching modules.\n",
       "\n",
       "    Subclasses should implement the `forward` method to dynamically process\n",
       "    an input image (or its features) based on content or context (like text instructions)\n",
       "    and return a structured representation of image features for the projector.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AdaptivePatcher\n",
       "\n",
       ">      AdaptivePatcher (config:Dict[str,Any])\n",
       "\n",
       "*Base interface for adaptive image patching modules.\n",
       "\n",
       "    Subclasses should implement the `forward` method to dynamically process\n",
       "    an input image (or its features) based on content or context (like text instructions)\n",
       "    and return a structured representation of image features for the projector.*"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AdaptivePatcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.2: Implement Variable Resolution Patcher Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This patcher implements the variable resolution strategy inspired by LLaVA-NeXT. It analyzes the input image's aspect ratio and selects the optimal grid configuration from a predefined set (`image_grid_pinpoints`). The actual image resizing and patching happen elsewhere, based on the grid configuration returned by this patcher.\n",
    "\n",
    "**Update for Step 8.1:** Modified to check `config['ablation']['force_patcher_strategy']`. If set to `'baseline'`, it bypasses aspect ratio calculation and returns metadata for the base 336x336 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VariableResolutionPatcher(AdaptivePatcher):\n",
    "    \"\"\"Adaptive patcher that selects an optimal grid resolution based on image aspect ratio,\n",
    "    with an option to force baseline behavior for ablation studies.\n",
    "\n",
    "    Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
    "    dimensions but does not perform the actual image processing or feature extraction.\n",
    "    The main model's forward pass should use the output metadata to handle the image.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the VariableResolutionPatcher.\n",
    "\n",
    "        Args:\n",
    "            config: Dictionary containing configuration. Expected keys:\n",
    "                    'model.adaptive_patcher.image_grid_pinpoints': List of [H, W] grids.\n",
    "                    'model.vision_config.image_size': Base image size (e.g., 336).\n",
    "                    'model.vision_config.patch_size': Patch size (e.g., 14).\n",
    "                    'ablation.force_patcher_strategy': Optional string ('baseline' or null).\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        # Default grid from LLaVA-NeXT / Project Spec\n",
    "        default_grid = [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]\n",
    "        # Retrieve nested config safely\n",
    "        patcher_config = self.config.get('model', {}).get('adaptive_patcher', {})\n",
    "        vision_config = self.config.get('model', {}).get('vision_config', {})\n",
    "        self.ablation_config = self.config.get('ablation', {})\n",
    "\n",
    "        self.image_grid_pinpoints = patcher_config.get('image_grid_pinpoints', default_grid)\n",
    "        self.base_image_size = vision_config.get('image_size', 336)\n",
    "        self.patch_size = vision_config.get('patch_size', 14)\n",
    "\n",
    "        # Ensure base resolution is included as an option (implicitly or explicitly)\n",
    "        self.base_grid = (self.base_image_size, self.base_image_size)\n",
    "        if list(self.base_grid) not in self.image_grid_pinpoints:\n",
    "             self.image_grid_pinpoints.append(list(self.base_grid))\n",
    "             \n",
    "        self.force_baseline = self.ablation_config.get('force_patcher_strategy', None) == 'baseline'\n",
    "        if self.force_baseline:\n",
    "            print(\"Initialized VariableResolutionPatcher: Ablation active - FORCING BASELINE grid (336x336).\")\n",
    "        else:\n",
    "             print(f\"Initialized VariableResolutionPatcher with grid options: {self.image_grid_pinpoints}\")\n",
    "\n",
    "    def select_best_resolution(self, original_height: int, original_width: int) -> Tuple[int, int]:\n",
    "        \"\"\"Selects the best grid resolution based on aspect ratio and minimizing waste.\n",
    "\n",
    "        Args:\n",
    "            original_height: Height of the raw input image.\n",
    "            original_width: Width of the raw input image.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: The selected best grid dimensions (height, width).\n",
    "        \"\"\"\n",
    "        original_aspect_ratio = original_height / original_width\n",
    "\n",
    "        best_fit_grid = None\n",
    "        min_wasted_pixels = float('inf')\n",
    "\n",
    "        for grid_h, grid_w in self.image_grid_pinpoints:\n",
    "            grid_aspect_ratio = grid_h / grid_w\n",
    "\n",
    "            # Calculate the dimensions if the image were scaled to fit this grid\n",
    "            # Scale based on the limiting dimension\n",
    "            scale_h = grid_h / original_height\n",
    "            scale_w = grid_w / original_width\n",
    "\n",
    "            if original_aspect_ratio > grid_aspect_ratio: # Scale based on height\n",
    "                scaled_h = grid_h\n",
    "                scaled_w = int(original_width * scale_h)\n",
    "            else: # Scale based on width\n",
    "                scaled_w = grid_w\n",
    "                scaled_h = int(original_height * scale_w)\n",
    "            \n",
    "            # Ensure scaled dimensions do not exceed grid dimensions (due to int conversion)\n",
    "            scaled_h = min(scaled_h, grid_h)\n",
    "            scaled_w = min(scaled_w, grid_w)\n",
    "            \n",
    "            # Calculate wasted area (pixels in the grid not covered by the scaled image)\n",
    "            grid_area = grid_h * grid_w\n",
    "            scaled_image_area = scaled_h * scaled_w\n",
    "            wasted_pixels = grid_area - scaled_image_area\n",
    "\n",
    "            # Prefer grids with less waste\n",
    "            # Among grids with similar waste, the reference code doesn't specify tie-breaking.\n",
    "            # Let's pick the first one encountered with the minimum waste.\n",
    "            # A slightly better tie-breaker might be aspect ratio closeness, but min waste is simpler.\n",
    "            if wasted_pixels < min_wasted_pixels:\n",
    "                min_wasted_pixels = wasted_pixels\n",
    "                best_fit_grid = (grid_h, grid_w)\n",
    "            # Simple tie-breaking: if waste is equal, prefer larger area (less scaling down)\n",
    "            elif wasted_pixels == min_wasted_pixels:\n",
    "                 if best_fit_grid is None or (grid_h * grid_w > best_fit_grid[0] * best_fit_grid[1]):\n",
    "                       best_fit_grid = (grid_h, grid_w)\n",
    "                       \n",
    "        # Fallback to base resolution if something went wrong\n",
    "        if best_fit_grid is None:\n",
    "            print(f\"Warning: Could not determine best fit grid for H={original_height}, W={original_width}. Defaulting to base {self.base_image_size}x{self.base_image_size}.\")\n",
    "            best_fit_grid = self.base_grid\n",
    "            \n",
    "        return best_fit_grid\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None, # Not used by this patcher\n",
    "        text_features: Optional[torch.Tensor] = None, # Not used by this patcher\n",
    "        raw_image: Optional[Image.Image] = None,\n",
    "        **kwargs\n",
    "    ) -> Tuple[None, Dict[str, Any]]: # Returns None for features, Dict for metadata\n",
    "        \"\"\"Determines the best grid resolution based on the raw image's aspect ratio\n",
    "           or returns the baseline grid if forced by ablation config.\n",
    "\n",
    "        Args:\n",
    "            raw_image: The original PIL Image object (ignored if force_baseline=True).\n",
    "            pixel_values: Ignored by this patcher.\n",
    "            text_features: Ignored by this patcher.\n",
    "            **kwargs: Ignored.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - None: This patcher does not directly return processed features.\n",
    "            - Dict[str, Any]: Metadata including:\n",
    "                - 'strategy': 'variable_resolution' or 'forced_baseline'\n",
    "                - 'selected_grid' (Tuple[int, int]): The chosen grid dimensions (H, W).\n",
    "                - 'num_patches_h' (int): Number of patches vertically in the grid.\n",
    "                - 'num_patches_w' (int): Number of patches horizontally in the grid.\n",
    "                - 'total_patches' (int): Total number of patches in the selected grid.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If raw_image is not provided (and baseline is not forced) or patch_size is invalid.\n",
    "        \"\"\"\n",
    "        selected_grid_h, selected_grid_w = self.base_grid\n",
    "        strategy_name = 'variable_resolution'\n",
    "\n",
    "        if self.force_baseline:\n",
    "            strategy_name = 'forced_baseline'\n",
    "            # Use the predefined base grid\n",
    "            selected_grid_h, selected_grid_w = self.base_grid\n",
    "        else:\n",
    "            if raw_image is None:\n",
    "                raise ValueError(\"VariableResolutionPatcher requires the 'raw_image' input when not forcing baseline.\")\n",
    "            original_width, original_height = raw_image.size\n",
    "            # Select the best grid based on aspect ratio\n",
    "            selected_grid_h, selected_grid_w = self.select_best_resolution(original_height, original_width)\n",
    "            \n",
    "        # Calculate number of patches for the selected grid\n",
    "        if self.patch_size <= 0:\n",
    "            raise ValueError(\"Patch size must be positive.\")\n",
    "        num_patches_h = selected_grid_h // self.patch_size\n",
    "        num_patches_w = selected_grid_w // self.patch_size\n",
    "        total_patches = num_patches_h * num_patches_w\n",
    "        \n",
    "        metadata = {\n",
    "            'strategy': strategy_name,\n",
    "            'selected_grid': (selected_grid_h, selected_grid_w),\n",
    "            'num_patches_h': num_patches_h,\n",
    "            'num_patches_w': num_patches_w,\n",
    "            'total_patches': total_patches\n",
    "        }\n",
    "        \n",
    "        # This patcher returns metadata for the main model to use, not processed features.\n",
    "        return None, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### VariableResolutionPatcher\n",
       "\n",
       ">      VariableResolutionPatcher (config:Dict[str,Any])\n",
       "\n",
       "*Adaptive patcher that selects an optimal grid resolution based on image aspect ratio,\n",
       "with an option to force baseline behavior for ablation studies.\n",
       "\n",
       "Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
       "dimensions but does not perform the actual image processing or feature extraction.\n",
       "The main model's forward pass should use the output metadata to handle the image.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### VariableResolutionPatcher\n",
       "\n",
       ">      VariableResolutionPatcher (config:Dict[str,Any])\n",
       "\n",
       "*Adaptive patcher that selects an optimal grid resolution based on image aspect ratio,\n",
       "with an option to force baseline behavior for ablation studies.\n",
       "\n",
       "Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
       "dimensions but does not perform the actual image processing or feature extraction.\n",
       "The main model's forward pass should use the output metadata to handle the image.*"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(VariableResolutionPatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized VariableResolutionPatcher with grid options: [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008], [336, 336]]\n",
      "Initialized VariableResolutionPatcher: Ablation active - FORCING BASELINE grid (336x336).\n",
      "--- Normal Mode --- \n",
      "Image (600, 600), Ratio 1.00 -> Selected Grid: (336, 336), Strategy: variable_resolution\n",
      "Image (400, 800), Ratio 0.50 -> Selected Grid: (336, 672), Strategy: variable_resolution\n",
      "--- Forced Baseline Mode --- \n",
      "Image (600, 600), Ratio 1.00 -> Selected Grid: (336, 336), Strategy: forced_baseline\n",
      "Image (400, 800), Ratio 0.50 -> Selected Grid: (336, 336), Strategy: forced_baseline\n",
      "Variable Resolution Patcher tests passed.\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "import PIL.Image\n",
    "\n",
    "# Create dummy config\n",
    "test_config = {\n",
    "    'model': {\n",
    "        'adaptive_patcher': {\n",
    "             'image_grid_pinpoints': [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]\n",
    "        },\n",
    "        'vision_config': {\n",
    "            'image_size': 336,\n",
    "            'patch_size': 14\n",
    "        }\n",
    "    },\n",
    "    'ablation': { # Add ablation config\n",
    "        'force_patcher_strategy': None # Default: no forcing\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Test Normal Operation --- \n",
    "patcher_normal = VariableResolutionPatcher(test_config)\n",
    "test_img_square = PIL.Image.new('RGB', (600, 600))\n",
    "test_img_tall = PIL.Image.new('RGB', (400, 800))\n",
    "\n",
    "_, meta_sq = patcher_normal(raw_image=test_img_square)\n",
    "_, meta_tall = patcher_normal(raw_image=test_img_tall)\n",
    "print(\"--- Normal Mode --- \")\n",
    "print(f\"Image (600, 600), Ratio 1.00 -> Selected Grid: {meta_sq['selected_grid']}, Strategy: {meta_sq['strategy']}\")\n",
    "print(f\"Image (400, 800), Ratio 0.50 -> Selected Grid: {meta_tall['selected_grid']}, Strategy: {meta_tall['strategy']}\")\n",
    "assert meta_sq['selected_grid'] == (336, 336)\n",
    "assert meta_tall['selected_grid'] == (336, 672)\n",
    "assert meta_sq['strategy'] == 'variable_resolution'\n",
    "\n",
    "# --- Test Forced Baseline --- \n",
    "test_config_forced = copy.deepcopy(test_config)\n",
    "test_config_forced['ablation']['force_patcher_strategy'] = 'baseline'\n",
    "patcher_forced = VariableResolutionPatcher(test_config_forced)\n",
    "\n",
    "_, meta_sq_forced = patcher_forced(raw_image=test_img_square)\n",
    "_, meta_tall_forced = patcher_forced(raw_image=test_img_tall)\n",
    "\n",
    "print(\"--- Forced Baseline Mode --- \")\n",
    "print(f\"Image (600, 600), Ratio 1.00 -> Selected Grid: {meta_sq_forced['selected_grid']}, Strategy: {meta_sq_forced['strategy']}\")\n",
    "print(f\"Image (400, 800), Ratio 0.50 -> Selected Grid: {meta_tall_forced['selected_grid']}, Strategy: {meta_tall_forced['strategy']}\")\n",
    "assert meta_sq_forced['selected_grid'] == (336, 336)\n",
    "assert meta_tall_forced['selected_grid'] == (336, 336)\n",
    "assert meta_sq_forced['strategy'] == 'forced_baseline'\n",
    "\n",
    "# Test ValueError if raw_image is None and not forced\n",
    "try:\n",
    "    patcher_normal(raw_image=None)\n",
    "    assert False, \"Should have raised ValueError when raw_image is None (normal mode)\"\n",
    "except ValueError:\n",
    "    pass # Expected\n",
    "\n",
    "# Test ValueError is NOT raised if raw_image is None and forced\n",
    "try:\n",
    "    patcher_forced(raw_image=None) # Should work, doesn't need raw_image\n",
    "except ValueError:\n",
    "     assert False, \"Should NOT have raised ValueError when raw_image is None (forced baseline mode)\"\n",
    "\n",
    "print(\"Variable Resolution Patcher tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.4: Define Adaptive LLaVA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves creating `AdaptiveLLaVAModel` inheriting from `BaselineLLaVAModel` and integrating the chosen `AdaptivePatcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Placeholder for other potential patcher implementations\n",
    "# class PredictorPatcher(AdaptivePatcher): ...\n",
    "# class TextGuidedPatcher(AdaptivePatcher): ...\n",
    "\n",
    "# Mapping from strategy name in config to Patcher class\n",
    "PATCHER_STRATEGIES = {\n",
    "    'variable_resolution': VariableResolutionPatcher,\n",
    "    # 'predictor': PredictorPatcher, # Add when implemented\n",
    "    # 'text_guided': TextGuidedPatcher, # Add when implemented\n",
    "}\n",
    "\n",
    "class AdaptiveLLaVAModel(BaselineLLaVAModel):\n",
    "    \"\"\"LLaVA Model extended with an Adaptive Patcher module.\n",
    "\n",
    "    Inherits from BaselineLLaVAModel and adds an adaptive patcher component\n",
    "    based on the configuration. The forward pass needs to be overridden\n",
    "    to incorporate the patcher's logic.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the Adaptive LLaVA model.\n",
    "\n",
    "        Loads baseline components and instantiates the specified adaptive patcher.\n",
    "\n",
    "        Args:\n",
    "            config: The main configuration dictionary.\n",
    "        \"\"\"\n",
    "        # Initialize baseline components (Vision Tower, LLM, Projector)\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.patcher = None\n",
    "        patcher_config = self.config.get('model', {}).get('adaptive_patcher', {})\n",
    "        patcher_enabled = patcher_config.get('enabled', False)\n",
    "        patcher_strategy = patcher_config.get('strategy')\n",
    "\n",
    "        if patcher_enabled and patcher_strategy:\n",
    "            if patcher_strategy in PATCHER_STRATEGIES:\n",
    "                PatcherClass = PATCHER_STRATEGIES[patcher_strategy]\n",
    "                try:\n",
    "                    # Pass the full config, including ablation settings\n",
    "                    self.patcher = PatcherClass(config)\n",
    "                    print(f\"Adaptive Patcher enabled with strategy: '{patcher_strategy}' ({PatcherClass.__name__})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error initializing patcher '{patcher_strategy}': {e}. Disabling patcher.\")\n",
    "                    self.patcher = None\n",
    "            else:\n",
    "                print(f\"Warning: Unknown adaptive patcher strategy '{patcher_strategy}'. Disabling patcher.\")\n",
    "                self.patcher = None\n",
    "        else:\n",
    "            print(\"Adaptive Patcher is disabled in the configuration.\")\n",
    "\n",
    "    # --- Step 6.5: Implement Adaptive Forward Pass --- \n",
    "    # Override the forward pass to integrate the patcher logic\n",
    "    def forward(self, \n",
    "                pixel_values: torch.Tensor,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None, \n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                # Add raw_images potentially needed by patcher\n",
    "                raw_images: Optional[List[Image.Image]] = None \n",
    "               ) -> CausalLMOutputWithPast: # Return type from transformers\n",
    "        \"\"\"Defines the forward pass of the Adaptive LLaVA model.\n",
    "\n",
    "        If an adaptive patcher is enabled, it's called to determine patching strategy\n",
    "        (metadata stored). The actual image features used currently come from the standard\n",
    "        `pixel_values` input (base resolution), regardless of patcher output. This allows\n",
    "        the structural integration without implementing complex variable feature handling yet.\n",
    "        \n",
    "        **Note:** For ablation studies, the patcher might be forced to return baseline metadata.\n",
    "        The actual processing logic based on metadata (variable number of patches, etc.) \n",
    "        is **not yet implemented** here; this forward pass currently behaves like the baseline.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Tensor of shape (batch_size, C, H, W) for the base resolution (e.g., 336x336).\n",
    "            input_ids: Tensor of shape (batch_size, sequence_length) containing token IDs\n",
    "                       and IMAGE_TOKEN_INDEX_PLACEHOLDER markers (-200).\n",
    "            attention_mask: Optional tensor of shape (batch_size, sequence_length).\n",
    "            labels: Optional tensor of shape (batch_size, sequence_length) corresponding\n",
    "                    to input_ids (with -100 masking) for loss calculation.\n",
    "            raw_images: Optional list of PIL Images for the batch, needed by some patchers.\n",
    "\n",
    "        Returns:\n",
    "            Output dictionary from the language model (transformers.CausalLMOutputWithPast).\n",
    "        \"\"\"\n",
    "        # print(\"AdaptiveLLaVAModel forward pass called.\") # Debug print\n",
    "        patcher_metadata_batch = [None] * pixel_values.shape[0] # Initialize metadata list\n",
    "\n",
    "        # --- Patcher Logic (Get Metadata) --- \n",
    "        if self.patcher is not None:\n",
    "            # Only require raw_images if not forcing baseline (baseline doesn't need raw image)\n",
    "            requires_raw = not getattr(self.patcher, 'force_baseline', False)\n",
    "            if requires_raw and (raw_images is None or len(raw_images) != pixel_values.shape[0]):\n",
    "                 warnings.warn(\"Patcher is active and needs raw_images, but they were not provided or length mismatch. Patcher cannot run.\")\n",
    "            else:\n",
    "                # Iterate through batch to get metadata for each sample\n",
    "                for i in range(pixel_values.shape[0]):\n",
    "                    try:\n",
    "                        current_raw_image = raw_images[i] if raw_images else None\n",
    "                        _, patcher_metadata_sample = self.patcher(raw_image=current_raw_image)\n",
    "                        patcher_metadata_batch[i] = patcher_metadata_sample\n",
    "                        # print(f\"Batch {i} Patcher metadata: {patcher_metadata_sample}\") # Debug print\n",
    "                    except Exception as e:\n",
    "                        warnings.warn(f\"Error running patcher for batch index {i}: {e}\")\n",
    "                # Store or log patcher_metadata_batch if needed\n",
    "                # Currently, we log/print but don't change the feature processing path.\n",
    "                self.current_patcher_metadata = patcher_metadata_batch # Store for potential inspection/logging\n",
    "        \n",
    "        # --- Feature Processing (Currently Still Baseline Behavior) --- \n",
    "        # TODO: Modify this section to use `patcher_metadata_batch` to handle variable features.\n",
    "        # This would involve either: \n",
    "        #   a) Dynamically processing images based on metadata (complex batching).\n",
    "        #   b) Pre-processing multiple image resolutions/patches, encoding all, then selecting/\n",
    "        #      combining features based on metadata here.\n",
    "\n",
    "        # 1. Encode Image (using standard pixel_values) & Project Features\n",
    "        image_features = self.encode_image(pixel_values) # (B, P_base, D_clip)\n",
    "        if image_features is None:\n",
    "            raise RuntimeError(\"Image encoding failed.\")\n",
    "        projected_image_features = self.projector(image_features) # (B, P_base, D_llm)\n",
    "        num_image_patches = projected_image_features.shape[1]\n",
    "\n",
    "        # --- Prepare LLM inputs (Same as baseline) --- \n",
    "        input_ids_clone = input_ids.clone()\n",
    "        input_ids_clone[input_ids_clone == self.image_token_index_marker] = 0 \n",
    "        text_embeddings = self.get_input_embeddings()(input_ids_clone) \n",
    "\n",
    "        new_input_embeds = []\n",
    "        new_labels = [] if labels is not None else None\n",
    "        new_attention_mask = []\n",
    "\n",
    "        for batch_idx in range(input_ids.shape[0]):\n",
    "            image_token_indices = torch.where(input_ids[batch_idx] == self.image_token_index_marker)[0]\n",
    "            if len(image_token_indices) == 0:\n",
    "                warnings.warn(f\"Image token placeholder {self.image_token_index_marker} not found in batch index {batch_idx}. Skipping image features.\")\n",
    "                new_input_embeds.append(text_embeddings[batch_idx])\n",
    "                current_attention_mask = attention_mask[batch_idx] if attention_mask is not None else (input_ids[batch_idx] != tokenizer.pad_token_id).long()\n",
    "                new_attention_mask.append(current_attention_mask)\n",
    "                if new_labels is not None and labels is not None:\n",
    "                    new_labels.append(labels[batch_idx])\n",
    "                continue\n",
    "\n",
    "            image_token_start_index = image_token_indices[0].item()\n",
    "\n",
    "            text_emb_before = text_embeddings[batch_idx, :image_token_start_index]\n",
    "            text_emb_after = text_embeddings[batch_idx, image_token_start_index + 1:]\n",
    "\n",
    "            cur_new_embed = torch.cat([\n",
    "                text_emb_before,\n",
    "                projected_image_features[batch_idx].to(text_embeddings.device, dtype=text_embeddings.dtype),\n",
    "                text_emb_after\n",
    "            ], dim=0)\n",
    "            new_input_embeds.append(cur_new_embed)\n",
    "\n",
    "            current_attention_mask = attention_mask[batch_idx] if attention_mask is not None else (input_ids[batch_idx] != tokenizer.pad_token_id).long()\n",
    "            mask_before = current_attention_mask[:image_token_start_index]\n",
    "            mask_image = torch.ones(num_image_patches, dtype=torch.long, device=current_attention_mask.device)\n",
    "            mask_after = current_attention_mask[image_token_start_index + 1:]\n",
    "            cur_new_mask = torch.cat([\n",
    "                mask_before,\n",
    "                mask_image,\n",
    "                mask_after\n",
    "            ], dim=0)\n",
    "            new_attention_mask.append(cur_new_mask)\n",
    "\n",
    "            if new_labels is not None and labels is not None:\n",
    "                label_before = labels[batch_idx, :image_token_start_index]\n",
    "                label_image = torch.full((num_image_patches,), self.ignore_index, dtype=torch.long, device=labels.device)\n",
    "                label_after = labels[batch_idx, image_token_start_index + 1:]\n",
    "                cur_new_label = torch.cat([\n",
    "                    label_before,\n",
    "                    label_image,\n",
    "                    label_after\n",
    "                ], dim=0)\n",
    "                new_labels.append(cur_new_label)\n",
    "\n",
    "        # --- Padding (Same as baseline) --- \n",
    "        padded_input_embeds = pad_sequence(new_input_embeds, batch_first=True, padding_value=0.0)\n",
    "        padded_attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)\n",
    "        padded_labels = None\n",
    "        if new_labels is not None:\n",
    "            padded_labels = pad_sequence(new_labels, batch_first=True, padding_value=self.ignore_index)\n",
    "\n",
    "        # --- Pass to LLM (Same as baseline) --- \n",
    "        outputs: CausalLMOutputWithPast = self.language_model(\n",
    "            inputs_embeds=padded_input_embeds,\n",
    "            attention_mask=padded_attention_mask,\n",
    "            labels=padded_labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AdaptiveLLaVAModel\n",
       "\n",
       ">      AdaptiveLLaVAModel (config:Dict[str,Any])\n",
       "\n",
       "*LLaVA Model extended with an Adaptive Patcher module.\n",
       "\n",
       "    Inherits from BaselineLLaVAModel and adds an adaptive patcher component\n",
       "    based on the configuration. The forward pass needs to be overridden\n",
       "    to incorporate the patcher's logic.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AdaptiveLLaVAModel\n",
       "\n",
       ">      AdaptiveLLaVAModel (config:Dict[str,Any])\n",
       "\n",
       "*LLaVA Model extended with an Adaptive Patcher module.\n",
       "\n",
       "    Inherits from BaselineLLaVAModel and adds an adaptive patcher component\n",
       "    based on the configuration. The forward pass needs to be overridden\n",
       "    to incorporate the patcher's logic.*"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AdaptiveLLaVAModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b221b591d04f9c91c856a5b84aa16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n",
      "LoRA is disabled in the configuration.\n",
      "LLM embedding size already matches tokenizer size. No resizing needed.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Adaptive Patcher enabled with strategy: 'variable_resolution' (VariableResolutionPatcher)\n",
      "Initialized VariableResolutionPatcher: Ablation active - FORCING BASELINE grid (336x336).\n",
      "Running Adaptive Forward Pass Test (Forced Baseline Ablation)...\n",
      "Output logits shape: torch.Size([2, 586, 32001])\n",
      "Output loss: tensor(10.4655, grad_fn=<NllLossBackward0>)\n",
      "Adaptive Forward Pass test successful!\n",
      "Cleaned up test_adaptive_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_140/4271200700.py:121: UserWarning: Patcher is active and needs raw_images, but they were not provided or length mismatch. Patcher cannot run.\n",
      "  warnings.warn(\"Patcher is active and needs raw_images, but they were not provided or length mismatch. Patcher cannot run.\")\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "import torch, gc\n",
    "from transformers import AutoModelForCausalLM, CLIPVisionModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    config_path = '../configs/config.yaml'\n",
    "    test_config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "\n",
    "    # --- Test with Adaptive Patcher Enabled (Forced Baseline) --- \n",
    "    if 'model' not in test_config: test_config['model'] = {}\n",
    "    if 'adaptive_patcher' not in test_config['model']: test_config['model']['adaptive_patcher'] = {}\n",
    "    test_config['model']['adaptive_patcher']['enabled'] = True\n",
    "    test_config['model']['adaptive_patcher']['strategy'] = 'variable_resolution'\n",
    "    # Add ablation setting\n",
    "    if 'ablation' not in test_config: test_config['ablation'] = {}\n",
    "    test_config['ablation']['force_patcher_strategy'] = 'baseline' # Force baseline for this test\n",
    "    \n",
    "    # Disable LoRA and Checkpointing for simplicity\n",
    "    if 'peft' not in test_config['model']: test_config['model']['peft'] = {}\n",
    "    test_config['model']['peft']['use_lora'] = False \n",
    "    test_config['model']['use_activation_checkpointing'] = False\n",
    "\n",
    "    # Instantiate the adaptive model\n",
    "    test_adaptive_model = AdaptiveLLaVAModel(test_config)\n",
    "    test_adaptive_model.eval() # Set to eval mode\n",
    "\n",
    "    print(\"Running Adaptive Forward Pass Test (Forced Baseline Ablation)...\")\n",
    "    # Prepare dummy inputs\n",
    "    batch_size = 2\n",
    "    seq_len = 15 # Short sequence for testing\n",
    "    img_size = 336 # From config\n",
    "    num_patches = 576 # (336/14)^2 - Base patches\n",
    "    llm_hidden_dim = test_config['model']['projector']['output_dim']\n",
    "    tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "    dummy_pixel_values = torch.randn(batch_size, 3, img_size, img_size)\n",
    "    dummy_input_ids = torch.randint(1, tokenizer_vocab_size, (batch_size, seq_len), dtype=torch.long)\n",
    "    placeholder_idx = 5 \n",
    "    dummy_input_ids[:, placeholder_idx] = IMAGE_TOKEN_INDEX_PLACEHOLDER\n",
    "    dummy_attention_mask = torch.ones_like(dummy_input_ids)\n",
    "    dummy_labels = dummy_input_ids.clone()\n",
    "    dummy_labels[:, :placeholder_idx+1] = IGNORE_INDEX\n",
    "    \n",
    "    # Raw images are not needed when forcing baseline, but pass None to test handling\n",
    "    dummy_raw_images = None \n",
    "\n",
    "    # Perform forward pass\n",
    "    with torch.no_grad(): \n",
    "         outputs = test_adaptive_model(\n",
    "            pixel_values=dummy_pixel_values,\n",
    "            input_ids=dummy_input_ids,\n",
    "            attention_mask=dummy_attention_mask,\n",
    "            labels=dummy_labels,\n",
    "            raw_images=dummy_raw_images # Pass None \n",
    "         )\n",
    "\n",
    "    # Check output type and attributes\n",
    "    assert isinstance(outputs, CausalLMOutputWithPast)\n",
    "    assert outputs.logits is not None\n",
    "    assert outputs.loss is not None\n",
    "\n",
    "    # Check output shapes (should match baseline)\n",
    "    expected_seq_len = seq_len - 1 + num_patches\n",
    "    expected_logits_shape = (batch_size, expected_seq_len, tokenizer_vocab_size)\n",
    "\n",
    "    assert outputs.logits.shape == expected_logits_shape, \\\n",
    "        f\"Expected logits shape {expected_logits_shape}, but got {outputs.logits.shape}\"\n",
    "    print(f\"Output logits shape: {outputs.logits.shape}\")\n",
    "    print(f\"Output loss: {outputs.loss}\")\n",
    "    \n",
    "    # Check if patcher metadata reflects forced baseline\n",
    "    assert hasattr(test_adaptive_model, 'current_patcher_metadata')\n",
    "    assert len(test_adaptive_model.current_patcher_metadata) == batch_size\n",
    "    assert test_adaptive_model.current_patcher_metadata[0]['selected_grid'] == (336, 336) # Forced baseline\n",
    "    assert test_adaptive_model.current_patcher_metadata[0]['strategy'] == 'forced_baseline'\n",
    "    assert test_adaptive_model.current_patcher_metadata[1]['selected_grid'] == (336, 336) # Forced baseline\n",
    "    assert test_adaptive_model.current_patcher_metadata[1]['strategy'] == 'forced_baseline'\n",
    "    \n",
    "    print(\"Adaptive Forward Pass test successful!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file {config_path} not found. Skipping AdaptiveLLaVAModel test.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Skipping test due to ImportError: {e}. (Likely `peft` is missing)\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during AdaptiveLLaVAModel test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Clean up\n",
    "    if 'test_adaptive_model' in locals():\n",
    "        if hasattr(test_adaptive_model, 'vision_tower') and test_adaptive_model.vision_tower is not None:\n",
    "            test_adaptive_model.vision_tower.to('cpu')\n",
    "        if hasattr(test_adaptive_model, 'language_model') and test_adaptive_model.language_model is not None:\n",
    "            test_adaptive_model.language_model.to('cpu')\n",
    "        if hasattr(test_adaptive_model, 'projector') and test_adaptive_model.projector is not None:\n",
    "            test_adaptive_model.projector.to('cpu')\n",
    "        del test_adaptive_model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Cleaned up test_adaptive_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/21_model_adaptive.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "llava"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}