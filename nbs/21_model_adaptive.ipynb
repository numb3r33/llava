{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Model Components\n",
    "\n",
    "> Defines adaptive components for the LLaVA model, starting with the Adaptive Patcher interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.adaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    # print(f\"Project root already in sys.path: {project_root_str}\") # Less verbose\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n",
      "Loaded config from ../configs/config.yaml\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pad_sequence # For padding variable length sequences\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast # For type hints\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# Import base model and utilities\n",
    "from llava.model.baseline import BaselineLLaVAModel\n",
    "from llava.utils import load_config\n",
    "from llava.data.preprocessing import tokenizer, IMAGE_TOKEN_INDEX_PLACEHOLDER, IGNORE_INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.1: Define Adaptive Patcher Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This base class defines the interface for any adaptive patching strategy. Subclasses will implement specific logic (e.g., variable resolution, attention-based patching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AdaptivePatcher(nn.Module):\n",
    "    \"\"\"Base interface for adaptive image patching modules.\n",
    "\n",
    "    Subclasses should implement the `forward` method to dynamically process\n",
    "    an input image (or its features) based on content or context (like text instructions)\n",
    "    and return a structured representation of image features for the projector.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the Adaptive Patcher.\n",
    "\n",
    "        Args:\n",
    "            config: Dictionary containing configuration relevant to the patcher strategy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Potentially load sub-modules or parameters based on config['strategy']\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None, # Existing features (e.g., from base processing)\n",
    "        text_features: Optional[torch.Tensor] = None, # For text-guided strategies\n",
    "        raw_image: Optional[Image.Image] = None, # Original image for properties like aspect ratio\n",
    "        **kwargs\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[Dict[str, Any]]]:\n",
    "        \"\"\"Processes the input image adaptively or determines processing strategy.\n",
    "\n",
    "        The exact inputs used and outputs returned depend on the specific strategy.\n",
    "        For example, a strategy predictor might only need pooled features,\n",
    "        while a variable resolution strategy primarily needs the raw image aspect ratio.\n",
    "        A text-guided strategy needs text_features and potentially patch features.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Optional preprocessed image tensor (e.g., B x C x H x W).\n",
    "                           Could represent the full image or specific patches.\n",
    "            text_features: Optional tensor containing embeddings of the instruction text.\n",
    "                           Needed for text-guided patching strategies.\n",
    "            raw_image: Optional PIL image, potentially needed for calculating aspect ratio\n",
    "                       or other properties not easily derived from pixel_values alone.\n",
    "            **kwargs: Additional keyword arguments specific to the patching strategy.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - Optional[torch.Tensor]: Processed/selected image features, if the patcher\n",
    "                                    directly outputs features. Shape might vary.\n",
    "                                    Can be None if the patcher only outputs metadata.\n",
    "            - Optional[Dict[str, Any]]: Metadata about the patching process or decision.\n",
    "                                      (e.g., strategy used, number of patches, selected grid).\n",
    "                                      Can be None if no metadata is generated.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This base method must be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement the forward method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AdaptivePatcher\n",
       "\n",
       ">      AdaptivePatcher (config:Dict[str,Any])\n",
       "\n",
       "*Base interface for adaptive image patching modules.\n",
       "\n",
       "    Subclasses should implement the `forward` method to dynamically process\n",
       "    an input image (or its features) based on content or context (like text instructions)\n",
       "    and return a structured representation of image features for the projector.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AdaptivePatcher\n",
       "\n",
       ">      AdaptivePatcher (config:Dict[str,Any])\n",
       "\n",
       "*Base interface for adaptive image patching modules.\n",
       "\n",
       "    Subclasses should implement the `forward` method to dynamically process\n",
       "    an input image (or its features) based on content or context (like text instructions)\n",
       "    and return a structured representation of image features for the projector.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AdaptivePatcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.2: Implement Variable Resolution Patcher Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This patcher implements the variable resolution strategy inspired by LLaVA-NeXT. It analyzes the input image's aspect ratio and selects the optimal grid configuration from a predefined set (`image_grid_pinpoints`). The actual image resizing and patching happen elsewhere, based on the grid configuration returned by this patcher.\n",
    "\n",
    "**Update for Step 8.1:** Modified to check `config['ablation']['force_patcher_strategy']`. If set to `'baseline'`, it bypasses aspect ratio calculation and returns metadata for the base 336x336 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VariableResolutionPatcher(AdaptivePatcher):\n",
    "    \"\"\"Adaptive patcher that selects an optimal grid resolution based on image aspect ratio,\n",
    "    with an option to force baseline behavior for ablation studies.\n",
    "\n",
    "    Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
    "    dimensions but does not perform the actual image processing or feature extraction.\n",
    "    The main model's forward pass should use the output metadata to handle the image.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the VariableResolutionPatcher.\n",
    "\n",
    "        Args:\n",
    "            config: Dictionary containing configuration. Expected keys:\n",
    "                    'model.adaptive_patcher.image_grid_pinpoints': List of [H, W] grids.\n",
    "                    'model.vision_config.image_size': Base image size (e.g., 336).\n",
    "                    'model.vision_config.patch_size': Patch size (e.g., 14).\n",
    "                    'ablation.force_patcher_strategy': Optional string ('baseline' or null).\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        # Default grid from LLaVA-NeXT / Project Spec\n",
    "        default_grid = [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]\n",
    "        # Retrieve nested config safely\n",
    "        patcher_config = self.config.get('model', {}).get('adaptive_patcher', {})\n",
    "        vision_config = self.config.get('model', {}).get('vision_config', {})\n",
    "        self.ablation_config = self.config.get('ablation', {})\n",
    "\n",
    "        self.image_grid_pinpoints = patcher_config.get('image_grid_pinpoints', default_grid)\n",
    "        self.base_image_size = vision_config.get('image_size', 336)\n",
    "        self.patch_size = vision_config.get('patch_size', 14)\n",
    "\n",
    "        # Ensure base resolution is included as an option (implicitly or explicitly)\n",
    "        self.base_grid = (self.base_image_size, self.base_image_size)\n",
    "        if list(self.base_grid) not in self.image_grid_pinpoints:\n",
    "             self.image_grid_pinpoints.append(list(self.base_grid))\n",
    "             \n",
    "        self.force_baseline = self.ablation_config.get('force_patcher_strategy', None) == 'baseline'\n",
    "        if self.force_baseline:\n",
    "            print(\"Initialized VariableResolutionPatcher: Ablation active - FORCING BASELINE grid (336x336).\")\n",
    "        else:\n",
    "             print(f\"Initialized VariableResolutionPatcher with grid options: {self.image_grid_pinpoints}\")\n",
    "\n",
    "    def select_best_resolution(self, original_height: int, original_width: int) -> Tuple[int, int]:\n",
    "        \"\"\"Selects the best grid resolution based on aspect ratio and minimizing waste.\n",
    "\n",
    "        Args:\n",
    "            original_height: Height of the raw input image.\n",
    "            original_width: Width of the raw input image.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: The selected best grid dimensions (height, width).\n",
    "        \"\"\"\n",
    "        original_aspect_ratio = original_height / original_width\n",
    "\n",
    "        best_fit_grid = None\n",
    "        min_wasted_pixels = float('inf')\n",
    "\n",
    "        for grid_h, grid_w in self.image_grid_pinpoints:\n",
    "            grid_aspect_ratio = grid_h / grid_w\n",
    "\n",
    "            # Calculate the dimensions if the image were scaled to fit this grid\n",
    "            # Scale based on the limiting dimension\n",
    "            scale_h = grid_h / original_height\n",
    "            scale_w = grid_w / original_width\n",
    "\n",
    "            if original_aspect_ratio > grid_aspect_ratio: # Scale based on height\n",
    "                scaled_h = grid_h\n",
    "                scaled_w = int(original_width * scale_h)\n",
    "            else: # Scale based on width\n",
    "                scaled_w = grid_w\n",
    "                scaled_h = int(original_height * scale_w)\n",
    "            \n",
    "            # Ensure scaled dimensions do not exceed grid dimensions (due to int conversion)\n",
    "            scaled_h = min(scaled_h, grid_h)\n",
    "            scaled_w = min(scaled_w, grid_w)\n",
    "            \n",
    "            # Calculate wasted area (pixels in the grid not covered by the scaled image)\n",
    "            grid_area = grid_h * grid_w\n",
    "            scaled_image_area = scaled_h * scaled_w\n",
    "            wasted_pixels = grid_area - scaled_image_area\n",
    "\n",
    "            # Prefer grids with less waste\n",
    "            # Among grids with similar waste, the reference code doesn't specify tie-breaking.\n",
    "            # Let's pick the first one encountered with the minimum waste.\n",
    "            # A slightly better tie-breaker might be aspect ratio closeness, but min waste is simpler.\n",
    "            if wasted_pixels < min_wasted_pixels:\n",
    "                min_wasted_pixels = wasted_pixels\n",
    "                best_fit_grid = (grid_h, grid_w)\n",
    "            # Simple tie-breaking: if waste is equal, prefer larger area (less scaling down)\n",
    "            elif wasted_pixels == min_wasted_pixels:\n",
    "                 if best_fit_grid is None or (grid_h * grid_w > best_fit_grid[0] * best_fit_grid[1]):\n",
    "                       best_fit_grid = (grid_h, grid_w)\n",
    "                       \n",
    "        # Fallback to base resolution if something went wrong\n",
    "        if best_fit_grid is None:\n",
    "            print(f\"Warning: Could not determine best fit grid for H={original_height}, W={original_width}. Defaulting to base {self.base_image_size}x{self.base_image_size}.\")\n",
    "            best_fit_grid = self.base_grid\n",
    "            \n",
    "        return best_fit_grid\n",
    "\n",
    "    def forward(self,\n",
    "                *args: Any, # Accept positional args for flexibility\n",
    "                pixel_values: Optional[torch.Tensor] = None,\n",
    "                input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                # Add raw_images potentially needed by patcher (extract from kwargs or batch dict)\n",
    "                raw_images: Optional[List[Image.Image]] = None,\n",
    "                **kwargs: Any # Accept keyword args\n",
    "               ) -> CausalLMOutputWithPast: # Return type from transformers\n",
    "        \"\"\"Defines the forward pass of the Adaptive LLaVA model.\n",
    "\n",
    "        Handles input flexibility. If an adaptive patcher is enabled, it's called first\n",
    "        to potentially determine processing strategy metadata. Currently, the image features\n",
    "        used still come from the standard `pixel_values` input, pending full adaptive implementation.\n",
    "\n",
    "        Args:\n",
    "            *args: Positional arguments. If args[0] is a dictionary, it's treated as the batch. If args[0] is a Tensor, it might be pixel_values.\n",
    "            pixel_values (Optional[torch.Tensor]): Tensor of image pixel values.\n",
    "            input_ids (Optional[torch.Tensor]): Tensor of input token IDs, potentially containing image markers (-200).\n",
    "            attention_mask (Optional[torch.Tensor]): Attention mask for input_ids.\n",
    "            labels (Optional[torch.Tensor]): Labels for language modeling loss (shifted internally).\n",
    "            raw_images (Optional[List[PIL.Image.Image]]): List of raw PIL images for the batch, if needed by the patcher.\n",
    "            **kwargs: Keyword arguments, potentially containing the input tensors or 'raw_images'.\n",
    "\n",
    "        Returns:\n",
    "            Output dictionary from the language model (transformers.CausalLMOutputWithPast).\n",
    "        \"\"\"\n",
    "        # --- Input Parsing Logic (Handles *args, **kwargs, dict in args[0], tensor in args[0]) ---\n",
    "        _pixel_values, _input_ids, _attention_mask, _labels, _raw_images = None, None, None, None, None\n",
    "        batch_dict = None\n",
    "\n",
    "        # Case 1: fastai standard case (batch dict in args[0])\n",
    "        if len(args) == 1 and isinstance(args[0], dict):\n",
    "            batch_dict = args[0]\n",
    "            _pixel_values = batch_dict.get('pixel_values')\n",
    "            _input_ids = batch_dict.get('input_ids')\n",
    "            _attention_mask = batch_dict.get('attention_mask')\n",
    "            _labels = batch_dict.get('labels')\n",
    "            _raw_images = batch_dict.get('raw_images') # Try getting raw_images from dict\n",
    "        # Case 2: Try kwargs next\n",
    "        elif kwargs:\n",
    "            _pixel_values = kwargs.get('pixel_values', pixel_values)\n",
    "            _input_ids = kwargs.get('input_ids', input_ids)\n",
    "            _attention_mask = kwargs.get('attention_mask', attention_mask)\n",
    "            _labels = kwargs.get('labels', labels)\n",
    "            _raw_images = kwargs.get('raw_images', raw_images) # Get raw_images from kwargs\n",
    "        # Case 3: Check if only pixel_values was passed positionally (e.g., summary)\n",
    "        elif len(args) == 1 and isinstance(args[0], torch.Tensor) and pixel_values is None:\n",
    "             _pixel_values = args[0]\n",
    "             # Attempt to get others from formal params (likely None)\n",
    "             _input_ids = input_ids\n",
    "             _attention_mask = attention_mask\n",
    "             _labels = labels\n",
    "             _raw_images = raw_images # Get raw_images from formal params\n",
    "        # Case 4: Fallback to formal parameters if nothing else worked\n",
    "        else:\n",
    "             _pixel_values = pixel_values\n",
    "             _input_ids = input_ids\n",
    "             _attention_mask = attention_mask\n",
    "             _labels = labels\n",
    "             _raw_images = raw_images\n",
    "\n",
    "        # --- Handle learner.summary() potentially needing dummy text inputs ---\n",
    "        if _pixel_values is not None and _input_ids is None:\n",
    "            warnings.warn(\"Adaptive forward() received pixel_values but not input_ids after parsing. \"\n",
    "                          \"Creating dummy text inputs (likely for learner.summary()).\", UserWarning)\n",
    "            batch_size = _pixel_values.shape[0]\n",
    "            dummy_seq_len = 1\n",
    "            target_device = _pixel_values.device\n",
    "            if tokenizer is None: raise RuntimeError(\"Tokenizer not available for dummy inputs.\")\n",
    "\n",
    "            _input_ids = torch.full(\n",
    "                (batch_size, dummy_seq_len),\n",
    "                tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0,\n",
    "                dtype=torch.long,\n",
    "                device=target_device\n",
    "            )\n",
    "            _input_ids[:, 0] = self.image_token_index_marker\n",
    "            _attention_mask = torch.ones_like(_input_ids)\n",
    "            _labels = None\n",
    "            _raw_images = None # No raw images needed/available for dummy case\n",
    "\n",
    "        # --- Final check for essential inputs ---\n",
    "        if _pixel_values is None or _input_ids is None:\n",
    "            err_msg_parts = [\"Adaptive forward() missing required arguments after parsing: pixel_values and input_ids must be provided.\"]\n",
    "            # ... (add more details as needed) ...\n",
    "            raise ValueError(\"\\n\".join(err_msg_parts))\n",
    "\n",
    "        # Use resolved values from here on\n",
    "        pixel_values, input_ids, attention_mask, labels, raw_images = _pixel_values, _input_ids, _attention_mask, _labels, _raw_images\n",
    "        # --- End Input Parsing & Dummy Input Handling ---\n",
    "\n",
    "\n",
    "        # --- Patcher Logic (Get Metadata) ---\n",
    "        patcher_metadata_batch = [None] * pixel_values.shape[0] # Initialize metadata list\n",
    "        if self.patcher is not None:\n",
    "            # Pass necessary inputs to the patcher\n",
    "            # Currently VariableResolutionPatcher only needs raw_image if not forced baseline\n",
    "            requires_raw = not getattr(self.patcher, 'force_baseline', False)\n",
    "            if requires_raw and (raw_images is None or len(raw_images) != pixel_values.shape[0]):\n",
    "                 warnings.warn(f\"Patcher '{self.patcher.__class__.__name__}' needs raw_images, but they were not provided or length mismatch.\", UserWarning)\n",
    "                 # Decide behavior: Proceed without metadata, or raise error? For now, proceed.\n",
    "            elif requires_raw or getattr(self.patcher, 'force_baseline', False): # Run if raw needed or baseline forced\n",
    "                for i in range(pixel_values.shape[0]):\n",
    "                    try:\n",
    "                        current_raw_image = raw_images[i] if raw_images else None\n",
    "                        # Pass only the necessary inputs to the patcher's forward\n",
    "                        _, patcher_metadata_sample = self.patcher(raw_image=current_raw_image)\n",
    "                        patcher_metadata_batch[i] = patcher_metadata_sample\n",
    "                    except Exception as e:\n",
    "                        warnings.warn(f\"Error running patcher for batch index {i}: {e}\")\n",
    "                self.current_patcher_metadata = patcher_metadata_batch # Store for inspection\n",
    "        # --- End Patcher Logic ---\n",
    "\n",
    "        # --- Feature Processing (Currently Still Baseline Behavior) ---\n",
    "        # TODO: Modify this section based on `patcher_metadata_batch`.\n",
    "        # This currently ignores the patcher output metadata and uses baseline features.\n",
    "\n",
    "        if self.language_model is None or self.vision_tower is None or self.projector is None:\n",
    "            raise RuntimeError(\"Model components (LLM, Vision Tower, Projector) are not fully loaded.\")\n",
    "\n",
    "        image_features = self.encode_image(pixel_values) # (B, P_base, D_clip)\n",
    "        if image_features is None:\n",
    "            raise RuntimeError(\"Image encoding failed.\")\n",
    "\n",
    "        projector_device = next(self.projector.parameters()).device\n",
    "        projector_dtype = next(self.projector.parameters()).dtype\n",
    "        image_features = image_features.to(projector_device, dtype=projector_dtype)\n",
    "        projected_image_features = self.projector(image_features) # (B, P_base, D_llm)\n",
    "        num_image_patches = projected_image_features.shape[1]\n",
    "\n",
    "        # --- Prepare LLM inputs (Same as baseline) ---\n",
    "        embedding_layer = self.get_input_embeddings()\n",
    "        target_device = embedding_layer.weight.device\n",
    "        target_dtype = embedding_layer.weight.dtype\n",
    "\n",
    "        input_ids_clone = input_ids.clone().to(target_device)\n",
    "        input_ids_clone[input_ids_clone == self.image_token_index_marker] = 0\n",
    "\n",
    "        text_embeddings = embedding_layer(input_ids_clone)\n",
    "        projected_image_features = projected_image_features.to(target_device, dtype=target_dtype)\n",
    "\n",
    "        new_input_embeds = []\n",
    "        new_labels = [] if labels is not None else None\n",
    "        new_attention_mask = []\n",
    "\n",
    "        for batch_idx in range(input_ids.shape[0]):\n",
    "            current_input_ids_slice = input_ids[batch_idx].to(target_device)\n",
    "            image_token_indices = torch.where(current_input_ids_slice == self.image_token_index_marker)[0]\n",
    "\n",
    "            if len(image_token_indices) == 0:\n",
    "                warnings.warn(f\"Image token placeholder {self.image_token_index_marker} not found in batch index {batch_idx}. Using text embeddings only.\", UserWarning)\n",
    "                new_input_embeds.append(text_embeddings[batch_idx])\n",
    "                current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()\n",
    "                new_attention_mask.append(current_attention_mask_slice)\n",
    "                if new_labels is not None and labels is not None:\n",
    "                    new_labels.append(labels[batch_idx].to(target_device))\n",
    "                continue\n",
    "\n",
    "            image_token_start_index = image_token_indices[0].item()\n",
    "            text_emb_before = text_embeddings[batch_idx, :image_token_start_index]\n",
    "            text_emb_after = text_embeddings[batch_idx, image_token_start_index + 1:]\n",
    "\n",
    "            cur_new_embed = torch.cat([\n",
    "                text_emb_before,\n",
    "                projected_image_features[batch_idx],\n",
    "                text_emb_after\n",
    "            ], dim=0)\n",
    "            new_input_embeds.append(cur_new_embed)\n",
    "\n",
    "            current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()\n",
    "            mask_before = current_attention_mask_slice[:image_token_start_index]\n",
    "            mask_image = torch.ones(num_image_patches, dtype=torch.long, device=target_device)\n",
    "            mask_after = current_attention_mask_slice[image_token_start_index + 1:]\n",
    "            cur_new_mask = torch.cat([mask_before, mask_image, mask_after], dim=0)\n",
    "            new_attention_mask.append(cur_new_mask)\n",
    "\n",
    "            if new_labels is not None and labels is not None:\n",
    "                current_labels_slice = labels[batch_idx].to(target_device)\n",
    "                label_before = current_labels_slice[:image_token_start_index]\n",
    "                label_image = torch.full((num_image_patches,), self.ignore_index, dtype=torch.long, device=target_device)\n",
    "                label_after = current_labels_slice[image_token_start_index + 1:]\n",
    "                cur_new_label = torch.cat([label_before, label_image, label_after], dim=0)\n",
    "                new_labels.append(cur_new_label)\n",
    "\n",
    "        # --- Padding (Same as baseline) ---\n",
    "        padded_input_embeds = pad_sequence(new_input_embeds, batch_first=True, padding_value=0.0)\n",
    "        padded_attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)\n",
    "        padded_labels = None\n",
    "        if new_labels is not None and len(new_labels) > 0:\n",
    "            padded_labels = pad_sequence(new_labels, batch_first=True, padding_value=self.ignore_index)\n",
    "\n",
    "        # --- Pass to LLM (Same as baseline) ---\n",
    "        outputs: CausalLMOutputWithPast = self.language_model(\n",
    "            inputs_embeds=padded_input_embeds,\n",
    "            attention_mask=padded_attention_mask,\n",
    "            labels=padded_labels, # Pass potentially None labels\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### VariableResolutionPatcher\n",
       "\n",
       ">      VariableResolutionPatcher (config:Dict[str,Any])\n",
       "\n",
       "*Adaptive patcher that selects an optimal grid resolution based on image aspect ratio,\n",
       "with an option to force baseline behavior for ablation studies.\n",
       "\n",
       "Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
       "dimensions but does not perform the actual image processing or feature extraction.\n",
       "The main model's forward pass should use the output metadata to handle the image.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### VariableResolutionPatcher\n",
       "\n",
       ">      VariableResolutionPatcher (config:Dict[str,Any])\n",
       "\n",
       "*Adaptive patcher that selects an optimal grid resolution based on image aspect ratio,\n",
       "with an option to force baseline behavior for ablation studies.\n",
       "\n",
       "Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
       "dimensions but does not perform the actual image processing or feature extraction.\n",
       "The main model's forward pass should use the output metadata to handle the image.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(VariableResolutionPatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized VariableResolutionPatcher with grid options: [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008], [336, 336]]\n",
      "Initialized VariableResolutionPatcher: Ablation active - FORCING BASELINE grid (336x336).\n",
      "--- Normal Mode --- \n",
      "Image (600, 600), Ratio 1.00 -> Selected Grid: (336, 336), Strategy: variable_resolution\n",
      "Image (400, 800), Ratio 0.50 -> Selected Grid: (336, 672), Strategy: variable_resolution\n",
      "--- Forced Baseline Mode --- \n",
      "Image (600, 600), Ratio 1.00 -> Selected Grid: (336, 336), Strategy: forced_baseline\n",
      "Image (400, 800), Ratio 0.50 -> Selected Grid: (336, 336), Strategy: forced_baseline\n",
      "Variable Resolution Patcher tests passed.\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "import PIL.Image\n",
    "\n",
    "# Create dummy config\n",
    "test_config = {\n",
    "    'model': {\n",
    "        'adaptive_patcher': {\n",
    "             'image_grid_pinpoints': [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]\n",
    "        },\n",
    "        'vision_config': {\n",
    "            'image_size': 336,\n",
    "            'patch_size': 14\n",
    "        }\n",
    "    },\n",
    "    'ablation': { # Add ablation config\n",
    "        'force_patcher_strategy': None # Default: no forcing\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Test Normal Operation --- \n",
    "patcher_normal = VariableResolutionPatcher(test_config)\n",
    "test_img_square = PIL.Image.new('RGB', (600, 600))\n",
    "test_img_tall = PIL.Image.new('RGB', (400, 800))\n",
    "\n",
    "_, meta_sq = patcher_normal(raw_image=test_img_square)\n",
    "_, meta_tall = patcher_normal(raw_image=test_img_tall)\n",
    "print(\"--- Normal Mode --- \")\n",
    "print(f\"Image (600, 600), Ratio 1.00 -> Selected Grid: {meta_sq['selected_grid']}, Strategy: {meta_sq['strategy']}\")\n",
    "print(f\"Image (400, 800), Ratio 0.50 -> Selected Grid: {meta_tall['selected_grid']}, Strategy: {meta_tall['strategy']}\")\n",
    "assert meta_sq['selected_grid'] == (336, 336)\n",
    "assert meta_tall['selected_grid'] == (336, 672)\n",
    "assert meta_sq['strategy'] == 'variable_resolution'\n",
    "\n",
    "# --- Test Forced Baseline --- \n",
    "test_config_forced = copy.deepcopy(test_config)\n",
    "test_config_forced['ablation']['force_patcher_strategy'] = 'baseline'\n",
    "patcher_forced = VariableResolutionPatcher(test_config_forced)\n",
    "\n",
    "_, meta_sq_forced = patcher_forced(raw_image=test_img_square)\n",
    "_, meta_tall_forced = patcher_forced(raw_image=test_img_tall)\n",
    "\n",
    "print(\"--- Forced Baseline Mode --- \")\n",
    "print(f\"Image (600, 600), Ratio 1.00 -> Selected Grid: {meta_sq_forced['selected_grid']}, Strategy: {meta_sq_forced['strategy']}\")\n",
    "print(f\"Image (400, 800), Ratio 0.50 -> Selected Grid: {meta_tall_forced['selected_grid']}, Strategy: {meta_tall_forced['strategy']}\")\n",
    "assert meta_sq_forced['selected_grid'] == (336, 336)\n",
    "assert meta_tall_forced['selected_grid'] == (336, 336)\n",
    "assert meta_sq_forced['strategy'] == 'forced_baseline'\n",
    "\n",
    "# Test ValueError if raw_image is None and not forced\n",
    "try:\n",
    "    patcher_normal(raw_image=None)\n",
    "    assert False, \"Should have raised ValueError when raw_image is None (normal mode)\"\n",
    "except ValueError:\n",
    "    pass # Expected\n",
    "\n",
    "# Test ValueError is NOT raised if raw_image is None and forced\n",
    "try:\n",
    "    patcher_forced(raw_image=None) # Should work, doesn't need raw_image\n",
    "except ValueError:\n",
    "     assert False, \"Should NOT have raised ValueError when raw_image is None (forced baseline mode)\"\n",
    "\n",
    "print(\"Variable Resolution Patcher tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.4: Define Adaptive LLaVA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves creating `AdaptiveLLaVAModel` inheriting from `BaselineLLaVAModel` and integrating the chosen `AdaptivePatcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Placeholder for other potential patcher implementations\n",
    "# class PredictorPatcher(AdaptivePatcher): ...\n",
    "# class TextGuidedPatcher(AdaptivePatcher): ...\n",
    "\n",
    "# Mapping from strategy name in config to Patcher class\n",
    "PATCHER_STRATEGIES = {\n",
    "    'variable_resolution': VariableResolutionPatcher,\n",
    "    # 'predictor': PredictorPatcher, # Add when implemented\n",
    "    # 'text_guided': TextGuidedPatcher, # Add when implemented\n",
    "}\n",
    "\n",
    "class AdaptiveLLaVAModel(BaselineLLaVAModel):\n",
    "    \"\"\"LLaVA Model extended with an Adaptive Patcher module.\n",
    "\n",
    "    Inherits from BaselineLLaVAModel and adds an adaptive patcher component\n",
    "    based on the configuration. The forward pass needs to be overridden\n",
    "    to incorporate the patcher's logic.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the Adaptive LLaVA model.\n",
    "\n",
    "        Loads baseline components and instantiates the specified adaptive patcher.\n",
    "\n",
    "        Args:\n",
    "            config: The main configuration dictionary.\n",
    "        \"\"\"\n",
    "        # Initialize baseline components (Vision Tower, LLM, Projector)\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.patcher = None\n",
    "        patcher_config = self.config.get('model', {}).get('adaptive_patcher', {})\n",
    "        patcher_enabled = patcher_config.get('enabled', False)\n",
    "        patcher_strategy = patcher_config.get('strategy')\n",
    "\n",
    "        if patcher_enabled and patcher_strategy:\n",
    "            if patcher_strategy in PATCHER_STRATEGIES:\n",
    "                PatcherClass = PATCHER_STRATEGIES[patcher_strategy]\n",
    "                try:\n",
    "                    # Pass the full config, including ablation settings\n",
    "                    self.patcher = PatcherClass(config)\n",
    "                    print(f\"Adaptive Patcher enabled with strategy: '{patcher_strategy}' ({PatcherClass.__name__})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error initializing patcher '{patcher_strategy}': {e}. Disabling patcher.\")\n",
    "                    self.patcher = None\n",
    "            else:\n",
    "                print(f\"Warning: Unknown adaptive patcher strategy '{patcher_strategy}'. Disabling patcher.\")\n",
    "                self.patcher = None\n",
    "        else:\n",
    "            print(\"Adaptive Patcher is disabled in the configuration.\")\n",
    "\n",
    "    # --- Step 6.5: Implement Adaptive Forward Pass ---\n",
    "    # Override the forward pass to integrate the patcher logic\n",
    "    def forward(self,\n",
    "                *args: Any, # Accept positional args for flexibility\n",
    "                pixel_values: Optional[torch.Tensor] = None,\n",
    "                input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                # Add raw_images potentially needed by patcher (extract from kwargs or batch dict)\n",
    "                raw_images: Optional[List[Image.Image]] = None,\n",
    "                **kwargs: Any # Accept keyword args\n",
    "               ) -> CausalLMOutputWithPast: # Return type from transformers\n",
    "        \"\"\"Defines the forward pass of the Adaptive LLaVA model.\n",
    "\n",
    "        If an adaptive patcher is enabled, it's called to determine patching strategy\n",
    "        (metadata stored). The actual image features used currently come from the standard\n",
    "        `pixel_values` input (base resolution), regardless of patcher output. This allows\n",
    "        the structural integration without implementing complex variable feature handling yet.\n",
    "\n",
    "        Handles input flexibility for compatibility with fastai Learner.\n",
    "\n",
    "        Args:\n",
    "            *args: Positional arguments. If args[0] is a dictionary, it's treated as the batch. If args[0] is a Tensor, it might be pixel_values.\n",
    "            pixel_values (Optional[torch.Tensor]): Tensor of image pixel values.\n",
    "            input_ids (Optional[torch.Tensor]): Tensor of input token IDs, potentially containing image markers (-200).\n",
    "            attention_mask (Optional[torch.Tensor]): Attention mask for input_ids.\n",
    "            labels (Optional[torch.Tensor]): Labels for language modeling loss (shifted internally).\n",
    "            raw_images (Optional[List[PIL.Image.Image]]): List of raw PIL images for the batch, if needed by the patcher.\n",
    "            **kwargs: Keyword arguments, potentially containing the input tensors or 'raw_images'.\n",
    "\n",
    "        Returns:\n",
    "            Output dictionary from the language model (transformers.CausalLMOutputWithPast).\n",
    "        \"\"\"\n",
    "        # --- Input Parsing Logic (Handles *args, **kwargs, dict in args[0], tensor in args[0]) ---\n",
    "        _pixel_values, _input_ids, _attention_mask, _labels, _raw_images = None, None, None, None, None\n",
    "        batch_dict = {}\n",
    "\n",
    "        if len(args) == 1 and isinstance(args[0], dict):\n",
    "            batch_dict = args[0]\n",
    "            _pixel_values = batch_dict.get('pixel_values')\n",
    "            _input_ids = batch_dict.get('input_ids')\n",
    "            _attention_mask = batch_dict.get('attention_mask')\n",
    "            _labels = batch_dict.get('labels')\n",
    "            _raw_images = batch_dict.get('raw_images')\n",
    "        elif len(args) == 1 and isinstance(args[0], torch.Tensor):\n",
    "            _pixel_values = args[0]\n",
    "        \n",
    "        # Prioritize kwargs, then formal params if not found elsewhere\n",
    "        if _pixel_values is None: _pixel_values = kwargs.get('pixel_values', pixel_values)\n",
    "        if _input_ids is None: _input_ids = kwargs.get('input_ids', input_ids)\n",
    "        if _attention_mask is None: _attention_mask = kwargs.get('attention_mask', attention_mask)\n",
    "        if _labels is None: _labels = kwargs.get('labels', labels)\n",
    "        if _raw_images is None: _raw_images = kwargs.get('raw_images', raw_images)\n",
    "\n",
    "        # --- Handle learner.summary() case: Create dummy text inputs ---\n",
    "        is_summary_call = (_pixel_values is not None and _input_ids is None)\n",
    "        if is_summary_call:\n",
    "            # print(\"Warning: input_ids not provided, creating dummy inputs for summary/tracing.\") # Optional warning\n",
    "            batch_size = _pixel_values.shape[0]\n",
    "            dummy_seq_len = 1 # Minimal sequence length\n",
    "            target_device = _pixel_values.device \n",
    "            if tokenizer is None: raise RuntimeError(\"Tokenizer not available to create dummy inputs.\")\n",
    "            \n",
    "            _input_ids = torch.full(\n",
    "                (batch_size, dummy_seq_len),\n",
    "                tokenizer.pad_token_id,\n",
    "                dtype=torch.long,\n",
    "                device=target_device\n",
    "            )\n",
    "            _attention_mask = torch.zeros_like(_input_ids)\n",
    "            _labels = None\n",
    "        \n",
    "        # --- Final check for essential inputs ---\n",
    "        if _pixel_values is None or _input_ids is None:\n",
    "            err_msg_parts = [\"Adaptive forward() missing required arguments: pixel_values and input_ids must be provided.\"]\n",
    "            err_msg_parts.append(f\"  Resolved pixel_values is None: {_pixel_values is None}\")\n",
    "            err_msg_parts.append(f\"  Resolved input_ids is None: {_input_ids is None}\")\n",
    "            err_msg_parts.append(f\"  len(args): {len(args)}, type(args[0]) if args else 'N/A': {type(args[0]) if args else 'N/A'}\")\n",
    "            err_msg_parts.append(f\"  kwargs keys: {list(kwargs.keys())}\")\n",
    "            raise ValueError(\"\\n\".join(err_msg_parts))\n",
    "\n",
    "        # Use resolved values from here on\n",
    "        pixel_values, input_ids, attention_mask, labels, raw_images = _pixel_values, _input_ids, _attention_mask, _labels, _raw_images\n",
    "        # --- End Input Parsing & Dummy Input Handling ---\n",
    "\n",
    "\n",
    "        # --- Patcher Logic (Get Metadata) ---\n",
    "        patcher_metadata_batch = [None] * pixel_values.shape[0] # Initialize metadata list\n",
    "        if self.patcher is not None:\n",
    "            requires_raw = not getattr(self.patcher, 'force_baseline', False)\n",
    "            if requires_raw and (raw_images is None or len(raw_images) != pixel_values.shape[0]):\n",
    "                 warnings.warn(f\"Patcher '{self.patcher.__class__.__name__}' is active and needs raw_images, but they were not provided or length mismatch. Patcher cannot run effectively.\", UserWarning)\n",
    "                 # Proceed without patcher metadata if raw images are missing when needed\n",
    "            else:\n",
    "                # Iterate through batch to get metadata for each sample\n",
    "                for i in range(pixel_values.shape[0]):\n",
    "                    try:\n",
    "                        current_raw_image = raw_images[i] if raw_images else None\n",
    "                        _, patcher_metadata_sample = self.patcher(raw_image=current_raw_image)\n",
    "                        patcher_metadata_batch[i] = patcher_metadata_sample\n",
    "                    except Exception as e:\n",
    "                        warnings.warn(f\"Error running patcher for batch index {i}: {e}\")\n",
    "                self.current_patcher_metadata = patcher_metadata_batch # Store for potential inspection/logging\n",
    "\n",
    "        # --- Feature Processing (Currently Still Baseline Behavior) ---\n",
    "        # TODO: Modify this section based on `patcher_metadata_batch`.\n",
    "        # For now, it proceeds exactly like the baseline model using the standard pixel_values input.\n",
    "\n",
    "        if self.language_model is None or self.vision_tower is None or self.projector is None:\n",
    "            raise RuntimeError(\"Model components (LLM, Vision Tower, Projector) are not fully loaded.\")\n",
    "\n",
    "        image_features = self.encode_image(pixel_values) # (B, P_base, D_clip)\n",
    "        if image_features is None:\n",
    "            raise RuntimeError(\"Image encoding failed.\")\n",
    "\n",
    "        projector_device = next(self.projector.parameters()).device\n",
    "        image_features = image_features.to(projector_device)\n",
    "        projected_image_features = self.projector(image_features) # (B, P_base, D_llm)\n",
    "        num_image_patches = projected_image_features.shape[1]\n",
    "\n",
    "        # --- Prepare LLM inputs (Same as baseline) ---\n",
    "        embedding_layer = self.get_input_embeddings()\n",
    "        target_device = embedding_layer.weight.device\n",
    "\n",
    "        input_ids_clone = input_ids.clone().to(target_device)\n",
    "        input_ids_clone[input_ids_clone == self.image_token_index_marker] = 0\n",
    "\n",
    "        text_embeddings = embedding_layer(input_ids_clone)\n",
    "        projected_image_features = projected_image_features.to(target_device, dtype=text_embeddings.dtype)\n",
    "\n",
    "        new_input_embeds = []\n",
    "        new_labels = [] if labels is not None else None\n",
    "        new_attention_mask = []\n",
    "\n",
    "        for batch_idx in range(input_ids.shape[0]):\n",
    "            current_input_ids_slice = input_ids[batch_idx].to(target_device)\n",
    "            image_token_indices = torch.where(current_input_ids_slice == self.image_token_index_marker)[0]\n",
    "\n",
    "            if len(image_token_indices) == 0:\n",
    "                # If using dummy inputs, image marker won't be found, handle gracefully.\n",
    "                if is_summary_call:\n",
    "                    new_input_embeds.append(text_embeddings[batch_idx])\n",
    "                    current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else torch.zeros_like(current_input_ids_slice)\n",
    "                    new_attention_mask.append(current_attention_mask_slice)\n",
    "                else: # Normal operation, warn if marker is missing\n",
    "                    warnings.warn(f\"Image token placeholder {self.image_token_index_marker} not found in batch index {batch_idx}. Skipping image features.\")\n",
    "                    new_input_embeds.append(text_embeddings[batch_idx])\n",
    "                    current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()\n",
    "                    new_attention_mask.append(current_attention_mask_slice)\n",
    "                    if new_labels is not None and labels is not None:\n",
    "                        new_labels.append(labels[batch_idx].to(target_device))\n",
    "                continue # Move to next item in batch\n",
    "\n",
    "            image_token_start_index = image_token_indices[0].item()\n",
    "            # Ensure indices are valid before slicing\n",
    "            if image_token_start_index >= text_embeddings.shape[1]:\n",
    "                 warnings.warn(f\"Calculated image_token_start_index {image_token_start_index} is out of bounds for text_embeddings shape {text_embeddings.shape}. Skipping.\")\n",
    "                 new_input_embeds.append(text_embeddings[batch_idx])\n",
    "                 new_attention_mask.append(attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long())\n",
    "                 if new_labels is not None and labels is not None: new_labels.append(labels[batch_idx].to(target_device))\n",
    "                 continue\n",
    "\n",
    "\n",
    "            text_emb_before = text_embeddings[batch_idx, :image_token_start_index]\n",
    "            text_emb_after = text_embeddings[batch_idx, image_token_start_index + 1:]\n",
    "\n",
    "            cur_new_embed = torch.cat([\n",
    "                text_emb_before,\n",
    "                projected_image_features[batch_idx],\n",
    "                text_emb_after\n",
    "            ], dim=0)\n",
    "            new_input_embeds.append(cur_new_embed)\n",
    "\n",
    "            current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()\n",
    "            mask_before = current_attention_mask_slice[:image_token_start_index]\n",
    "            mask_image = torch.ones(num_image_patches, dtype=torch.long, device=target_device)\n",
    "            mask_after = current_attention_mask_slice[image_token_start_index + 1:]\n",
    "            cur_new_mask = torch.cat([mask_before, mask_image, mask_after], dim=0)\n",
    "            new_attention_mask.append(cur_new_mask)\n",
    "\n",
    "            if new_labels is not None and labels is not None:\n",
    "                current_labels_slice = labels[batch_idx].to(target_device)\n",
    "                label_before = current_labels_slice[:image_token_start_index]\n",
    "                label_image = torch.full((num_image_patches,), self.ignore_index, dtype=torch.long, device=target_device)\n",
    "                label_after = current_labels_slice[image_token_start_index + 1:]\n",
    "                cur_new_label = torch.cat([label_before, label_image, label_after], dim=0)\n",
    "                new_labels.append(cur_new_label)\n",
    "\n",
    "        # --- Padding (Same as baseline) ---\n",
    "        padded_input_embeds = pad_sequence(new_input_embeds, batch_first=True, padding_value=0.0)\n",
    "        padded_attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)\n",
    "        padded_labels = None\n",
    "        if new_labels is not None:\n",
    "            padded_labels = pad_sequence(new_labels, batch_first=True, padding_value=self.ignore_index)\n",
    "\n",
    "        # --- Pass to LLM (Same as baseline) ---\n",
    "        outputs: CausalLMOutputWithPast = self.language_model(\n",
    "            inputs_embeds=padded_input_embeds,\n",
    "            attention_mask=padded_attention_mask,\n",
    "            labels=padded_labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AdaptiveLLaVAModel\n",
       "\n",
       ">      AdaptiveLLaVAModel (config:Dict[str,Any])\n",
       "\n",
       "*LLaVA Model extended with an Adaptive Patcher module.\n",
       "\n",
       "    Inherits from BaselineLLaVAModel and adds an adaptive patcher component\n",
       "    based on the configuration. The forward pass needs to be overridden\n",
       "    to incorporate the patcher's logic.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AdaptiveLLaVAModel\n",
       "\n",
       ">      AdaptiveLLaVAModel (config:Dict[str,Any])\n",
       "\n",
       "*LLaVA Model extended with an Adaptive Patcher module.\n",
       "\n",
       "    Inherits from BaselineLLaVAModel and adds an adaptive patcher component\n",
       "    based on the configuration. The forward pass needs to be overridden\n",
       "    to incorporate the patcher's logic.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AdaptiveLLaVAModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b221b591d04f9c91c856a5b84aa16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n",
      "LoRA is disabled in the configuration.\n",
      "LLM embedding size already matches tokenizer size. No resizing needed.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Adaptive Patcher enabled with strategy: 'variable_resolution' (VariableResolutionPatcher)\n",
      "Initialized VariableResolutionPatcher: Ablation active - FORCING BASELINE grid (336x336).\n",
      "Running Adaptive Forward Pass Test (Forced Baseline Ablation)...\n",
      "Output logits shape: torch.Size([2, 586, 32001])\n",
      "Output loss: tensor(10.4655, grad_fn=<NllLossBackward0>)\n",
      "Adaptive Forward Pass test successful!\n",
      "Cleaned up test_adaptive_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_140/4271200700.py:121: UserWarning: Patcher is active and needs raw_images, but they were not provided or length mismatch. Patcher cannot run.\n",
      "  warnings.warn(\"Patcher is active and needs raw_images, but they were not provided or length mismatch. Patcher cannot run.\")\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "import torch, gc\n",
    "from transformers import AutoModelForCausalLM, CLIPVisionModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    config_path = '../configs/config.yaml'\n",
    "    test_config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "\n",
    "    # --- Test with Adaptive Patcher Enabled (Forced Baseline) --- \n",
    "    if 'model' not in test_config: test_config['model'] = {}\n",
    "    if 'adaptive_patcher' not in test_config['model']: test_config['model']['adaptive_patcher'] = {}\n",
    "    test_config['model']['adaptive_patcher']['enabled'] = True\n",
    "    test_config['model']['adaptive_patcher']['strategy'] = 'variable_resolution'\n",
    "    # Add ablation setting\n",
    "    if 'ablation' not in test_config: test_config['ablation'] = {}\n",
    "    test_config['ablation']['force_patcher_strategy'] = 'baseline' # Force baseline for this test\n",
    "    \n",
    "    # Disable LoRA and Checkpointing for simplicity\n",
    "    if 'peft' not in test_config['model']: test_config['model']['peft'] = {}\n",
    "    test_config['model']['peft']['use_lora'] = False \n",
    "    test_config['model']['use_activation_checkpointing'] = False\n",
    "\n",
    "    # Instantiate the adaptive model\n",
    "    test_adaptive_model = AdaptiveLLaVAModel(test_config)\n",
    "    test_adaptive_model.eval() # Set to eval mode\n",
    "\n",
    "    print(\"Running Adaptive Forward Pass Test (Forced Baseline Ablation)...\")\n",
    "    # Prepare dummy inputs\n",
    "    batch_size = 2\n",
    "    seq_len = 15 # Short sequence for testing\n",
    "    img_size = 336 # From config\n",
    "    num_patches = 576 # (336/14)^2 - Base patches\n",
    "    llm_hidden_dim = test_config['model']['projector']['output_dim']\n",
    "    tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "    dummy_pixel_values = torch.randn(batch_size, 3, img_size, img_size)\n",
    "    dummy_input_ids = torch.randint(1, tokenizer_vocab_size, (batch_size, seq_len), dtype=torch.long)\n",
    "    placeholder_idx = 5 \n",
    "    dummy_input_ids[:, placeholder_idx] = IMAGE_TOKEN_INDEX_PLACEHOLDER\n",
    "    dummy_attention_mask = torch.ones_like(dummy_input_ids)\n",
    "    dummy_labels = dummy_input_ids.clone()\n",
    "    dummy_labels[:, :placeholder_idx+1] = IGNORE_INDEX\n",
    "    \n",
    "    # Raw images are not needed when forcing baseline, but pass None to test handling\n",
    "    dummy_raw_images = None \n",
    "\n",
    "    # Perform forward pass\n",
    "    with torch.no_grad(): \n",
    "         outputs = test_adaptive_model(\n",
    "            pixel_values=dummy_pixel_values,\n",
    "            input_ids=dummy_input_ids,\n",
    "            attention_mask=dummy_attention_mask,\n",
    "            labels=dummy_labels,\n",
    "            raw_images=dummy_raw_images # Pass None \n",
    "         )\n",
    "\n",
    "    # Check output type and attributes\n",
    "    assert isinstance(outputs, CausalLMOutputWithPast)\n",
    "    assert outputs.logits is not None\n",
    "    assert outputs.loss is not None\n",
    "\n",
    "    # Check output shapes (should match baseline)\n",
    "    expected_seq_len = seq_len - 1 + num_patches\n",
    "    expected_logits_shape = (batch_size, expected_seq_len, tokenizer_vocab_size)\n",
    "\n",
    "    assert outputs.logits.shape == expected_logits_shape, \\\n",
    "        f\"Expected logits shape {expected_logits_shape}, but got {outputs.logits.shape}\"\n",
    "    print(f\"Output logits shape: {outputs.logits.shape}\")\n",
    "    print(f\"Output loss: {outputs.loss}\")\n",
    "    \n",
    "    # Check if patcher metadata reflects forced baseline\n",
    "    assert hasattr(test_adaptive_model, 'current_patcher_metadata')\n",
    "    assert len(test_adaptive_model.current_patcher_metadata) == batch_size\n",
    "    assert test_adaptive_model.current_patcher_metadata[0]['selected_grid'] == (336, 336) # Forced baseline\n",
    "    assert test_adaptive_model.current_patcher_metadata[0]['strategy'] == 'forced_baseline'\n",
    "    assert test_adaptive_model.current_patcher_metadata[1]['selected_grid'] == (336, 336) # Forced baseline\n",
    "    assert test_adaptive_model.current_patcher_metadata[1]['strategy'] == 'forced_baseline'\n",
    "    \n",
    "    print(\"Adaptive Forward Pass test successful!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file {config_path} not found. Skipping AdaptiveLLaVAModel test.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Skipping test due to ImportError: {e}. (Likely `peft` is missing)\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during AdaptiveLLaVAModel test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Clean up\n",
    "    if 'test_adaptive_model' in locals():\n",
    "        if hasattr(test_adaptive_model, 'vision_tower') and test_adaptive_model.vision_tower is not None:\n",
    "            test_adaptive_model.vision_tower.to('cpu')\n",
    "        if hasattr(test_adaptive_model, 'language_model') and test_adaptive_model.language_model is not None:\n",
    "            test_adaptive_model.language_model.to('cpu')\n",
    "        if hasattr(test_adaptive_model, 'projector') and test_adaptive_model.projector is not None:\n",
    "            test_adaptive_model.projector.to('cpu')\n",
    "        del test_adaptive_model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Cleaned up test_adaptive_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
