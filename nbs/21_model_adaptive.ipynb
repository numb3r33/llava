{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Model Components\n",
    "\n",
    "> Defines adaptive components for the LLaVA model, starting with the Adaptive Patcher interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.adaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    # print(f\"Project root already in sys.path: {project_root_str}\") # Less verbose\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n",
      "Loaded config from configs/config.yaml\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pad_sequence # For padding variable length sequences\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast # For type hints\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Import base model and utilities\n",
    "from llava.model.baseline import BaselineLLaVAModel\n",
    "from llava.utils import load_config\n",
    "from llava.data.preprocessing import tokenizer, IMAGE_TOKEN_INDEX_PLACEHOLDER, IGNORE_INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.1: Define Adaptive Patcher Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This base class defines the interface for any adaptive patching strategy. Subclasses will implement specific logic (e.g., variable resolution, attention-based patching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AdaptivePatcher(nn.Module):\n",
    "    \"\"\"Base interface for adaptive image patching modules.\n",
    "\n",
    "    Subclasses should implement the `forward` method to dynamically process\n",
    "    an input image (or its features) based on content or context (like text instructions)\n",
    "    and return a structured representation of image features for the projector.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the Adaptive Patcher.\n",
    "\n",
    "        Args:\n",
    "            config: Dictionary containing configuration relevant to the patcher strategy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Potentially load sub-modules or parameters based on config['strategy']\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None, # Existing features (e.g., from base processing)\n",
    "        text_features: Optional[torch.Tensor] = None, # For text-guided strategies\n",
    "        raw_image: Optional[Image.Image] = None, # Original image for properties like aspect ratio\n",
    "        **kwargs\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[Dict[str, Any]]]:\n",
    "        \"\"\"Processes the input image adaptively or determines processing strategy.\n",
    "\n",
    "        The exact inputs used and outputs returned depend on the specific strategy.\n",
    "        For example, a strategy predictor might only need pooled features,\n",
    "        while a variable resolution strategy primarily needs the raw image aspect ratio.\n",
    "        A text-guided strategy needs text_features and potentially patch features.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Optional preprocessed image tensor (e.g., B x C x H x W).\n",
    "                           Could represent the full image or specific patches.\n",
    "            text_features: Optional tensor containing embeddings of the instruction text.\n",
    "                           Needed for text-guided patching strategies.\n",
    "            raw_image: Optional PIL image, potentially needed for calculating aspect ratio\n",
    "                       or other properties not easily derived from pixel_values alone.\n",
    "            **kwargs: Additional keyword arguments specific to the patching strategy.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - Optional[torch.Tensor]: Processed/selected image features, if the patcher\n",
    "                                    directly outputs features. Shape might vary.\n",
    "                                    Can be None if the patcher only outputs metadata.\n",
    "            - Optional[Dict[str, Any]]: Metadata about the patching process or decision.\n",
    "                                      (e.g., strategy used, number of patches, selected grid).\n",
    "                                      Can be None if no metadata is generated.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This base method must be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement the forward method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AdaptivePatcher\n",
       "\n",
       ">      AdaptivePatcher (config:Dict[str,Any])\n",
       "\n",
       "*Base interface for adaptive image patching modules.\n",
       "\n",
       "    Subclasses should implement the `forward` method to dynamically process\n",
       "    an input image (or its features) based on content or context (like text instructions)\n",
       "    and return a structured representation of image features for the projector.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AdaptivePatcher\n",
       "\n",
       ">      AdaptivePatcher (config:Dict[str,Any])\n",
       "\n",
       "*Base interface for adaptive image patching modules.\n",
       "\n",
       "    Subclasses should implement the `forward` method to dynamically process\n",
       "    an input image (or its features) based on content or context (like text instructions)\n",
       "    and return a structured representation of image features for the projector.*"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AdaptivePatcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.2: Implement Variable Resolution Patcher Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This patcher implements the variable resolution strategy inspired by LLaVA-NeXT. It analyzes the input image's aspect ratio and selects the optimal grid configuration from a predefined set (`image_grid_pinpoints`). The actual image resizing and patching happen elsewhere, based on the grid configuration returned by this patcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VariableResolutionPatcher(AdaptivePatcher):\n",
    "    \"\"\"Adaptive patcher that selects an optimal grid resolution based on image aspect ratio.\n",
    "\n",
    "    Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
    "    dimensions but does not perform the actual image processing or feature extraction.\n",
    "    The main model's forward pass should use the output metadata to handle the image.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the VariableResolutionPatcher.\n",
    "\n",
    "        Args:\n",
    "            config: Dictionary containing configuration. Expected keys:\n",
    "                    'model.adaptive_patcher.image_grid_pinpoints': List of [H, W] grids.\n",
    "                    'model.vision_config.image_size': Base image size (e.g., 336).\n",
    "                    'model.vision_config.patch_size': Patch size (e.g., 14).\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        # Default grid from LLaVA-NeXT / Project Spec\n",
    "        default_grid = [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]\n",
    "        # Retrieve nested config safely\n",
    "        patcher_config = self.config.get('model', {}).get('adaptive_patcher', {})\n",
    "        vision_config = self.config.get('model', {}).get('vision_config', {})\n",
    "        \n",
    "        self.image_grid_pinpoints = patcher_config.get('image_grid_pinpoints', default_grid)\n",
    "        self.base_image_size = vision_config.get('image_size', 336)\n",
    "        self.patch_size = vision_config.get('patch_size', 14)\n",
    "        \n",
    "        # Ensure base resolution is included as an option (implicitly or explicitly)\n",
    "        base_grid = [self.base_image_size, self.base_image_size]\n",
    "        if base_grid not in self.image_grid_pinpoints:\n",
    "             self.image_grid_pinpoints.append(base_grid)\n",
    "        print(f\"Initialized VariableResolutionPatcher with grid options: {self.image_grid_pinpoints}\")\n",
    "\n",
    "    def select_best_resolution(self, original_height: int, original_width: int) -> Tuple[int, int]:\n",
    "        \"\"\"Selects the best grid resolution based on aspect ratio and minimizing waste.\n",
    "\n",
    "        Args:\n",
    "            original_height: Height of the raw input image.\n",
    "            original_width: Width of the raw input image.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: The selected best grid dimensions (height, width).\n",
    "        \"\"\"\n",
    "        original_aspect_ratio = original_height / original_width\n",
    "\n",
    "        best_fit_grid = None\n",
    "        min_wasted_pixels = float('inf')\n",
    "\n",
    "        for grid_h, grid_w in self.image_grid_pinpoints:\n",
    "            grid_aspect_ratio = grid_h / grid_w\n",
    "\n",
    "            # Calculate the dimensions if the image were scaled to fit this grid\n",
    "            # Scale based on the limiting dimension\n",
    "            scale_h = grid_h / original_height\n",
    "            scale_w = grid_w / original_width\n",
    "\n",
    "            if original_aspect_ratio > grid_aspect_ratio: # Scale based on height\n",
    "                scaled_h = grid_h\n",
    "                scaled_w = int(original_width * scale_h)\n",
    "            else: # Scale based on width\n",
    "                scaled_w = grid_w\n",
    "                scaled_h = int(original_height * scale_w)\n",
    "            \n",
    "            # Ensure scaled dimensions do not exceed grid dimensions (due to int conversion)\n",
    "            scaled_h = min(scaled_h, grid_h)\n",
    "            scaled_w = min(scaled_w, grid_w)\n",
    "            \n",
    "            # Calculate wasted area (pixels in the grid not covered by the scaled image)\n",
    "            grid_area = grid_h * grid_w\n",
    "            scaled_image_area = scaled_h * scaled_w\n",
    "            wasted_pixels = grid_area - scaled_image_area\n",
    "\n",
    "            # Prefer grids with less waste\n",
    "            # Among grids with similar waste, the reference code doesn't specify tie-breaking.\n",
    "            # Let's pick the first one encountered with the minimum waste.\n",
    "            # A slightly better tie-breaker might be aspect ratio closeness, but min waste is simpler.\n",
    "            if wasted_pixels < min_wasted_pixels:\n",
    "                min_wasted_pixels = wasted_pixels\n",
    "                best_fit_grid = (grid_h, grid_w)\n",
    "            # Simple tie-breaking: if waste is equal, prefer larger area (less scaling down)\n",
    "            elif wasted_pixels == min_wasted_pixels:\n",
    "                 if best_fit_grid is None or (grid_h * grid_w > best_fit_grid[0] * best_fit_grid[1]):\n",
    "                       best_fit_grid = (grid_h, grid_w)\n",
    "                       \n",
    "        # Fallback to base resolution if something went wrong\n",
    "        if best_fit_grid is None:\n",
    "            print(f\"Warning: Could not determine best fit grid for H={original_height}, W={original_width}. Defaulting to base {self.base_image_size}x{self.base_image_size}.\")\n",
    "            best_fit_grid = (self.base_image_size, self.base_image_size)\n",
    "            \n",
    "        return best_fit_grid\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None, # Not used by this patcher\n",
    "        text_features: Optional[torch.Tensor] = None, # Not used by this patcher\n",
    "        raw_image: Optional[Image.Image] = None,\n",
    "        **kwargs\n",
    "    ) -> Tuple[None, Dict[str, Any]]: # Returns None for features, Dict for metadata\n",
    "        \"\"\"Determines the best grid resolution based on the raw image's aspect ratio.\n",
    "\n",
    "        Args:\n",
    "            raw_image: The original PIL Image object.\n",
    "            pixel_values: Ignored by this patcher.\n",
    "            text_features: Ignored by this patcher.\n",
    "            **kwargs: Ignored.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - None: This patcher does not directly return processed features.\n",
    "            - Dict[str, Any]: Metadata including:\n",
    "                - 'strategy': 'variable_resolution'\n",
    "                - 'selected_grid' (Tuple[int, int]): The chosen grid dimensions (H, W).\n",
    "                - 'num_patches_h' (int): Number of patches vertically in the grid.\n",
    "                - 'num_patches_w' (int): Number of patches horizontally in the grid.\n",
    "                - 'total_patches' (int): Total number of patches in the selected grid.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If raw_image is not provided or patch_size is invalid.\n",
    "        \"\"\"\n",
    "        if raw_image is None:\n",
    "            raise ValueError(\"VariableResolutionPatcher requires the 'raw_image' (PIL Image) input.\")\n",
    "\n",
    "        original_width, original_height = raw_image.size\n",
    "        \n",
    "        # Select the best grid based on aspect ratio\n",
    "        selected_grid_h, selected_grid_w = self.select_best_resolution(original_height, original_width)\n",
    "        \n",
    "        # Calculate number of patches for the selected grid\n",
    "        if self.patch_size <= 0:\n",
    "            raise ValueError(\"Patch size must be positive.\")\n",
    "        num_patches_h = selected_grid_h // self.patch_size\n",
    "        num_patches_w = selected_grid_w // self.patch_size\n",
    "        total_patches = num_patches_h * num_patches_w\n",
    "        \n",
    "        metadata = {\n",
    "            'strategy': 'variable_resolution',\n",
    "            'selected_grid': (selected_grid_h, selected_grid_w),\n",
    "            'num_patches_h': num_patches_h,\n",
    "            'num_patches_w': num_patches_w,\n",
    "            'total_patches': total_patches\n",
    "        }\n",
    "        \n",
    "        # This patcher returns metadata for the main model to use, not processed features.\n",
    "        return None, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### VariableResolutionPatcher\n",
       "\n",
       ">      VariableResolutionPatcher (config:Dict[str,Any])\n",
       "\n",
       "*Adaptive patcher that selects an optimal grid resolution based on image aspect ratio.\n",
       "\n",
       "Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
       "dimensions but does not perform the actual image processing or feature extraction.\n",
       "The main model's forward pass should use the output metadata to handle the image.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### VariableResolutionPatcher\n",
       "\n",
       ">      VariableResolutionPatcher (config:Dict[str,Any])\n",
       "\n",
       "*Adaptive patcher that selects an optimal grid resolution based on image aspect ratio.\n",
       "\n",
       "Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
       "dimensions but does not perform the actual image processing or feature extraction.\n",
       "The main model's forward pass should use the output metadata to handle the image.*"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(VariableResolutionPatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized VariableResolutionPatcher with grid options: [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008], [336, 336]]\n",
      "Image (600, 600), Ratio 1.00 -> Selected Grid: (336, 336), Patches: 24x24=576\n",
      "Image (400, 800), Ratio 0.50 -> Selected Grid: (336, 672), Patches: 24x48=1152\n",
      "Image (800, 400), Ratio 2.00 -> Selected Grid: (672, 336), Patches: 48x24=1152\n",
      "Image (1200, 400), Ratio 3.00 -> Selected Grid: (1008, 336), Patches: 72x24=1728\n",
      "Image (400, 1200), Ratio 0.33 -> Selected Grid: (336, 1008), Patches: 24x72=1728\n",
      "Image (800, 800), Ratio 1.00 -> Selected Grid: (672, 672), Patches: 48x48=2304\n",
      "Variable Resolution Patcher tests passed.\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "import PIL.Image\n",
    "\n",
    "# Create dummy config\n",
    "test_config = {\n",
    "    'model': {\n",
    "        'adaptive_patcher': {\n",
    "             'image_grid_pinpoints': [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]\n",
    "        },\n",
    "        'vision_config': {\n",
    "            'image_size': 336,\n",
    "            'patch_size': 14\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Instantiate the patcher\n",
    "patcher = VariableResolutionPatcher(test_config)\n",
    "\n",
    "# Test cases with different aspect ratios\n",
    "test_images = {\n",
    "    \"square\": PIL.Image.new('RGB', (600, 600)),\n",
    "    \"tall\": PIL.Image.new('RGB', (400, 800)),\n",
    "    \"wide\": PIL.Image.new('RGB', (800, 400)),\n",
    "    \"very_wide\": PIL.Image.new('RGB', (1200, 400)),\n",
    "    \"very_tall\": PIL.Image.new('RGB', (400, 1200)),\n",
    "    \"large_square\": PIL.Image.new('RGB', (800, 800)), # Test selection of larger grids\n",
    "}\n",
    "\n",
    "expected_grids = {\n",
    "    \"square\": (336, 336), # Closest default is base 1:1\n",
    "    \"tall\": (336, 672), # 1:2\n",
    "    \"wide\": (672, 336), # 2:1\n",
    "    \"very_wide\": (1008, 336), # 3:1\n",
    "    \"very_tall\": (336, 1008), # 1:3\n",
    "    \"large_square\": (672, 672), # Prefers larger grid for less scaling if waste is equal\n",
    "}\n",
    "\n",
    "for name, img in test_images.items():\n",
    "    _, metadata = patcher(raw_image=img)\n",
    "    selected_grid = metadata['selected_grid']\n",
    "    total_patches = metadata['total_patches']\n",
    "    expected_grid = expected_grids[name]\n",
    "    w, h = img.size\n",
    "    print(f\"Image ({h}, {w}), Ratio {h/w:.2f} -> Selected Grid: {selected_grid}, Patches: {metadata['num_patches_h']}x{metadata['num_patches_w']}={total_patches}\")\n",
    "    assert selected_grid == expected_grid, f\"Test '{name}' failed. Expected {expected_grid}, got {selected_grid}\"\n",
    "\n",
    "# Test ValueError if raw_image is None\n",
    "try:\n",
    "    patcher(raw_image=None)\n",
    "    assert False, \"Should have raised ValueError when raw_image is None\"\n",
    "except ValueError:\n",
    "    pass # Expected\n",
    "\n",
    "print(\"Variable Resolution Patcher tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.4: Define Adaptive LLaVA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves creating `AdaptiveLLaVAModel` inheriting from `BaselineLLaVAModel` and integrating the chosen `AdaptivePatcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Placeholder for other potential patcher implementations\n",
    "# class PredictorPatcher(AdaptivePatcher): ...\n",
    "# class TextGuidedPatcher(AdaptivePatcher): ...\n",
    "\n",
    "# Mapping from strategy name in config to Patcher class\n",
    "PATCHER_STRATEGIES = {\n",
    "    'variable_resolution': VariableResolutionPatcher,\n",
    "    # 'predictor': PredictorPatcher, # Add when implemented\n",
    "    # 'text_guided': TextGuidedPatcher, # Add when implemented\n",
    "}\n",
    "\n",
    "class AdaptiveLLaVAModel(BaselineLLaVAModel):\n",
    "    \"\"\"LLaVA Model extended with an Adaptive Patcher module.\n",
    "\n",
    "    Inherits from BaselineLLaVAModel and adds an adaptive patcher component\n",
    "    based on the configuration. The forward pass needs to be overridden\n",
    "    to incorporate the patcher's logic.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the Adaptive LLaVA model.\n",
    "\n",
    "        Loads baseline components and instantiates the specified adaptive patcher.\n",
    "\n",
    "        Args:\n",
    "            config: The main configuration dictionary.\n",
    "        \"\"\"\n",
    "        # Initialize baseline components (Vision Tower, LLM, Projector)\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.patcher = None\n",
    "        patcher_config = self.config.get('model', {}).get('adaptive_patcher', {})\n",
    "        patcher_enabled = patcher_config.get('enabled', False)\n",
    "        patcher_strategy = patcher_config.get('strategy')\n",
    "\n",
    "        if patcher_enabled and patcher_strategy:\n",
    "            if patcher_strategy in PATCHER_STRATEGIES:\n",
    "                PatcherClass = PATCHER_STRATEGIES[patcher_strategy]\n",
    "                try:\n",
    "                    self.patcher = PatcherClass(config) # Pass the full config\n",
    "                    print(f\"Adaptive Patcher enabled with strategy: '{patcher_strategy}' ({PatcherClass.__name__})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error initializing patcher '{patcher_strategy}': {e}. Disabling patcher.\")\n",
    "                    self.patcher = None\n",
    "            else:\n",
    "                print(f\"Warning: Unknown adaptive patcher strategy '{patcher_strategy}'. Disabling patcher.\")\n",
    "                self.patcher = None\n",
    "        else:\n",
    "            print(\"Adaptive Patcher is disabled in the configuration.\")\n",
    "\n",
    "    # --- Step 6.5: Implement Adaptive Forward Pass --- \n",
    "    # Override the forward pass to integrate the patcher logic\n",
    "    def forward(self, \n",
    "                pixel_values: torch.Tensor,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None, \n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                # Add raw_images potentially needed by patcher\n",
    "                raw_images: Optional[List[Image.Image]] = None \n",
    "               ) -> CausalLMOutputWithPast: # Return type from transformers\n",
    "        \"\"\"Defines the forward pass of the Adaptive LLaVA model.\n",
    "\n",
    "        If an adaptive patcher is enabled, it's called to determine patching strategy\n",
    "        (metadata stored). The actual image features used currently come from the standard\n",
    "        `pixel_values` input (base resolution), regardless of patcher output. This allows\n",
    "        the structural integration without implementing complex variable feature handling yet.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Tensor of shape (batch_size, C, H, W) for the base resolution (e.g., 336x336).\n",
    "            input_ids: Tensor of shape (batch_size, sequence_length) containing token IDs\n",
    "                       and IMAGE_TOKEN_INDEX_PLACEHOLDER markers (-200).\n",
    "            attention_mask: Optional tensor of shape (batch_size, sequence_length).\n",
    "            labels: Optional tensor of shape (batch_size, sequence_length) corresponding\n",
    "                    to input_ids (with -100 masking) for loss calculation.\n",
    "            raw_images: Optional list of PIL Images for the batch, needed by some patchers.\n",
    "\n",
    "        Returns:\n",
    "            Output dictionary from the language model (transformers.CausalLMOutputWithPast).\n",
    "        \"\"\"\n",
    "        # print(\"AdaptiveLLaVAModel forward pass called.\") # Debug print\n",
    "        patcher_metadata_batch = [None] * pixel_values.shape[0] # Initialize metadata list\n",
    "\n",
    "        # --- Patcher Logic --- \n",
    "        if self.patcher is not None:\n",
    "            if raw_images is None or len(raw_images) != pixel_values.shape[0]:\n",
    "                 warnings.warn(\"Patcher is enabled but 'raw_images' not provided or length mismatch. Cannot run patcher logic.\")\n",
    "            else:\n",
    "                # Iterate through batch to get metadata for each sample\n",
    "                for i in range(pixel_values.shape[0]):\n",
    "                    try:\n",
    "                        _, patcher_metadata_sample = self.patcher(raw_image=raw_images[i])\n",
    "                        patcher_metadata_batch[i] = patcher_metadata_sample\n",
    "                        # print(f\"Batch {i} Patcher metadata: {patcher_metadata_sample}\") # Debug print\n",
    "                    except Exception as e:\n",
    "                        warnings.warn(f\"Error running patcher for batch index {i}: {e}\")\n",
    "                # Store or log patcher_metadata_batch if needed\n",
    "                # Currently, we log/print but don't change the feature processing path.\n",
    "                self.current_patcher_metadata = patcher_metadata_batch # Store for potential inspection/logging\n",
    "        \n",
    "        # --- Baseline Feature Processing (Simplified path for now) --- \n",
    "        # 1. Encode Image (using standard pixel_values) & Project Features\n",
    "        image_features = self.encode_image(pixel_values) # (B, P_base, D_clip)\n",
    "        if image_features is None:\n",
    "            raise RuntimeError(\"Image encoding failed.\")\n",
    "        projected_image_features = self.projector(image_features) # (B, P_base, D_llm)\n",
    "        num_image_patches = projected_image_features.shape[1]\n",
    "\n",
    "        # --- Prepare LLM inputs (Same as baseline) --- \n",
    "        input_ids_clone = input_ids.clone()\n",
    "        input_ids_clone[input_ids_clone == self.image_token_index_marker] = 0 \n",
    "        text_embeddings = self.get_input_embeddings()(input_ids_clone) \n",
    "\n",
    "        new_input_embeds = []\n",
    "        new_labels = [] if labels is not None else None\n",
    "        new_attention_mask = []\n",
    "\n",
    "        for batch_idx in range(input_ids.shape[0]):\n",
    "            image_token_indices = torch.where(input_ids[batch_idx] == self.image_token_index_marker)[0]\n",
    "            if len(image_token_indices) == 0:\n",
    "                warnings.warn(f\"Image token placeholder {self.image_token_index_marker} not found in batch index {batch_idx}. Skipping image features.\")\n",
    "                new_input_embeds.append(text_embeddings[batch_idx])\n",
    "                current_attention_mask = attention_mask[batch_idx] if attention_mask is not None else (input_ids[batch_idx] != tokenizer.pad_token_id).long()\n",
    "                new_attention_mask.append(current_attention_mask)\n",
    "                if new_labels is not None and labels is not None:\n",
    "                    new_labels.append(labels[batch_idx])\n",
    "                continue\n",
    "\n",
    "            image_token_start_index = image_token_indices[0].item()\n",
    "\n",
    "            text_emb_before = text_embeddings[batch_idx, :image_token_start_index]\n",
    "            text_emb_after = text_embeddings[batch_idx, image_token_start_index + 1:]\n",
    "\n",
    "            cur_new_embed = torch.cat([\n",
    "                text_emb_before,\n",
    "                projected_image_features[batch_idx].to(text_embeddings.device, dtype=text_embeddings.dtype),\n",
    "                text_emb_after\n",
    "            ], dim=0)\n",
    "            new_input_embeds.append(cur_new_embed)\n",
    "\n",
    "            current_attention_mask = attention_mask[batch_idx] if attention_mask is not None else (input_ids[batch_idx] != tokenizer.pad_token_id).long()\n",
    "            mask_before = current_attention_mask[:image_token_start_index]\n",
    "            mask_image = torch.ones(num_image_patches, dtype=torch.long, device=current_attention_mask.device)\n",
    "            mask_after = current_attention_mask[image_token_start_index + 1:]\n",
    "            cur_new_mask = torch.cat([\n",
    "                mask_before,\n",
    "                mask_image,\n",
    "                mask_after\n",
    "            ], dim=0)\n",
    "            new_attention_mask.append(cur_new_mask)\n",
    "\n",
    "            if new_labels is not None and labels is not None:\n",
    "                label_before = labels[batch_idx, :image_token_start_index]\n",
    "                label_image = torch.full((num_image_patches,), self.ignore_index, dtype=torch.long, device=labels.device)\n",
    "                label_after = labels[batch_idx, image_token_start_index + 1:]\n",
    "                cur_new_label = torch.cat([\n",
    "                    label_before,\n",
    "                    label_image,\n",
    "                    label_after\n",
    "                ], dim=0)\n",
    "                new_labels.append(cur_new_label)\n",
    "\n",
    "        # --- Padding (Same as baseline) --- \n",
    "        padded_input_embeds = pad_sequence(new_input_embeds, batch_first=True, padding_value=0.0)\n",
    "        padded_attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)\n",
    "        padded_labels = None\n",
    "        if new_labels is not None:\n",
    "            padded_labels = pad_sequence(new_labels, batch_first=True, padding_value=self.ignore_index)\n",
    "\n",
    "        # --- Pass to LLM (Same as baseline) --- \n",
    "        outputs: CausalLMOutputWithPast = self.language_model(\n",
    "            inputs_embeds=padded_input_embeds,\n",
    "            attention_mask=padded_attention_mask,\n",
    "            labels=padded_labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AdaptiveLLaVAModel\n",
       "\n",
       ">      AdaptiveLLaVAModel (config:Dict[str,Any])\n",
       "\n",
       "*LLaVA Model extended with an Adaptive Patcher module.\n",
       "\n",
       "    Inherits from BaselineLLaVAModel and adds an adaptive patcher component\n",
       "    based on the configuration. The forward pass needs to be overridden\n",
       "    to incorporate the patcher's logic.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AdaptiveLLaVAModel\n",
       "\n",
       ">      AdaptiveLLaVAModel (config:Dict[str,Any])\n",
       "\n",
       "*LLaVA Model extended with an Adaptive Patcher module.\n",
       "\n",
       "    Inherits from BaselineLLaVAModel and adds an adaptive patcher component\n",
       "    based on the configuration. The forward pass needs to be overridden\n",
       "    to incorporate the patcher's logic.*"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AdaptiveLLaVAModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342849c49c814d98a9c9e812f8007417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n",
      "LoRA is disabled in the configuration.\n",
      "LLM embedding size already matches tokenizer size. No resizing needed.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Adaptive Patcher enabled with strategy: 'variable_resolution' (VariableResolutionPatcher)\n",
      "Initialized VariableResolutionPatcher with grid options: [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008], [336, 336]]\n",
      "Running Adaptive Forward Pass Test...\n",
      "Batch 0 Patcher metadata: {'strategy': 'variable_resolution', 'selected_grid': (336, 336), 'num_patches_h': 24, 'num_patches_w': 24, 'total_patches': 576}\n",
      "Batch 1 Patcher metadata: {'strategy': 'variable_resolution', 'selected_grid': (336, 672), 'num_patches_h': 24, 'num_patches_w': 48, 'total_patches': 1152}\n",
      "Output logits shape: torch.Size([2, 586, 32001])\n",
      "Output loss: tensor(10.4655, grad_fn=<NllLossBackward0>)\n",
      "Adaptive Forward Pass test successful!\n",
      "Cleaned up test_adaptive_model\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "import torch, gc\n",
    "from transformers import AutoModelForCausalLM, CLIPVisionModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    config_path = '../configs/config.yaml'\n",
    "    test_config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "\n",
    "    # --- Test with Adaptive Patcher Enabled --- \n",
    "    if 'model' not in test_config: test_config['model'] = {}\n",
    "    if 'adaptive_patcher' not in test_config['model']: test_config['model']['adaptive_patcher'] = {}\n",
    "    test_config['model']['adaptive_patcher']['enabled'] = True\n",
    "    test_config['model']['adaptive_patcher']['strategy'] = 'variable_resolution' # Test this strategy\n",
    "    # Disable LoRA and Checkpointing for simplicity in this forward pass test\n",
    "    if 'peft' not in test_config['model']: test_config['model']['peft'] = {}\n",
    "    test_config['model']['peft']['use_lora'] = False \n",
    "    test_config['model']['use_activation_checkpointing'] = False\n",
    "\n",
    "    # Instantiate the adaptive model\n",
    "    test_adaptive_model = AdaptiveLLaVAModel(test_config)\n",
    "    test_adaptive_model.eval() # Set to eval mode\n",
    "\n",
    "    print(\"Running Adaptive Forward Pass Test...\")\n",
    "    # Prepare dummy inputs\n",
    "    batch_size = 2\n",
    "    seq_len = 15 # Short sequence for testing\n",
    "    img_size = 336 # From config\n",
    "    num_patches = 576 # (336/14)^2 - Base patches\n",
    "    llm_hidden_dim = test_config['model']['projector']['output_dim']\n",
    "    tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "    dummy_pixel_values = torch.randn(batch_size, 3, img_size, img_size)\n",
    "    # Create input_ids with placeholder\n",
    "    dummy_input_ids = torch.randint(1, tokenizer_vocab_size, (batch_size, seq_len), dtype=torch.long)\n",
    "    placeholder_idx = 5 # Place the image token marker at index 5\n",
    "    dummy_input_ids[:, placeholder_idx] = IMAGE_TOKEN_INDEX_PLACEHOLDER\n",
    "    dummy_attention_mask = torch.ones_like(dummy_input_ids)\n",
    "    # Create labels (copy input_ids, mask placeholder and potentially prompt)\n",
    "    dummy_labels = dummy_input_ids.clone()\n",
    "    dummy_labels[:, :placeholder_idx+1] = IGNORE_INDEX # Mask prompt + image token\n",
    "    \n",
    "    # Create dummy raw images\n",
    "    dummy_raw_images = [\n",
    "        Image.new('RGB', (600, 600)), # Square\n",
    "        Image.new('RGB', (400, 800))  # Tall\n",
    "    ]\n",
    "\n",
    "    # Perform forward pass\n",
    "    with torch.no_grad(): \n",
    "         outputs = test_adaptive_model(\n",
    "            pixel_values=dummy_pixel_values,\n",
    "            input_ids=dummy_input_ids,\n",
    "            attention_mask=dummy_attention_mask,\n",
    "            labels=dummy_labels,\n",
    "            raw_images=dummy_raw_images # Pass raw images for patcher\n",
    "         )\n",
    "\n",
    "    # Check output type and attributes\n",
    "    assert isinstance(outputs, CausalLMOutputWithPast)\n",
    "    assert hasattr(outputs, 'logits')\n",
    "    assert outputs.logits is not None\n",
    "    assert hasattr(outputs, 'loss') # Should have loss since labels were provided\n",
    "    assert outputs.loss is not None\n",
    "\n",
    "    # Check output shapes (should match baseline for now)\n",
    "    expected_seq_len = seq_len - 1 + num_patches\n",
    "    expected_logits_shape = (batch_size, expected_seq_len, tokenizer_vocab_size)\n",
    "\n",
    "    assert outputs.logits.shape == expected_logits_shape, \\\n",
    "        f\"Expected logits shape {expected_logits_shape}, but got {outputs.logits.shape}\"\n",
    "    print(f\"Output logits shape: {outputs.logits.shape}\")\n",
    "    print(f\"Output loss: {outputs.loss}\")\n",
    "    \n",
    "    # Check if patcher metadata was stored (optional check)\n",
    "    assert hasattr(test_adaptive_model, 'current_patcher_metadata')\n",
    "    assert len(test_adaptive_model.current_patcher_metadata) == batch_size\n",
    "    assert test_adaptive_model.current_patcher_metadata[0]['selected_grid'] == (336, 336) # Expected for square image\n",
    "    assert test_adaptive_model.current_patcher_metadata[1]['selected_grid'] == (336, 672) # Expected for tall image\n",
    "    \n",
    "    print(\"Adaptive Forward Pass test successful!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file {config_path} not found. Skipping AdaptiveLLaVAModel test.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Skipping test due to ImportError: {e}. (Likely `peft` is missing)\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during AdaptiveLLaVAModel test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Clean up\n",
    "    if 'test_adaptive_model' in locals():\n",
    "        if hasattr(test_adaptive_model, 'vision_tower') and test_adaptive_model.vision_tower is not None:\n",
    "            test_adaptive_model.vision_tower.to('cpu')\n",
    "        if hasattr(test_adaptive_model, 'language_model') and test_adaptive_model.language_model is not None:\n",
    "            test_adaptive_model.language_model.to('cpu')\n",
    "        if hasattr(test_adaptive_model, 'projector') and test_adaptive_model.projector is not None:\n",
    "            test_adaptive_model.projector.to('cpu')\n",
    "        del test_adaptive_model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Cleaned up test_adaptive_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/21_model_adaptive.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "llava"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
