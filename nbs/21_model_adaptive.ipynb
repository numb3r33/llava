{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Model Components\n",
    "\n",
    "> Defines adaptive components for the LLaVA model, starting with the Adaptive Patcher interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.adaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    # print(f\"Project root already in sys.path: {project_root_str}\") # Less verbose\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.1: Define Adaptive Patcher Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This base class defines the interface for any adaptive patching strategy. Subclasses will implement specific logic (e.g., variable resolution, attention-based patching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AdaptivePatcher(nn.Module):\n",
    "    \"\"\"Base interface for adaptive image patching modules.\n",
    "\n",
    "    Subclasses should implement the `forward` method to dynamically process\n",
    "    an input image (or its features) based on content or context (like text instructions)\n",
    "    and return a structured representation of image features for the projector.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the Adaptive Patcher.\n",
    "\n",
    "        Args:\n",
    "            config: Dictionary containing configuration relevant to the patcher strategy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Potentially load sub-modules or parameters based on config['strategy']\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None, # Existing features (e.g., from base processing)\n",
    "        text_features: Optional[torch.Tensor] = None, # For text-guided strategies\n",
    "        raw_image: Optional[Image.Image] = None, # Original image for properties like aspect ratio\n",
    "        **kwargs\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[Dict[str, Any]]]:\n",
    "        \"\"\"Processes the input image adaptively or determines processing strategy.\n",
    "\n",
    "        The exact inputs used and outputs returned depend on the specific strategy.\n",
    "        For example, a strategy predictor might only need pooled features,\n",
    "        while a variable resolution strategy primarily needs the raw image aspect ratio.\n",
    "        A text-guided strategy needs text_features and potentially patch features.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Optional preprocessed image tensor (e.g., B x C x H x W).\n",
    "                           Could represent the full image or specific patches.\n",
    "            text_features: Optional tensor containing embeddings of the instruction text.\n",
    "                           Needed for text-guided patching strategies.\n",
    "            raw_image: Optional PIL image, potentially needed for calculating aspect ratio\n",
    "                       or other properties not easily derived from pixel_values alone.\n",
    "            **kwargs: Additional keyword arguments specific to the patching strategy.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - Optional[torch.Tensor]: Processed/selected image features, if the patcher\n",
    "                                    directly outputs features. Shape might vary.\n",
    "                                    Can be None if the patcher only outputs metadata.\n",
    "            - Optional[Dict[str, Any]]: Metadata about the patching process or decision.\n",
    "                                      (e.g., strategy used, number of patches, selected grid).\n",
    "                                      Can be None if no metadata is generated.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This base method must be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement the forward method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AdaptivePatcher\n",
       "\n",
       ">      AdaptivePatcher (config:Dict[str,Any])\n",
       "\n",
       "*Base interface for adaptive image patching modules.\n",
       "\n",
       "    Subclasses should implement the `forward` method to dynamically process\n",
       "    an input image (or its features) based on content or context (like text instructions)\n",
       "    and return a structured representation of image features for the projector.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AdaptivePatcher\n",
       "\n",
       ">      AdaptivePatcher (config:Dict[str,Any])\n",
       "\n",
       "*Base interface for adaptive image patching modules.\n",
       "\n",
       "    Subclasses should implement the `forward` method to dynamically process\n",
       "    an input image (or its features) based on content or context (like text instructions)\n",
       "    and return a structured representation of image features for the projector.*"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AdaptivePatcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.2: Implement Variable Resolution Patcher Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This patcher implements the variable resolution strategy inspired by LLaVA-NeXT. It analyzes the input image's aspect ratio and selects the optimal grid configuration from a predefined set (`image_grid_pinpoints`). The actual image resizing and patching happen elsewhere, based on the grid configuration returned by this patcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VariableResolutionPatcher(AdaptivePatcher):\n",
    "    \"\"\"Adaptive patcher that selects an optimal grid resolution based on image aspect ratio.\n",
    "\n",
    "    Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
    "    dimensions but does not perform the actual image processing or feature extraction.\n",
    "    The main model's forward pass should use the output metadata to handle the image.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"Initializes the VariableResolutionPatcher.\n",
    "\n",
    "        Args:\n",
    "            config: Dictionary containing configuration. Expected keys:\n",
    "                    'model.adaptive_patcher.image_grid_pinpoints': List of [H, W] grids.\n",
    "                    'model.vision_config.image_size': Base image size (e.g., 336).\n",
    "                    'model.vision_config.patch_size': Patch size (e.g., 14).\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        # Default grid from LLaVA-NeXT / Project Spec\n",
    "        default_grid = [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]\n",
    "        self.image_grid_pinpoints = self.config.get('model', {}).get('adaptive_patcher', {}).get('image_grid_pinpoints', default_grid)\n",
    "        # Add the base resolution to the grid options\n",
    "        self.base_image_size = self.config.get('model', {}).get('vision_config', {}).get('image_size', 336)\n",
    "        self.patch_size = self.config.get('model', {}).get('vision_config', {}).get('patch_size', 14)\n",
    "        \n",
    "        # Ensure base resolution is included as an option (implicitly or explicitly)\n",
    "        base_grid = [self.base_image_size, self.base_image_size]\n",
    "        if base_grid not in self.image_grid_pinpoints:\n",
    "             self.image_grid_pinpoints.append(base_grid)\n",
    "        print(f\"Initialized VariableResolutionPatcher with grid options: {self.image_grid_pinpoints}\")\n",
    "\n",
    "    def select_best_resolution(self, original_height: int, original_width: int) -> Tuple[int, int]:\n",
    "        \"\"\"Selects the best grid resolution based on aspect ratio and minimizing waste.\n",
    "\n",
    "        Args:\n",
    "            original_height: Height of the raw input image.\n",
    "            original_width: Width of the raw input image.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: The selected best grid dimensions (height, width).\n",
    "        \"\"\"\n",
    "        original_aspect_ratio = original_height / original_width\n",
    "\n",
    "        best_fit_grid = None\n",
    "        min_wasted_pixels = float('inf')\n",
    "\n",
    "        for grid_h, grid_w in self.image_grid_pinpoints:\n",
    "            grid_aspect_ratio = grid_h / grid_w\n",
    "\n",
    "            # Calculate the dimensions if the image were scaled to fit this grid\n",
    "            # Scale based on the limiting dimension\n",
    "            scale_h = grid_h / original_height\n",
    "            scale_w = grid_w / original_width\n",
    "\n",
    "            if original_aspect_ratio > grid_aspect_ratio: # Scale based on height\n",
    "                scaled_h = grid_h\n",
    "                scaled_w = int(original_width * scale_h)\n",
    "            else: # Scale based on width\n",
    "                scaled_w = grid_w\n",
    "                scaled_h = int(original_height * scale_w)\n",
    "            \n",
    "            # Ensure scaled dimensions do not exceed grid dimensions (due to int conversion)\n",
    "            scaled_h = min(scaled_h, grid_h)\n",
    "            scaled_w = min(scaled_w, grid_w)\n",
    "            \n",
    "            # Calculate wasted area (pixels in the grid not covered by the scaled image)\n",
    "            grid_area = grid_h * grid_w\n",
    "            scaled_image_area = scaled_h * scaled_w\n",
    "            wasted_pixels = grid_area - scaled_image_area\n",
    "\n",
    "            # Prefer grids with less waste\n",
    "            # Among grids with similar waste, the reference code doesn't specify tie-breaking.\n",
    "            # Let's pick the first one encountered with the minimum waste.\n",
    "            # A slightly better tie-breaker might be aspect ratio closeness, but min waste is simpler.\n",
    "            if wasted_pixels < min_wasted_pixels:\n",
    "                min_wasted_pixels = wasted_pixels\n",
    "                best_fit_grid = (grid_h, grid_w)\n",
    "            # Simple tie-breaking: if waste is equal, prefer larger area (less scaling down)\n",
    "            elif wasted_pixels == min_wasted_pixels:\n",
    "                 if best_fit_grid is None or (grid_h * grid_w > best_fit_grid[0] * best_fit_grid[1]):\n",
    "                       best_fit_grid = (grid_h, grid_w)\n",
    "                       \n",
    "        # Fallback to base resolution if something went wrong\n",
    "        if best_fit_grid is None:\n",
    "            print(f\"Warning: Could not determine best fit grid for H={original_height}, W={original_width}. Defaulting to base {self.base_image_size}x{self.base_image_size}.\")\n",
    "            best_fit_grid = (self.base_image_size, self.base_image_size)\n",
    "            \n",
    "        return best_fit_grid\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None, # Not used by this patcher\n",
    "        text_features: Optional[torch.Tensor] = None, # Not used by this patcher\n",
    "        raw_image: Optional[Image.Image] = None,\n",
    "        **kwargs\n",
    "    ) -> Tuple[None, Dict[str, Any]]: # Returns None for features, Dict for metadata\n",
    "        \"\"\"Determines the best grid resolution based on the raw image's aspect ratio.\n",
    "\n",
    "        Args:\n",
    "            raw_image: The original PIL Image object.\n",
    "            pixel_values: Ignored by this patcher.\n",
    "            text_features: Ignored by this patcher.\n",
    "            **kwargs: Ignored.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - None: This patcher does not directly return processed features.\n",
    "            - Dict[str, Any]: Metadata including:\n",
    "                - 'selected_grid' (Tuple[int, int]): The chosen grid dimensions (H, W).\n",
    "                - 'num_patches_h' (int): Number of patches vertically in the grid.\n",
    "                - 'num_patches_w' (int): Number of patches horizontally in the grid.\n",
    "                - 'total_patches' (int): Total number of patches in the selected grid.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If raw_image is not provided.\n",
    "        \"\"\"\n",
    "        if raw_image is None:\n",
    "            raise ValueError(\"VariableResolutionPatcher requires the 'raw_image' (PIL Image) input.\")\n",
    "\n",
    "        original_width, original_height = raw_image.size\n",
    "        \n",
    "        # Select the best grid based on aspect ratio\n",
    "        selected_grid_h, selected_grid_w = self.select_best_resolution(original_height, original_width)\n",
    "        \n",
    "        # Calculate number of patches for the selected grid\n",
    "        if self.patch_size <= 0:\n",
    "            raise ValueError(\"Patch size must be positive.\")\n",
    "        num_patches_h = selected_grid_h // self.patch_size\n",
    "        num_patches_w = selected_grid_w // self.patch_size\n",
    "        total_patches = num_patches_h * num_patches_w\n",
    "        \n",
    "        metadata = {\n",
    "            'strategy': 'variable_resolution',\n",
    "            'selected_grid': (selected_grid_h, selected_grid_w),\n",
    "            'num_patches_h': num_patches_h,\n",
    "            'num_patches_w': num_patches_w,\n",
    "            'total_patches': total_patches\n",
    "        }\n",
    "        \n",
    "        # This patcher returns metadata for the main model to use, not processed features.\n",
    "        return None, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### VariableResolutionPatcher\n",
       "\n",
       ">      VariableResolutionPatcher (config:Dict[str,Any])\n",
       "\n",
       "*Adaptive patcher that selects an optimal grid resolution based on image aspect ratio.\n",
       "\n",
       "Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
       "dimensions but does not perform the actual image processing or feature extraction.\n",
       "The main model's forward pass should use the output metadata to handle the image.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### VariableResolutionPatcher\n",
       "\n",
       ">      VariableResolutionPatcher (config:Dict[str,Any])\n",
       "\n",
       "*Adaptive patcher that selects an optimal grid resolution based on image aspect ratio.\n",
       "\n",
       "Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing\n",
       "dimensions but does not perform the actual image processing or feature extraction.\n",
       "The main model's forward pass should use the output metadata to handle the image.*"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(VariableResolutionPatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized VariableResolutionPatcher with grid options: [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008], [336, 336]]\n",
      "Image (600, 600), Ratio 1.00 -> Selected Grid: (336, 336), Patches: 24x24=576\n",
      "Image (400, 800), Ratio 0.50 -> Selected Grid: (336, 672), Patches: 24x48=1152\n",
      "Image (800, 400), Ratio 2.00 -> Selected Grid: (672, 336), Patches: 48x24=1152\n",
      "Image (1200, 400), Ratio 3.00 -> Selected Grid: (1008, 336), Patches: 72x24=1728\n",
      "Image (400, 1200), Ratio 0.33 -> Selected Grid: (336, 1008), Patches: 24x72=1728\n",
      "Image (800, 800), Ratio 1.00 -> Selected Grid: (672, 672), Patches: 48x48=2304\n",
      "Variable Resolution Patcher tests passed.\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "import PIL.Image\n",
    "\n",
    "# Create dummy config\n",
    "test_config = {\n",
    "    'model': {\n",
    "        'adaptive_patcher': {\n",
    "             'image_grid_pinpoints': [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]\n",
    "        },\n",
    "        'vision_config': {\n",
    "            'image_size': 336,\n",
    "            'patch_size': 14\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Instantiate the patcher\n",
    "patcher = VariableResolutionPatcher(test_config)\n",
    "\n",
    "# Test cases with different aspect ratios\n",
    "test_images = {\n",
    "    \"square\": PIL.Image.new('RGB', (600, 600)),\n",
    "    \"tall\": PIL.Image.new('RGB', (400, 800)),\n",
    "    \"wide\": PIL.Image.new('RGB', (800, 400)),\n",
    "    \"very_wide\": PIL.Image.new('RGB', (1200, 400)),\n",
    "    \"very_tall\": PIL.Image.new('RGB', (400, 1200)),\n",
    "    \"large_square\": PIL.Image.new('RGB', (800, 800)), # Test selection of larger grids\n",
    "}\n",
    "\n",
    "expected_grids = {\n",
    "    \"square\": (336, 336), # Closest default is base 1:1\n",
    "    \"tall\": (336, 672), # 1:2\n",
    "    \"wide\": (672, 336), # 2:1\n",
    "    \"very_wide\": (1008, 336), # 3:1\n",
    "    \"very_tall\": (336, 1008), # 1:3\n",
    "    \"large_square\": (672, 672), # Prefers larger grid for less scaling if waste is equal\n",
    "}\n",
    "\n",
    "for name, img in test_images.items():\n",
    "    _, metadata = patcher(raw_image=img)\n",
    "    selected_grid = metadata['selected_grid']\n",
    "    total_patches = metadata['total_patches']\n",
    "    expected_grid = expected_grids[name]\n",
    "    w, h = img.size\n",
    "    print(f\"Image ({h}, {w}), Ratio {h/w:.2f} -> Selected Grid: {selected_grid}, Patches: {metadata['num_patches_h']}x{metadata['num_patches_w']}={total_patches}\")\n",
    "    assert selected_grid == expected_grid, f\"Test '{name}' failed. Expected {expected_grid}, got {selected_grid}\"\n",
    "\n",
    "# Test ValueError if raw_image is None\n",
    "try:\n",
    "    patcher(raw_image=None)\n",
    "    assert False, \"Should have raised ValueError when raw_image is None\"\n",
    "except ValueError:\n",
    "    pass # Expected\n",
    "\n",
    "print(\"Variable Resolution Patcher tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.4: Define Adaptive LLaVA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will involve creating `AdaptiveLLaVAModel` inheriting from `BaselineLLaVAModel` and integrating the chosen `AdaptivePatcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for AdaptiveLLaVAModel definition (Step 6.4)\n",
    "# from llava.model.baseline import BaselineLLaVAModel\n",
    "#\n",
    "# class AdaptiveLLaVAModel(BaselineLLaVAModel):\n",
    "#     def __init__(self, config: Dict[str, Any]):\n",
    "#         super().__init__(config)\n",
    "#         patcher_strategy = config.get('model', {}).get('adaptive_patcher', {}).get('strategy')\n",
    "#         if patcher_strategy == 'variable_resolution':\n",
    "#             self.patcher = VariableResolutionPatcher(config)\n",
    "#         # Add other strategies here\n",
    "#         # elif patcher_strategy == 'predictor':\n",
    "#         #     self.patcher = PredictorPatcher(config)\n",
    "#         else:\n",
    "#             print(f\"Warning: Unknown or no adaptive patcher strategy specified ('{patcher_strategy}'). Using baseline behavior.\")\n",
    "#             self.patcher = None\n",
    "#\n",
    "#     # Override forward pass (Step 6.5)\n",
    "#     def forward(self, ...):\n",
    "#         # ... implementation ...\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/21_model_adaptive.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "llava"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}