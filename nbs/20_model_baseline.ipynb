{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Architecture\n",
    "\n",
    "> Defines the baseline LLaVA 1.5 model architecture, including the projector and the combined model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n",
      "Loaded config from configs/config.yaml\n",
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence # For padding variable length sequences\n",
    "from transformers import CLIPVisionModel, AutoModelForCausalLM, AutoConfig\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling, CausalLMOutputWithPast # For type hints\n",
    "import warnings\n",
    "from typing import Optional # Added for type hinting\n",
    "\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, PeftModel # Import PEFT components\n",
    "    _peft_available = True\n",
    "except ImportError:\n",
    "    print(\"Warning: peft library not found. LoRA functionality will be disabled.\")\n",
    "    LoraConfig, get_peft_model, PeftModel = None, None, None # Define as None if not available\n",
    "    _peft_available = False\n",
    "\n",
    "from llava.utils import load_config # Assuming utils notebook is created\n",
    "from llava.data.preprocessing import IMAGE_TOKEN_INDEX_PLACEHOLDER, IGNORE_INDEX, tokenizer, DEFAULT_IMAGE_TOKEN # Import constants and tokenizer\n",
    "\n",
    "# Ensure tokenizer is loaded (it should be from the import)\n",
    "if tokenizer is None:\n",
    "    print(\"Warning: Tokenizer could not be imported from data.preprocessing. Trying to load it again...\")\n",
    "    try:\n",
    "        # Need to load config first to get LLM name\n",
    "        _config_temp = load_config('configs/config.yaml')\n",
    "        _llm_name_temp = _config_temp.get('model', {}).get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')\n",
    "        _tokenizer_max_len_temp = _config_temp.get('data', {}).get('tokenizer_model_max_length', 2048)\n",
    "        _tokenizer_padding_side_temp = _config_temp.get('data', {}).get('tokenizer_padding_side', 'right')\n",
    "\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            _llm_name_temp,\n",
    "            model_max_length=_tokenizer_max_len_temp,\n",
    "            padding_side=_tokenizer_padding_side_temp,\n",
    "            use_fast=True,\n",
    "        )\n",
    "        # Re-add special token logic if necessary (should match preprocessing step)\n",
    "        # from llava.data.preprocessing import DEFAULT_IMAGE_TOKEN # Already imported\n",
    "        if DEFAULT_IMAGE_TOKEN not in tokenizer.get_vocab():\n",
    "             num_added = tokenizer.add_special_tokens({'additional_special_tokens': [DEFAULT_IMAGE_TOKEN]})\n",
    "             print(f\"Added {num_added} token(s) during re-load. New vocab size: {len(tokenizer)}\")\n",
    "        if tokenizer.pad_token is None:\n",
    "             tokenizer.pad_token = tokenizer.eos_token\n",
    "             print(f\"Set pad token to EOS during re-load.\")\n",
    "        print(f\"Tokenizer re-loaded successfully for {_llm_name_temp}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal Error: Could not load tokenizer. {e}\")\n",
    "        # Depending on the context, might raise an error or exit\n",
    "        raise ImportError(\"Tokenizer is essential and could not be loaded.\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Define Projector Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projector module connects the vision encoder's output to the language model's input space. In LLaVA 1.5, this is typically a simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLaVAProjector(nn.Module):\n",
    "    \"\"\"A simple 2-layer MLP projector to map vision features to LLM embedding space.\n",
    "\n",
    "    Maps input_dim (e.g., CLIP ViT-L output dim = 1024) to\n",
    "    output_dim (e.g., Vicuna-7B hidden dim = 4096).\n",
    "\n",
    "    Architecture: Linear -> GELU -> Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        \"\"\"Initializes the MLP projector.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimension of the input vision features.\n",
    "            output_dim: Dimension of the LLM's hidden space.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(output_dim, output_dim, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Projects the input features.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, num_patches, input_dim) or similar.\n",
    "\n",
    "        Returns:\n",
    "            Projected tensor of shape (batch_size, num_patches, output_dim).\n",
    "        \"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LLaVAProjector\n",
       "\n",
       ">      LLaVAProjector (input_dim:int, output_dim:int)\n",
       "\n",
       "*A simple 2-layer MLP projector to map vision features to LLM embedding space.\n",
       "\n",
       "    Maps input_dim (e.g., CLIP ViT-L output dim = 1024) to\n",
       "    output_dim (e.g., Vicuna-7B hidden dim = 4096).\n",
       "\n",
       "    Architecture: Linear -> GELU -> Linear*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LLaVAProjector\n",
       "\n",
       ">      LLaVAProjector (input_dim:int, output_dim:int)\n",
       "\n",
       "*A simple 2-layer MLP projector to map vision features to LLM embedding space.\n",
       "\n",
       "    Maps input_dim (e.g., CLIP ViT-L output dim = 1024) to\n",
       "    output_dim (e.g., Vicuna-7B hidden dim = 4096).\n",
       "\n",
       "    Architecture: Linear -> GELU -> Linear*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLaVAProjector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projector test passed!\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# Test projector instantiation and forward pass\n",
    "inp_dim = 1024\n",
    "out_dim = 4096\n",
    "projector = LLaVAProjector(input_dim=inp_dim, output_dim=out_dim)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "batch_size = 2\n",
    "num_patches = 576 # Example for 336x336 input with patch size 14\n",
    "dummy_input = torch.randn(batch_size, num_patches, inp_dim)\n",
    "\n",
    "# Perform forward pass\n",
    "output = projector(dummy_input)\n",
    "\n",
    "# Check output shape\n",
    "assert output.shape == (batch_size, num_patches, out_dim), f\"Expected shape {(batch_size, num_patches, out_dim)}, but got {output.shape}\"\n",
    "print(\"Projector test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2, 2.3, 2.4, 4.2: Define Baseline LLaVA Model & Integrate Components + LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class combines the Vision Encoder, the Projector, and the Language Model to form the complete LLaVA baseline model. We integrate the loading of the Vision Encoder (CLIP) and the Language Model (Vicuna), handle necessary configurations like embedding resizing, and **now integrate optional LoRA application using the `peft` library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaselineLLaVAModel(nn.Module):\n",
    "    \"\"\"Baseline LLaVA 1.5 model combining Vision Encoder, Projector, and LLM.\n",
    "\n",
    "    Loads the specified Vision Encoder (CLIP) and LLM (Vicuna) from Hugging Face Hub,\n",
    "    along with the Projector. Handles freezing components, LLM embedding resizing,\n",
    "    and optionally applies LoRA to the LLM based on the configuration.\n",
    "    Implements `encode_image`, activation checkpointing, and the main `forward` pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: dict):\n",
    "        \"\"\"Initializes the Baseline LLaVA model structure.\n",
    "\n",
    "        Loads Vision Encoder, LLM, and Projector based on the config.\n",
    "        Resizes LLM embeddings to match the tokenizer vocabulary size.\n",
    "        Optionally applies LoRA to the LLM.\n",
    "        Optionally applies activation checkpointing to the LLM.\n",
    "\n",
    "        Args:\n",
    "            config: Dictionary containing model configuration, including paths/names\n",
    "                    for vision encoder, LLM, projector dimensions, LoRA settings, checkpointing, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config # Store the main config\n",
    "        self.model_config = config.get('model', {}) # Store the model sub-config\n",
    "\n",
    "        # Initialize components placeholders\n",
    "        self.vision_tower = None\n",
    "        self.language_model = None\n",
    "        self.projector = None\n",
    "\n",
    "        # Store configuration details needed\n",
    "        self.image_token_index_marker = self.model_config.get('image_token_index_marker', IMAGE_TOKEN_INDEX_PLACEHOLDER)\n",
    "        self.ignore_index = IGNORE_INDEX # Use imported constant\n",
    "        self.vision_feature_layer = self.model_config.get('vision_feature_layer', -2)\n",
    "        self.vision_encoder_name = self.model_config.get('vision_encoder_name_or_path', 'openai/clip-vit-large-patch14-336')\n",
    "        self.llm_name = self.model_config.get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')\n",
    "\n",
    "        # --- Load component configs first --- #\n",
    "        llm_hf_config = AutoConfig.from_pretrained(self.llm_name)\n",
    "        vision_hf_config = AutoConfig.from_pretrained(self.vision_encoder_name)\n",
    "\n",
    "        # --- Determine Projector dimensions --- #\n",
    "        projector_cfg_from_yaml = self.model_config.get('projector', {})\n",
    "        # Input: Use vision model's hidden size\n",
    "        proj_input_dim = projector_cfg_from_yaml.get('input_dim', vision_hf_config.hidden_size)\n",
    "        # Output: Use LLM's hidden size\n",
    "        proj_output_dim = projector_cfg_from_yaml.get('output_dim', llm_hf_config.hidden_size)\n",
    "\n",
    "        # --- Initialize Projector --- #\n",
    "        print(f\"Initializing Projector: Input Dim={proj_input_dim}, Output Dim={proj_output_dim}\")\n",
    "        self.projector = LLaVAProjector(proj_input_dim, proj_output_dim)\n",
    "\n",
    "        # --- Load Vision Tower --- #\n",
    "        self.load_vision_tower(expected_output_dim=proj_input_dim)\n",
    "\n",
    "        # --- Load Language Model (and potentially apply LoRA) --- #\n",
    "        self.load_language_model()\n",
    "\n",
    "        # --- Resize LLM Embeddings --- #\n",
    "        self.resize_llm_embeddings()\n",
    "        \n",
    "        # --- Apply Activation Checkpointing if enabled --- #\n",
    "        self.apply_activation_checkpointing()\n",
    "\n",
    "    def load_vision_tower(self, expected_output_dim: int):\n",
    "        \"\"\"Loads the CLIP Vision Model based on the configuration.\n",
    "        Args:\n",
    "            expected_output_dim: The dimension the projector expects as input,\n",
    "                                 used for validation against the loaded vision tower.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading Vision Tower: {self.vision_encoder_name}...\")\n",
    "            self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_encoder_name)\n",
    "            print(f\"Vision Tower loaded successfully.\")\n",
    "\n",
    "            # Freeze the vision tower by default (LLaVA Stage 1 & commonly Stage 2)\n",
    "            self.vision_tower.requires_grad_(False)\n",
    "            print(f\"Vision Tower weights frozen.\")\n",
    "\n",
    "            # Check if vision tower output dim matches projector input dim\n",
    "            vision_output_dim = self.vision_tower.config.hidden_size\n",
    "            if vision_output_dim != expected_output_dim:\n",
    "                warnings.warn(\n",
    "                    f\"Vision Tower output dimension ({vision_output_dim}) does not match \"\n",
    "                    f\"Projector input dimension ({expected_output_dim}). Check vision model and config['model']['projector']['input_dim'].\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Vision Tower ({self.vision_encoder_name}): {e}\")\n",
    "            print(\"Please ensure the model name is correct and you have internet connectivity.\")\n",
    "            self.vision_tower = None # Set to None if loading fails\n",
    "\n",
    "    def load_language_model(self):\n",
    "        \"\"\"Loads the Language Model and optionally applies LoRA based on the configuration.\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading Language Model: {self.llm_name}...\")\n",
    "            # TODO: Add logic for 4-bit loading (QLoRA) here if needed, requires bitsandbytes\n",
    "            # Example: load_in_4bit=self.model_config.get('quantization', {}).get('load_in_4bit', False)\n",
    "            self.language_model = AutoModelForCausalLM.from_pretrained(self.llm_name)\n",
    "            print(f\"Language Model loaded successfully.\")\n",
    "\n",
    "            # Freeze the base LLM weights before applying PEFT\n",
    "            self.language_model.requires_grad_(False)\n",
    "            print(f\"Base Language Model weights frozen.\")\n",
    "\n",
    "            # --- Apply PEFT/LoRA (Step 4.2) --- \n",
    "            peft_config = self.model_config.get('peft', {})\n",
    "            use_lora = peft_config.get('use_lora', False)\n",
    "\n",
    "            if use_lora:\n",
    "                if not _peft_available:\n",
    "                    print(\"Warning: `use_lora` is true in config, but peft library is not installed. Skipping LoRA.\")\n",
    "                else:\n",
    "                    print(\"Applying LoRA...\")\n",
    "                    lora_r = peft_config.get('lora_r', 8)\n",
    "                    lora_alpha = peft_config.get('lora_alpha', 16)\n",
    "                    lora_dropout = peft_config.get('lora_dropout', 0.05)\n",
    "                    target_modules = peft_config.get('target_modules', ['q_proj', 'v_proj']) # Default targets\n",
    "                    # TODO: Add task_type (CAUSAL_LM) if needed by peft version\n",
    "                    # from peft import TaskType\n",
    "                    \n",
    "                    lora_config = LoraConfig(\n",
    "                        r=lora_r,\n",
    "                        lora_alpha=lora_alpha,\n",
    "                        target_modules=target_modules,\n",
    "                        lora_dropout=lora_dropout,\n",
    "                        bias=\"none\", # Common setting for LoRA\n",
    "                        # task_type=TaskType.CAUSAL_LM # Specify task type if needed\n",
    "                    )\n",
    "                    \n",
    "                    self.language_model = get_peft_model(self.language_model, lora_config)\n",
    "                    print(f\"LoRA applied. Config: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}, modules={target_modules}\")\n",
    "                    # Print trainable parameters after applying LoRA\n",
    "                    self.language_model.print_trainable_parameters()\n",
    "            else:\n",
    "                print(\"LoRA is disabled in the configuration.\")\n",
    "            # ----------------------------------\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Language Model ({self.llm_name}): {e}\")\n",
    "            print(\"Please ensure the model name is correct and you have internet connectivity / memory.\")\n",
    "            self.language_model = None # Set to None if loading fails\n",
    "\n",
    "    def resize_llm_embeddings(self):\n",
    "        \"\"\"Resizes the LLM's token embeddings to match the tokenizer size.\"\"\"\n",
    "        if self.language_model is None or tokenizer is None:\n",
    "            print(\"Warning: Cannot resize LLM embeddings. LLM or tokenizer not available.\")\n",
    "            return\n",
    "\n",
    "        current_embeddings = self.get_input_embeddings() # Use helper to handle PEFT\n",
    "        current_vocab_size = current_embeddings.weight.size(0)\n",
    "        target_vocab_size = len(tokenizer)\n",
    "\n",
    "        if current_vocab_size != target_vocab_size:\n",
    "            print(f\"Resizing LLM token embeddings from {current_vocab_size} to {target_vocab_size} (tokenizer size)...\")\n",
    "            self.language_model.resize_token_embeddings(target_vocab_size)\n",
    "            # Initialization of new embeddings is handled by resize_token_embeddings\n",
    "            print(\"LLM token embeddings resized.\")\n",
    "        else:\n",
    "            print(\"LLM embedding size already matches tokenizer size. No resizing needed.\")\n",
    "            \n",
    "    def apply_activation_checkpointing(self):\n",
    "        \"\"\"Applies gradient checkpointing if enabled in the config.\"\"\"\n",
    "        use_checkpointing = self.model_config.get('use_activation_checkpointing', False)\n",
    "        if use_checkpointing:\n",
    "            if self.language_model is not None:\n",
    "                 # Check if it's a PeftModel, need to call on base_model if so\n",
    "                 # The `gradient_checkpointing_enable` method exists on the PeftModel wrapper too\n",
    "                 model_to_checkpoint = self.language_model \n",
    "                 \n",
    "                 # Check if the model has the gradient_checkpointing_enable method\n",
    "                 if hasattr(model_to_checkpoint, 'gradient_checkpointing_enable'):\n",
    "                     try:\n",
    "                         model_to_checkpoint.gradient_checkpointing_enable()\n",
    "                         print(\"Activation checkpointing enabled for the language model.\")\n",
    "                     except Exception as e:\n",
    "                         print(f\"Warning: Failed to enable activation checkpointing: {e}\")\n",
    "                 else:\n",
    "                     print(\"Warning: Language model does not have 'gradient_checkpointing_enable' method. Cannot enable activation checkpointing automatically.\")\n",
    "            else:\n",
    "                 print(\"Warning: Activation checkpointing enabled in config, but language model is not loaded.\")\n",
    "        else:\n",
    "             print(\"Activation checkpointing is disabled in the configuration.\")\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        \"\"\"Helper to get the LLM's input embedding layer.\"\"\"\n",
    "        if self.language_model is None:\n",
    "            raise ValueError(\"Language model not loaded.\")\n",
    "        return self.language_model.get_input_embeddings()\n",
    "\n",
    "    def encode_image(self, pixel_values: torch.Tensor) -> torch.Tensor | None:\n",
    "        \"\"\"Encodes images using the vision tower and extracts features from the specified layer.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Tensor of shape (batch_size, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            Tensor containing the selected image features (batch_size, num_patches, hidden_dim),\n",
    "            or None if the vision tower failed to load.\n",
    "        \"\"\"\n",
    "        if self.vision_tower is None:\n",
    "            print(\"Error: Vision tower not loaded, cannot encode image.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Ensure pixel_values are on the same device as the vision tower\n",
    "            device = self.vision_tower.device\n",
    "            pixel_values = pixel_values.to(device, dtype=self.vision_tower.dtype)\n",
    "\n",
    "            # Pass image through vision tower. Request hidden states to access intermediate layers.\n",
    "            vision_outputs: BaseModelOutputWithPooling = self.vision_tower(\n",
    "                pixel_values,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "\n",
    "            # Extract features from the specified layer.\n",
    "            # hidden_states is a tuple: (embedding_layer_output, layer1_output, ..., last_layer_output)\n",
    "            # We use vision_feature_layer (e.g., -2 for second-to-last) to index into this tuple.\n",
    "            image_features = vision_outputs.hidden_states[self.vision_feature_layer]\n",
    "\n",
    "            # The output includes features for the [CLS] token and patch tokens.\n",
    "            # LLaVA uses only the patch features for projection.\n",
    "            # Shape: (batch_size, num_tokens, hidden_dim), where num_tokens = num_patches + 1 (for CLS)\n",
    "            # We remove the first token ([CLS]) feature.\n",
    "            image_features = image_features[:, 1:, :] # Shape: (batch_size, num_patches, hidden_dim)\n",
    "\n",
    "            return image_features\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during image encoding: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    # --- Step 2.5: Implement Baseline Forward Pass ---\n",
    "    def forward(self,\n",
    "                pixel_values: torch.Tensor,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None, # Made optional for HF compatibility\n",
    "                labels: Optional[torch.Tensor] = None # Made optional\n",
    "               ) -> CausalLMOutputWithPast:\n",
    "        \"\"\"Defines the forward pass of the LLaVA model.\n",
    "\n",
    "        Processes the image, projects features, combines with text embeddings,\n",
    "        and passes the result through the language model.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Tensor of shape (batch_size, C, H, W).\n",
    "            input_ids: Tensor of shape (batch_size, sequence_length) containing token IDs\n",
    "                       and IMAGE_TOKEN_INDEX_PLACEHOLDER markers (-200).\n",
    "            attention_mask: Optional tensor of shape (batch_size, sequence_length).\n",
    "                            If None, it will be constructed based on input_ids padding.\n",
    "            labels: Optional tensor of shape (batch_size, sequence_length) corresponding\n",
    "                    to input_ids (with -100 masking) for loss calculation.\n",
    "\n",
    "        Returns:\n",
    "            Output dictionary from the language model (transformers.CausalLMOutputWithPast),\n",
    "            typically including 'logits'. If labels are provided, the output includes 'loss'.\n",
    "        \"\"\"\n",
    "        if self.language_model is None or self.vision_tower is None or self.projector is None:\n",
    "            raise RuntimeError(\"Model components (LLM, Vision Tower, Projector) are not fully loaded.\")\n",
    "\n",
    "        # 1. Encode Image & Project Features\n",
    "        image_features = self.encode_image(pixel_values) # (B, P, D_clip)\n",
    "        if image_features is None:\n",
    "            raise RuntimeError(\"Image encoding failed.\")\n",
    "        projected_image_features = self.projector(image_features) # (B, P, D_llm)\n",
    "        num_image_patches = projected_image_features.shape[1]\n",
    "\n",
    "        # 2. Prepare inputs for embedding replacement\n",
    "        # We need input embeddings, not input_ids directly for replacement\n",
    "        input_ids_clone = input_ids.clone() # Clone to avoid modifying original\n",
    "        input_ids_clone[input_ids_clone == self.image_token_index_marker] = 0 # Replace marker temporarily (e.g., with pad token ID) for embedding lookup\n",
    "\n",
    "        # Get base text embeddings\n",
    "        text_embeddings = self.get_input_embeddings()(input_ids_clone) # (B, S, D_llm)\n",
    "\n",
    "        # 3. Combine Text and Image Embeddings\n",
    "        new_input_embeds = []\n",
    "        new_labels = [] if labels is not None else None\n",
    "        new_attention_mask = []\n",
    "\n",
    "        for batch_idx in range(input_ids.shape[0]):\n",
    "            # Find the placeholder token index in the original input_ids\n",
    "            image_token_indices = torch.where(input_ids[batch_idx] == self.image_token_index_marker)[0]\n",
    "            if len(image_token_indices) == 0:\n",
    "                # Should not happen if data processing is correct\n",
    "                warnings.warn(f\"Image token placeholder {self.image_token_index_marker} not found in batch index {batch_idx}. Skipping image features.\")\n",
    "                # Fallback: use original text embeddings and mask\n",
    "                new_input_embeds.append(text_embeddings[batch_idx])\n",
    "                # Construct attention mask if not provided initially\n",
    "                current_attention_mask = attention_mask[batch_idx] if attention_mask is not None else (input_ids[batch_idx] != tokenizer.pad_token_id).long()\n",
    "                new_attention_mask.append(current_attention_mask)\n",
    "                if new_labels is not None and labels is not None:\n",
    "                    new_labels.append(labels[batch_idx])\n",
    "                continue\n",
    "\n",
    "            image_token_start_index = image_token_indices[0].item()\n",
    "\n",
    "            # Slice text embeddings\n",
    "            text_emb_before = text_embeddings[batch_idx, :image_token_start_index]\n",
    "            text_emb_after = text_embeddings[batch_idx, image_token_start_index + 1:]\n",
    "\n",
    "            # Concatenate to form the final sequence embedding\n",
    "            cur_new_embed = torch.cat([\n",
    "                text_emb_before,\n",
    "                projected_image_features[batch_idx].to(text_embeddings.device, dtype=text_embeddings.dtype), # Ensure device/dtype match\n",
    "                text_emb_after\n",
    "            ], dim=0)\n",
    "            new_input_embeds.append(cur_new_embed)\n",
    "\n",
    "            # Construct the new attention mask for this sample\n",
    "            # If original mask was provided, use it; otherwise create from input_ids\n",
    "            current_attention_mask = attention_mask[batch_idx] if attention_mask is not None else (input_ids[batch_idx] != tokenizer.pad_token_id).long()\n",
    "            mask_before = current_attention_mask[:image_token_start_index]\n",
    "            mask_image = torch.ones(num_image_patches, dtype=torch.long, device=current_attention_mask.device)\n",
    "            mask_after = current_attention_mask[image_token_start_index + 1:]\n",
    "            cur_new_mask = torch.cat([\n",
    "                mask_before,\n",
    "                mask_image,\n",
    "                mask_after\n",
    "            ], dim=0)\n",
    "            new_attention_mask.append(cur_new_mask)\n",
    "\n",
    "            # Construct the new labels for this sample if needed\n",
    "            if new_labels is not None and labels is not None:\n",
    "                label_before = labels[batch_idx, :image_token_start_index]\n",
    "                label_image = torch.full((num_image_patches,), self.ignore_index, dtype=torch.long, device=labels.device)\n",
    "                label_after = labels[batch_idx, image_token_start_index + 1:]\n",
    "                cur_new_label = torch.cat([\n",
    "                    label_before,\n",
    "                    label_image,\n",
    "                    label_after\n",
    "                ], dim=0)\n",
    "                new_labels.append(cur_new_label)\n",
    "\n",
    "        # 4. Pad the sequences in the batch to the maximum length\n",
    "        # batch_first=True ensures output shape is (B, S_final, D_llm)\n",
    "        padded_input_embeds = pad_sequence(new_input_embeds, batch_first=True, padding_value=0.0)\n",
    "        padded_attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)\n",
    "        padded_labels = None\n",
    "        if new_labels is not None:\n",
    "            padded_labels = pad_sequence(new_labels, batch_first=True, padding_value=self.ignore_index)\n",
    "\n",
    "        # 5. Pass combined embeddings and new mask to the language model\n",
    "        outputs: CausalLMOutputWithPast = self.language_model(\n",
    "            inputs_embeds=padded_input_embeds,\n",
    "            attention_mask=padded_attention_mask,\n",
    "            labels=padded_labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### BaselineLLaVAModel\n",
       "\n",
       ">      BaselineLLaVAModel (config:dict)\n",
       "\n",
       "*Baseline LLaVA 1.5 model combining Vision Encoder, Projector, and LLM.\n",
       "\n",
       "    Loads the specified Vision Encoder (CLIP) and LLM (Vicuna) from Hugging Face Hub,\n",
       "    along with the Projector. Handles freezing components, LLM embedding resizing,\n",
       "    and optionally applies LoRA to the LLM based on the configuration.\n",
       "    Implements `encode_image`, activation checkpointing, and the main `forward` pass.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### BaselineLLaVAModel\n",
       "\n",
       ">      BaselineLLaVAModel (config:dict)\n",
       "\n",
       "*Baseline LLaVA 1.5 model combining Vision Encoder, Projector, and LLM.\n",
       "\n",
       "    Loads the specified Vision Encoder (CLIP) and LLM (Vicuna) from Hugging Face Hub,\n",
       "    along with the Projector. Handles freezing components, LLM embedding resizing,\n",
       "    and optionally applies LoRA to the LLM based on the configuration.\n",
       "    Implements `encode_image`, activation checkpointing, and the main `forward` pass.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BaselineLLaVAModel, name='BaselineLLaVAModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from configs/config.yaml\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7518a9e0062481e975ed420461c4fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n",
      "Applying LoRA...\n",
      "LoRA applied. Config: r=8, alpha=16, dropout=0.05, modules=['q_proj', 'v_proj']\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "LLM embedding size already matches tokenizer size. No resizing needed.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "BaselineLLaVAModel test passed!\n",
      "  - Projector exists: True (Type: <class '__main__.LLaVAProjector'>)\n",
      "  - Vision Tower exists: True (Type: <class 'transformers.models.clip.modeling_clip.CLIPVisionModel'>)\n",
      "  - Language Model exists: True (Type: <class 'peft.peft_model.PeftModelForCausalLM'>)\n",
      "\n",
      "Testing encode_image...\n",
      "Image features shape: torch.Size([2, 576, 1024])\n",
      "encode_image test passed!\n",
      "\n",
      "Testing LLM embedding size...\n",
      "LLM embedding vocab size: 32001\n",
      "Tokenizer vocab size: 32001\n",
      "LLM embedding size test passed!\n",
      "Cleaned up baseline_model\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# Test BaselineLLaVAModel instantiation including LLM loading and embedding resize\n",
    "import torch, gc # Add torch and gc\n",
    "from transformers import AutoModelForCausalLM, CLIPVisionModel # Add imports\n",
    "\n",
    "try:\n",
    "    config_path = 'configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # --- Test with LoRA enabled (assuming config enables it) --- \n",
    "    config['model']['peft']['use_lora'] = True # Ensure LoRA is tested\n",
    "    config['model']['use_activation_checkpointing'] = False # Disable checkpointing for this basic test\n",
    "    \n",
    "    baseline_model = BaselineLLaVAModel(config)\n",
    "\n",
    "    # Basic checks\n",
    "    assert isinstance(baseline_model, nn.Module)\n",
    "    assert baseline_model.projector is not None\n",
    "    assert baseline_model.vision_tower is not None\n",
    "    assert isinstance(baseline_model.vision_tower, CLIPVisionModel)\n",
    "    assert baseline_model.language_model is not None # Should be loaded now\n",
    "    # Check if LLM is wrapped by PeftModel if LoRA is enabled\n",
    "    if _peft_available and config['model']['peft']['use_lora']:\n",
    "         assert isinstance(baseline_model.language_model, PeftModel), f\"Expected PeftModel, but got {type(baseline_model.language_model)}\"\n",
    "         print(\"Language model correctly wrapped by PeftModel.\")\n",
    "    else:\n",
    "         assert isinstance(baseline_model.language_model, AutoModelForCausalLM)\n",
    "         print(\"Language model is standard AutoModelForCausalLM (LoRA disabled or peft not available).\")\n",
    "         \n",
    "    print(\"BaselineLLaVAModel test passed!\")\n",
    "    print(f\"  - Projector exists: {baseline_model.projector is not None} (Type: {type(baseline_model.projector)})\")\n",
    "    print(f\"  - Vision Tower exists: {baseline_model.vision_tower is not None} (Type: {type(baseline_model.vision_tower)})\")\n",
    "    print(f\"  - Language Model exists: {baseline_model.language_model is not None} (Type: {type(baseline_model.language_model)})\")\n",
    "\n",
    "    # Test encode_image helper (already tested, but good to re-check)\n",
    "    print(\"\\nTesting encode_image...\")\n",
    "    batch_size = 2\n",
    "    dummy_pixel_values = torch.randn(batch_size, 3, 336, 336) # Example image batch\n",
    "    image_features = baseline_model.encode_image(dummy_pixel_values)\n",
    "\n",
    "    assert image_features is not None\n",
    "    expected_shape = (batch_size, 576, 1024)\n",
    "    assert image_features.shape == expected_shape, f\"Expected shape {expected_shape}, but got {image_features.shape}\"\n",
    "    print(f\"Image features shape: {image_features.shape}\")\n",
    "    print(\"encode_image test passed!\")\n",
    "\n",
    "    # Test embedding size\n",
    "    print(\"\\nTesting LLM embedding size...\")\n",
    "    llm_embeddings = baseline_model.get_input_embeddings()\n",
    "    llm_vocab_size = llm_embeddings.weight.size(0)\n",
    "    tokenizer_vocab_size = len(tokenizer)\n",
    "    print(f\"LLM embedding vocab size: {llm_vocab_size}\")\n",
    "    print(f\"Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
    "    assert llm_vocab_size == tokenizer_vocab_size, \\\n",
    "        f\"Mismatch! LLM embedding size ({llm_vocab_size}) != Tokenizer size ({tokenizer_vocab_size})\"\n",
    "    print(\"LLM embedding size test passed!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file {config_path} not found. Skipping BaselineLLaVAModel test.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Skipping test due to ImportError: {e}. (Likely `peft` is missing)\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during BaselineLLaVAModel test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up large model from memory if running tests in a notebook\n",
    "# import gc # Already imported\n",
    "if 'baseline_model' in locals():\n",
    "    # Ensure model is moved to CPU before deleting if it was on GPU\n",
    "    if hasattr(baseline_model, 'vision_tower') and baseline_model.vision_tower is not None:\n",
    "        baseline_model.vision_tower.to('cpu')\n",
    "    if hasattr(baseline_model, 'language_model') and baseline_model.language_model is not None:\n",
    "        baseline_model.language_model.to('cpu')\n",
    "    if hasattr(baseline_model, 'projector') and baseline_model.projector is not None:\n",
    "        baseline_model.projector.to('cpu')\n",
    "    del baseline_model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Cleaned up baseline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from configs/config.yaml\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fa498f41f3420c845110d269361447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n",
      "Applying LoRA...\n",
      "LoRA applied. Config: r=8, alpha=16, dropout=0.05, modules=['q_proj', 'v_proj']\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "LLM embedding size already matches tokenizer size. No resizing needed.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Running forward pass test...\n",
      "Created dummy inputs.\n",
      "Original input_ids shape: torch.Size([2, 15])\n",
      "Padded inputs_embeds shape: (2, 586, 4096)\n",
      "Padded attention_mask shape: (2, 586)\n",
      "Padded labels shape: (2, 586)\n",
      "Output logits shape: torch.Size([2, 586, 32001])\n",
      "Output loss: tensor(10.3799, grad_fn=<NllLossBackward0>)\n",
      "Forward pass test successful!\n",
      "Cleaned up forward_test_model\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# Test the implemented forward pass\n",
    "import torch, gc\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "try:\n",
    "    config_path = 'configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # Ensure LoRA is enabled for this test if peft is available\n",
    "    if _peft_available:\n",
    "         config['model']['peft']['use_lora'] = True\n",
    "    else:\n",
    "         config['model']['peft']['use_lora'] = False\n",
    "    config['model']['use_activation_checkpointing'] = False # Ensure checkpointing is off for this basic test\n",
    "\n",
    "    # Instantiate the model\n",
    "    # Reduce memory usage for testing if possible (e.g., use smaller test models if available)\n",
    "    # Or ensure enough memory is allocated\n",
    "    forward_test_model = BaselineLLaVAModel(config)\n",
    "\n",
    "    # Set model to evaluation mode for testing (disables dropout etc.)\n",
    "    forward_test_model.eval()\n",
    "\n",
    "    print(\"Running forward pass test...\")\n",
    "    # Prepare dummy inputs\n",
    "    batch_size = 2\n",
    "    seq_len = 15 # Short sequence for testing\n",
    "    img_size = 336 # From config\n",
    "    num_patches = 576 # (336/14)^2\n",
    "    llm_hidden_dim = config['model']['projector']['output_dim']\n",
    "    tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "    dummy_pixel_values = torch.randn(batch_size, 3, img_size, img_size)\n",
    "    # Create input_ids with placeholder\n",
    "    dummy_input_ids = torch.randint(1, tokenizer_vocab_size, (batch_size, seq_len), dtype=torch.long)\n",
    "    placeholder_idx = 5 # Place the image token marker at index 5\n",
    "    dummy_input_ids[:, placeholder_idx] = IMAGE_TOKEN_INDEX_PLACEHOLDER\n",
    "    # Create attention mask (mask padding if any, here assume no padding initially)\n",
    "    dummy_attention_mask = torch.ones_like(dummy_input_ids)\n",
    "    # Create labels (copy input_ids, mask placeholder and potentially prompt)\n",
    "    dummy_labels = dummy_input_ids.clone()\n",
    "    dummy_labels[:, :placeholder_idx+1] = IGNORE_INDEX # Mask prompt + image token\n",
    "\n",
    "    print(\"Created dummy inputs.\")\n",
    "    print(f\"Original input_ids shape: {dummy_input_ids.shape}\")\n",
    "\n",
    "    # Perform forward pass\n",
    "    with torch.no_grad(): # Disable gradient calculation for testing inference\n",
    "         outputs = forward_test_model(\n",
    "            pixel_values=dummy_pixel_values,\n",
    "            input_ids=dummy_input_ids,\n",
    "            attention_mask=dummy_attention_mask,\n",
    "            labels=dummy_labels\n",
    "         )\n",
    "\n",
    "    # Check output type and attributes\n",
    "    assert isinstance(outputs, CausalLMOutputWithPast)\n",
    "    assert hasattr(outputs, 'logits')\n",
    "    assert outputs.logits is not None\n",
    "    assert hasattr(outputs, 'loss') # Should have loss since labels were provided\n",
    "    assert outputs.loss is not None\n",
    "\n",
    "    # Check output shapes\n",
    "    expected_seq_len = seq_len - 1 + num_patches\n",
    "    expected_logits_shape = (batch_size, expected_seq_len, tokenizer_vocab_size)\n",
    "\n",
    "    # Print shapes from inside forward pass (for debugging if needed)\n",
    "    # Accessing internal shapes requires modification or inspection\n",
    "    # Instead, check the final output shapes\n",
    "    print(f\"Padded inputs_embeds shape: ({batch_size}, {expected_seq_len}, {llm_hidden_dim})\") # Expected shape after padding\n",
    "    print(f\"Padded attention_mask shape: ({batch_size}, {expected_seq_len})\") # Expected shape after padding\n",
    "    print(f\"Padded labels shape: ({batch_size}, {expected_seq_len})\") # Expected shape after padding\n",
    "\n",
    "    assert outputs.logits.shape == expected_logits_shape, \\\n",
    "        f\"Expected logits shape {expected_logits_shape}, but got {outputs.logits.shape}\"\n",
    "    print(f\"Output logits shape: {outputs.logits.shape}\")\n",
    "    print(f\"Output loss: {outputs.loss}\")\n",
    "    print(\"Forward pass test successful!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file {config_path} not found. Skipping forward pass test.\")\n",
    "except ImportError as e:\n",
    "     print(f\"Skipping forward pass test due to ImportError: {e}. (Likely `peft` is missing)\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during forward pass test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up\n",
    "if 'forward_test_model' in locals():\n",
    "    if hasattr(forward_test_model, 'vision_tower') and forward_test_model.vision_tower is not None:\n",
    "        forward_test_model.vision_tower.to('cpu')\n",
    "    if hasattr(forward_test_model, 'language_model') and forward_test_model.language_model is not None:\n",
    "        forward_test_model.language_model.to('cpu')\n",
    "    if hasattr(forward_test_model, 'projector') and forward_test_model.projector is not None:\n",
    "        forward_test_model.projector.to('cpu')\n",
    "    del forward_test_model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Cleaned up forward_test_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
