{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Architecture\n",
    "\n",
    "> Defines the baseline LLaVA 1.5 model architecture, including the projector and the combined model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n",
      "Loaded config from configs/config.yaml\n",
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n",
      "CLIP Mean: [0.48145466, 0.4578275, 0.40821073]\n",
      "CLIP Std: [0.26862954, 0.26130258, 0.27577711]\n",
      "Fastai Normalize Transform: Normalize -- {'mean': tensor([[[[0.4815]],\n",
      "\n",
      "         [[0.4578]],\n",
      "\n",
      "         [[0.4082]]]], device='cuda:0'), 'std': tensor([[[[0.2686]],\n",
      "\n",
      "         [[0.2613]],\n",
      "\n",
      "         [[0.2758]]]], device='cuda:0'), 'axes': (0, 2, 3)}\n",
      "(enc:2,dec:2)\n",
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence # For padding variable length sequences\n",
    "from transformers import CLIPVisionModel, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling, CausalLMOutputWithPast # For type hints\n",
    "import warnings\n",
    "from typing import Optional # Added for type hinting\n",
    "\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, PeftModel # Import PEFT components\n",
    "    _peft_available = True\n",
    "except ImportError:\n",
    "    print(\"Warning: peft library not found. LoRA functionality will be disabled.\")\n",
    "    LoraConfig, get_peft_model, PeftModel = None, None, None # Define as None if not available\n",
    "    _peft_available = False\n",
    "\n",
    "from llava.utils import load_config # Assuming utils notebook is created\n",
    "from llava.data.preprocessing import IMAGE_TOKEN_INDEX_PLACEHOLDER, IGNORE_INDEX, tokenizer, DEFAULT_IMAGE_TOKEN # Import constants and tokenizer\n",
    "\n",
    "# Ensure tokenizer is loaded (it should be from the import)\n",
    "if tokenizer is None:\n",
    "    print(\"Warning: Tokenizer could not be imported from data.preprocessing. Trying to load it again...\")\n",
    "    try:\n",
    "        # Need to load config first to get LLM name\n",
    "        _config_temp = load_config('configs/config.yaml')\n",
    "        _llm_name_temp = _config_temp.get('model', {}).get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')\n",
    "        _tokenizer_max_len_temp = _config_temp.get('data', {}).get('tokenizer_model_max_length', 2048)\n",
    "        _tokenizer_padding_side_temp = _config_temp.get('data', {}).get('tokenizer_padding_side', 'right')\n",
    "\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            _llm_name_temp,\n",
    "            model_max_length=_tokenizer_max_len_temp,\n",
    "            padding_side=_tokenizer_padding_side_temp,\n",
    "            use_fast=True,\n",
    "        )\n",
    "        # Re-add special token logic if necessary (should match preprocessing step)\n",
    "        # from llava.data.preprocessing import DEFAULT_IMAGE_TOKEN # Already imported\n",
    "        if DEFAULT_IMAGE_TOKEN not in tokenizer.get_vocab():\n",
    "             num_added = tokenizer.add_special_tokens({'additional_special_tokens': [DEFAULT_IMAGE_TOKEN]})\n",
    "             print(f\"Added {num_added} token(s) during re-load. New vocab size: {len(tokenizer)}\")\n",
    "        if tokenizer.pad_token is None:\n",
    "             tokenizer.pad_token = tokenizer.eos_token\n",
    "             print(f\"Set pad token to EOS during re-load.\")\n",
    "        print(f\"Tokenizer re-loaded successfully for {_llm_name_temp}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal Error: Could not load tokenizer. {e}\")\n",
    "        # Depending on the context, might raise an error or exit\n",
    "        raise ImportError(\"Tokenizer is essential and could not be loaded.\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Define Projector Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projector module connects the vision encoder's output to the language model's input space. In LLaVA 1.5, this is typically a simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLaVAProjector(nn.Module):\n",
    "    \"\"\"A simple 2-layer MLP projector to map vision features to LLM embedding space.\n",
    "\n",
    "    Maps input_dim (e.g., CLIP ViT-L output dim = 1024) to\n",
    "    output_dim (e.g., Vicuna-7B hidden dim = 4096).\n",
    "\n",
    "    Architecture: Linear -> GELU -> Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        \"\"\"Initializes the MLP projector.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimension of the input vision features.\n",
    "            output_dim: Dimension of the LLM's hidden space.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(output_dim, output_dim, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Projects the input features.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, num_patches, input_dim) or similar.\n",
    "\n",
    "        Returns:\n",
    "            Projected tensor of shape (batch_size, num_patches, output_dim).\n",
    "        \"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LLaVAProjector\n",
       "\n",
       ">      LLaVAProjector (input_dim:int, output_dim:int)\n",
       "\n",
       "*A simple 2-layer MLP projector to map vision features to LLM embedding space.\n",
       "\n",
       "    Maps input_dim (e.g., CLIP ViT-L output dim = 1024) to\n",
       "    output_dim (e.g., Vicuna-7B hidden dim = 4096).\n",
       "\n",
       "    Architecture: Linear -> GELU -> Linear*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LLaVAProjector\n",
       "\n",
       ">      LLaVAProjector (input_dim:int, output_dim:int)\n",
       "\n",
       "*A simple 2-layer MLP projector to map vision features to LLM embedding space.\n",
       "\n",
       "    Maps input_dim (e.g., CLIP ViT-L output dim = 1024) to\n",
       "    output_dim (e.g., Vicuna-7B hidden dim = 4096).\n",
       "\n",
       "    Architecture: Linear -> GELU -> Linear*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLaVAProjector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projector test passed!\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# Test projector instantiation and forward pass\n",
    "inp_dim = 1024\n",
    "out_dim = 4096\n",
    "projector = LLaVAProjector(input_dim=inp_dim, output_dim=out_dim)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "batch_size = 2\n",
    "num_patches = 576 # Example for 336x336 input with patch size 14\n",
    "dummy_input = torch.randn(batch_size, num_patches, inp_dim)\n",
    "\n",
    "# Perform forward pass\n",
    "output = projector(dummy_input)\n",
    "\n",
    "# Check output shape\n",
    "assert output.shape == (batch_size, num_patches, out_dim), f\"Expected shape {(batch_size, num_patches, out_dim)}, but got {output.shape}\"\n",
    "print(\"Projector test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2, 2.3, 2.4, 4.2: Define Baseline LLaVA Model & Integrate Components + LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class combines the Vision Encoder, the Projector, and the Language Model to form the complete LLaVA baseline model. We integrate the loading of the Vision Encoder (CLIP) and the Language Model (Vicuna), handle necessary configurations like embedding resizing, and **now integrate optional LoRA application using the `peft` library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaselineLLaVAModel(nn.Module):\n",
    "    \"\"\"Baseline LLaVA 1.5 model combining Vision Encoder, Projector, and LLM.\"\"\"\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config \n",
    "        self.model_config = config.get('model', {}) \n",
    "\n",
    "        self.vision_tower = None\n",
    "        self.language_model = None\n",
    "        self.projector = None\n",
    "\n",
    "        self.image_token_index_marker = self.model_config.get('image_token_index_marker', IMAGE_TOKEN_INDEX_PLACEHOLDER)\n",
    "        self.ignore_index = IGNORE_INDEX \n",
    "        self.vision_feature_layer = self.model_config.get('vision_feature_layer', -2)\n",
    "        self.vision_encoder_name = self.model_config.get('vision_encoder_name_or_path', 'openai/clip-vit-large-patch14-336')\n",
    "        self.llm_name = self.model_config.get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')\n",
    "\n",
    "        llm_hf_config = AutoConfig.from_pretrained(self.llm_name, trust_remote_code=True)\n",
    "        vision_hf_config = AutoConfig.from_pretrained(self.vision_encoder_name, trust_remote_code=True)\n",
    "\n",
    "        projector_cfg_from_yaml = self.model_config.get('projector', {})\n",
    "        \n",
    "        proj_input_dim_fallback = vision_hf_config.vision_config.hidden_size if hasattr(vision_hf_config, 'vision_config') else vision_hf_config.hidden_size\n",
    "        proj_input_dim = projector_cfg_from_yaml.get('input_dim', proj_input_dim_fallback)\n",
    "        proj_output_dim = projector_cfg_from_yaml.get('output_dim', llm_hf_config.hidden_size)\n",
    "\n",
    "        print(f\"Initializing Projector: Input Dim={proj_input_dim}, Output Dim={proj_output_dim}\")\n",
    "        self.projector = LLaVAProjector(proj_input_dim, proj_output_dim)\n",
    "\n",
    "        self.load_vision_tower(expected_output_dim=proj_input_dim)\n",
    "        self.load_language_model() \n",
    "        self.resize_llm_embeddings()\n",
    "        self.apply_activation_checkpointing()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            target_device = torch.device(\"cuda:0\")\n",
    "            if self.vision_tower is not None and next(self.vision_tower.parameters()).device != target_device:\n",
    "                print(f\"Moving vision_tower to {target_device}\")\n",
    "                self.vision_tower.to(target_device)\n",
    "            if self.projector is not None and next(self.projector.parameters()).device != target_device:\n",
    "                print(f\"Moving projector to {target_device}\")\n",
    "                self.projector.to(target_device)\n",
    "            if self.language_model is not None:\n",
    "                 llm_device = next(self.language_model.parameters()).device\n",
    "                 print(f\"Language model's first parameter is on device: {llm_device}\")\n",
    "\n",
    "\n",
    "    def load_vision_tower(self, expected_output_dim: int):\n",
    "        try:\n",
    "            print(f\"Loading Vision Tower: {self.vision_encoder_name}...\")\n",
    "            self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_encoder_name, trust_remote_code=True)\n",
    "            print(f\"Vision Tower loaded successfully.\")\n",
    "            self.vision_tower.requires_grad_(False)\n",
    "            print(f\"Vision Tower weights frozen.\")\n",
    "            vision_output_dim = self.vision_tower.config.hidden_size\n",
    "            if vision_output_dim != expected_output_dim:\n",
    "                warnings.warn(\n",
    "                    f\"Vision Tower output dimension ({vision_output_dim}) does not match \"\n",
    "                    f\"Projector input dimension ({expected_output_dim}).\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Vision Tower ({self.vision_encoder_name}): {e}\")\n",
    "            self.vision_tower = None\n",
    "\n",
    "    def load_language_model(self):\n",
    "        try:\n",
    "            print(f\"Loading Language Model: {self.llm_name}...\")\n",
    "            \n",
    "            quantization_config_dict = self.model_config.get('quantization', {})\n",
    "            load_in_4bit = quantization_config_dict.get('load_in_4bit', False)\n",
    "            bnb_config = None\n",
    "            extra_kwargs = {\"trust_remote_code\": True}\n",
    "\n",
    "            if load_in_4bit:\n",
    "                if BitsAndBytesConfig is None:\n",
    "                    raise ImportError(\"bitsandbytes library is required for 4-bit quantization but not found.\")\n",
    "                \n",
    "                bnb_4bit_quant_type = quantization_config_dict.get('bnb_4bit_quant_type', \"nf4\")\n",
    "                bnb_4bit_compute_dtype_str = quantization_config_dict.get('bnb_4bit_compute_dtype', \"float16\")\n",
    "                compute_dtype = getattr(torch, bnb_4bit_compute_dtype_str)\n",
    "\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "                    bnb_4bit_compute_dtype=compute_dtype,\n",
    "                    bnb_4bit_use_double_quant=quantization_config_dict.get('bnb_4bit_use_double_quant', False),\n",
    "                )\n",
    "                print(f\"QLoRA enabled: Loading LLM in 4-bit with compute dtype {compute_dtype}.\")\n",
    "                if torch.cuda.is_available():\n",
    "                    extra_kwargs[\"device_map\"] = {\"\": 0} \n",
    "                    print(f\"  Setting device_map to {{'': 0}} for QLoRA.\")\n",
    "                else:\n",
    "                    print(\"  Warning: CUDA not available, QLoRA device_map will not be set to GPU. Model will load on CPU if possible.\")\n",
    "                extra_kwargs[\"quantization_config\"] = bnb_config\n",
    "            \n",
    "            self.language_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.llm_name,\n",
    "                **extra_kwargs \n",
    "            )\n",
    "            print(f\"Language Model loaded successfully.\")\n",
    "            \n",
    "            if not load_in_4bit: \n",
    "                 self.language_model.requires_grad_(False)\n",
    "                 print(f\"Base Language Model weights frozen (non-QLoRA path).\")\n",
    "\n",
    "            peft_config_dict = self.model_config.get('peft', {}) \n",
    "            use_lora = peft_config_dict.get('use_lora', False)\n",
    "\n",
    "            if use_lora:\n",
    "                if not _peft_available:\n",
    "                    print(\"Warning: `use_lora` is true in config, but peft library is not installed. Skipping LoRA.\")\n",
    "                else:\n",
    "                    if isinstance(self.language_model, PeftModel):\n",
    "                        print(\"Language model is already a PeftModel. Ensure LoRA config matches if re-applying.\")\n",
    "                    \n",
    "                    print(\"Applying LoRA...\")\n",
    "                    lora_r = peft_config_dict.get('lora_r', 8)\n",
    "                    lora_alpha = peft_config_dict.get('lora_alpha', 16)\n",
    "                    lora_dropout = peft_config_dict.get('lora_dropout', 0.05)\n",
    "                    target_modules = peft_config_dict.get('target_modules', ['q_proj', 'v_proj'])\n",
    "                    \n",
    "                    current_lora_config = LoraConfig( \n",
    "                        r=lora_r,\n",
    "                        lora_alpha=lora_alpha,\n",
    "                        target_modules=target_modules,\n",
    "                        lora_dropout=lora_dropout,\n",
    "                        bias=\"none\",\n",
    "                    )\n",
    "                    self.language_model = get_peft_model(self.language_model, current_lora_config)\n",
    "                    print(f\"LoRA applied. Config: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}, modules={target_modules}\")\n",
    "                    self.language_model.print_trainable_parameters()\n",
    "            else:\n",
    "                print(\"LoRA is disabled in the configuration.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Language Model ({self.llm_name}): {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            self.language_model = None\n",
    "\n",
    "    def resize_llm_embeddings(self):\n",
    "        if self.language_model is None or tokenizer is None:\n",
    "            print(\"Warning: Cannot resize LLM embeddings. LLM or tokenizer not available.\")\n",
    "            return\n",
    "\n",
    "        current_embeddings = self.get_input_embeddings()\n",
    "        current_vocab_size = current_embeddings.weight.size(0)\n",
    "        target_vocab_size = len(tokenizer)\n",
    "\n",
    "        if current_vocab_size != target_vocab_size:\n",
    "            print(f\"Resizing LLM token embeddings from {current_vocab_size} to {target_vocab_size} (tokenizer size)...\")\n",
    "            self.language_model.resize_token_embeddings(target_vocab_size)\n",
    "            print(\"LLM token embeddings resized.\")\n",
    "        else:\n",
    "            print(\"LLM embedding size already matches tokenizer size. No resizing needed.\")\n",
    "            \n",
    "    def apply_activation_checkpointing(self):\n",
    "        use_checkpointing = self.model_config.get('use_activation_checkpointing', False)\n",
    "        if use_checkpointing:\n",
    "            if self.language_model is not None:\n",
    "                 model_to_checkpoint = self.language_model \n",
    "                 if hasattr(model_to_checkpoint, 'gradient_checkpointing_enable'):\n",
    "                     try:\n",
    "                         model_to_checkpoint.gradient_checkpointing_enable()\n",
    "                         print(\"Activation checkpointing enabled for the language model.\")\n",
    "                     except Exception as e:\n",
    "                         print(f\"Warning: Failed to enable activation checkpointing: {e}\")\n",
    "                 else:\n",
    "                     print(\"Warning: Language model does not have 'gradient_checkpointing_enable' method.\")\n",
    "            else:\n",
    "                 print(\"Warning: Activation checkpointing enabled in config, but language model is not loaded.\")\n",
    "        else:\n",
    "             print(\"Activation checkpointing is disabled in the configuration.\")\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        if self.language_model is None:\n",
    "            raise ValueError(\"Language model not loaded.\")\n",
    "        return self.language_model.get_input_embeddings()\n",
    "\n",
    "    def encode_image(self, pixel_values: torch.Tensor) -> torch.Tensor | None:\n",
    "        if self.vision_tower is None:\n",
    "            print(\"Error: Vision tower not loaded, cannot encode image.\")\n",
    "            return None\n",
    "        try:\n",
    "            vision_device = next(self.vision_tower.parameters()).device\n",
    "            vision_dtype = next(self.vision_tower.parameters()).dtype\n",
    "            pixel_values = pixel_values.to(vision_device, dtype=vision_dtype)\n",
    "\n",
    "            vision_outputs: BaseModelOutputWithPooling = self.vision_tower(\n",
    "                pixel_values,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            image_features = vision_outputs.hidden_states[self.vision_feature_layer]\n",
    "            image_features = image_features[:, 1:, :] \n",
    "            return image_features\n",
    "        except Exception as e:\n",
    "            print(f\"Error during image encoding: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def forward(self, \n",
    "                *args: Any, # To catch positional arguments like a single dict from Learner.summary\n",
    "                pixel_values: Optional[torch.Tensor] = None,\n",
    "                input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None, \n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                **kwargs: Any\n",
    "               ) -> CausalLMOutputWithPast:\n",
    "        \n",
    "        # Handle cases where inputs might be packed in a dictionary (e.g., from Learner)\n",
    "        # or passed as keyword arguments directly.\n",
    "        batch_dict = {}\n",
    "        if len(args) == 1 and isinstance(args[0], dict):\n",
    "            batch_dict = args[0]\n",
    "        elif len(args) > 0: # Attempt to map positional args if not a dict; less robust\n",
    "            # This path is less ideal; prefer keyword args or a single dict\n",
    "            if pixel_values is None and isinstance(args[0], torch.Tensor): pixel_values = args[0]\n",
    "            if input_ids is None and len(args) > 1 and isinstance(args[1], torch.Tensor): input_ids = args[1]\n",
    "            if attention_mask is None and len(args) > 2 and (args[2] is None or isinstance(args[2], torch.Tensor)): attention_mask = args[2]\n",
    "            if labels is None and len(args) > 3 and (args[3] is None or isinstance(args[3], torch.Tensor)): labels = args[3]\n",
    "\n",
    "        # Update from kwargs if they were provided\n",
    "        if 'pixel_values' in kwargs: pixel_values = kwargs['pixel_values']\n",
    "        if 'input_ids' in kwargs: input_ids = kwargs['input_ids']\n",
    "        if 'attention_mask' in kwargs: attention_mask = kwargs['attention_mask']\n",
    "        if 'labels' in kwargs: labels = kwargs['labels']\n",
    "        \n",
    "        # If batch_dict was populated, use its values preferentially or if others are None\n",
    "        if batch_dict:\n",
    "            pixel_values = batch_dict.get('pixel_values', pixel_values)\n",
    "            input_ids = batch_dict.get('input_ids', input_ids)\n",
    "            attention_mask = batch_dict.get('attention_mask', attention_mask)\n",
    "            labels = batch_dict.get('labels', labels)\n",
    "\n",
    "        if pixel_values is None or input_ids is None:\n",
    "            # This error will be caught by the Learner.summary() or training loop if inputs are malformed.\n",
    "            raise ValueError(\"forward() missing required arguments: pixel_values and input_ids must be provided.\")\n",
    "\n",
    "        if self.language_model is None or self.vision_tower is None or self.projector is None:\n",
    "            raise RuntimeError(\"Model components (LLM, Vision Tower, Projector) are not fully loaded.\")\n",
    "\n",
    "        image_features = self.encode_image(pixel_values)\n",
    "        if image_features is None:\n",
    "            raise RuntimeError(\"Image encoding failed.\")\n",
    "        \n",
    "        projector_device = next(self.projector.parameters()).device\n",
    "        image_features = image_features.to(projector_device)\n",
    "        projected_image_features = self.projector(image_features)\n",
    "        num_image_patches = projected_image_features.shape[1]\n",
    "        \n",
    "        embedding_layer = self.get_input_embeddings()\n",
    "        target_device = embedding_layer.weight.device\n",
    "        \n",
    "        input_ids_clone = input_ids.clone().to(target_device) \n",
    "        input_ids_clone[input_ids_clone == self.image_token_index_marker] = 0 \n",
    "        \n",
    "        text_embeddings = embedding_layer(input_ids_clone)\n",
    "        projected_image_features = projected_image_features.to(target_device, dtype=text_embeddings.dtype)\n",
    "\n",
    "        new_input_embeds = []\n",
    "        new_labels = [] if labels is not None else None\n",
    "        new_attention_mask = []\n",
    "\n",
    "        for batch_idx in range(input_ids.shape[0]):\n",
    "            current_input_ids_slice = input_ids[batch_idx].to(target_device)\n",
    "            image_token_indices = torch.where(current_input_ids_slice == self.image_token_index_marker)[0]\n",
    "            \n",
    "            if len(image_token_indices) == 0:\n",
    "                warnings.warn(f\"Image token placeholder {self.image_token_index_marker} not found in batch index {batch_idx}. Skipping image features.\")\n",
    "                new_input_embeds.append(text_embeddings[batch_idx])\n",
    "                current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()\n",
    "                new_attention_mask.append(current_attention_mask_slice)\n",
    "                if new_labels is not None and labels is not None:\n",
    "                    new_labels.append(labels[batch_idx].to(target_device))\n",
    "                continue\n",
    "\n",
    "            image_token_start_index = image_token_indices[0].item()\n",
    "            text_emb_before = text_embeddings[batch_idx, :image_token_start_index]\n",
    "            text_emb_after = text_embeddings[batch_idx, image_token_start_index + 1:]\n",
    "\n",
    "            cur_new_embed = torch.cat([\n",
    "                text_emb_before,\n",
    "                projected_image_features[batch_idx], \n",
    "                text_emb_after\n",
    "            ], dim=0)\n",
    "            new_input_embeds.append(cur_new_embed)\n",
    "\n",
    "            current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()\n",
    "            mask_before = current_attention_mask_slice[:image_token_start_index]\n",
    "            mask_image = torch.ones(num_image_patches, dtype=torch.long, device=target_device)\n",
    "            mask_after = current_attention_mask_slice[image_token_start_index + 1:]\n",
    "            cur_new_mask = torch.cat([mask_before, mask_image, mask_after], dim=0)\n",
    "            new_attention_mask.append(cur_new_mask)\n",
    "\n",
    "            if new_labels is not None and labels is not None:\n",
    "                current_labels_slice = labels[batch_idx].to(target_device)\n",
    "                label_before = current_labels_slice[:image_token_start_index]\n",
    "                label_image = torch.full((num_image_patches,), self.ignore_index, dtype=torch.long, device=target_device)\n",
    "                label_after = current_labels_slice[image_token_start_index + 1:]\n",
    "                cur_new_label = torch.cat([label_before, label_image, label_after], dim=0)\n",
    "                new_labels.append(cur_new_label)\n",
    "\n",
    "        padded_input_embeds = pad_sequence(new_input_embeds, batch_first=True, padding_value=0.0)\n",
    "        padded_attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)\n",
    "        padded_labels = None\n",
    "        if new_labels is not None:\n",
    "            padded_labels = pad_sequence(new_labels, batch_first=True, padding_value=self.ignore_index)\n",
    "        \n",
    "        outputs: CausalLMOutputWithPast = self.language_model(\n",
    "            inputs_embeds=padded_input_embeds, \n",
    "            attention_mask=padded_attention_mask, \n",
    "            labels=padded_labels, \n",
    "            return_dict=True\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### BaselineLLaVAModel\n",
       "\n",
       ">      BaselineLLaVAModel (config:dict)\n",
       "\n",
       "*Baseline LLaVA 1.5 model combining Vision Encoder, Projector, and LLM.\n",
       "\n",
       "    Loads the specified Vision Encoder (CLIP) and LLM (Vicuna) from Hugging Face Hub,\n",
       "    along with the Projector. Handles freezing components, LLM embedding resizing,\n",
       "    and optionally applies LoRA to the LLM based on the configuration.\n",
       "    Implements `encode_image`, activation checkpointing, and the main `forward` pass.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### BaselineLLaVAModel\n",
       "\n",
       ">      BaselineLLaVAModel (config:dict)\n",
       "\n",
       "*Baseline LLaVA 1.5 model combining Vision Encoder, Projector, and LLM.\n",
       "\n",
       "    Loads the specified Vision Encoder (CLIP) and LLM (Vicuna) from Hugging Face Hub,\n",
       "    along with the Projector. Handles freezing components, LLM embedding resizing,\n",
       "    and optionally applies LoRA to the LLM based on the configuration.\n",
       "    Implements `encode_image`, activation checkpointing, and the main `forward` pass.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BaselineLLaVAModel, name='BaselineLLaVAModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n",
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n",
      "QLoRA enabled: Loading LLM in 4-bit with compute dtype torch.float16.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e59b06bd6c8459991733690da385c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n",
      "Applying LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA applied. Config: r=8, alpha=16, dropout=0.05, modules=['q_proj', 'v_proj']\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n",
      "Resizing LLM token embeddings from 32000 to 32001 (tokenizer size)...\n",
      "LLM token embeddings resized.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Language model correctly wrapped by PeftModel.\n",
      "BaselineLLaVAModel test passed!\n",
      "  - Projector exists: True (Type: <class '__main__.LLaVAProjector'>)\n",
      "  - Vision Tower exists: True (Type: <class 'transformers.models.clip.modeling_clip.CLIPVisionModel'>)\n",
      "  - Language Model exists: True (Type: <class 'peft.peft_model.PeftModel'>)\n",
      "\n",
      "Testing encode_image...\n",
      "Image features shape: torch.Size([2, 576, 1024])\n",
      "encode_image test passed!\n",
      "\n",
      "Testing LLM embedding size...\n",
      "LLM embedding vocab size: 32001\n",
      "Tokenizer vocab size: 32001\n",
      "LLM embedding size test passed!\n",
      "Cleaned up baseline_model\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# Test BaselineLLaVAModel instantiation including LLM loading and embedding resize\n",
    "import torch, gc # Add torch and gc\n",
    "from transformers import AutoModelForCausalLM, CLIPVisionModel # Add imports\n",
    "\n",
    "try:\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # --- Test with LoRA enabled (assuming config enables it) --- \n",
    "    config['model']['peft']['use_lora'] = True # Ensure LoRA is tested\n",
    "    config['model']['use_activation_checkpointing'] = False # Disable checkpointing for this basic test\n",
    "    \n",
    "    baseline_model = BaselineLLaVAModel(config)\n",
    "\n",
    "    # Basic checks\n",
    "    assert isinstance(baseline_model, nn.Module)\n",
    "    assert baseline_model.projector is not None\n",
    "    assert baseline_model.vision_tower is not None\n",
    "    assert isinstance(baseline_model.vision_tower, CLIPVisionModel)\n",
    "    assert baseline_model.language_model is not None # Should be loaded now\n",
    "    # Check if LLM is wrapped by PeftModel if LoRA is enabled\n",
    "    if _peft_available and config['model']['peft']['use_lora']:\n",
    "         assert isinstance(baseline_model.language_model, PeftModel), f\"Expected PeftModel, but got {type(baseline_model.language_model)}\"\n",
    "         print(\"Language model correctly wrapped by PeftModel.\")\n",
    "    else:\n",
    "         assert isinstance(baseline_model.language_model, AutoModelForCausalLM)\n",
    "         print(\"Language model is standard AutoModelForCausalLM (LoRA disabled or peft not available).\")\n",
    "         \n",
    "    print(\"BaselineLLaVAModel test passed!\")\n",
    "    print(f\"  - Projector exists: {baseline_model.projector is not None} (Type: {type(baseline_model.projector)})\")\n",
    "    print(f\"  - Vision Tower exists: {baseline_model.vision_tower is not None} (Type: {type(baseline_model.vision_tower)})\")\n",
    "    print(f\"  - Language Model exists: {baseline_model.language_model is not None} (Type: {type(baseline_model.language_model)})\")\n",
    "\n",
    "    # Test encode_image helper (already tested, but good to re-check)\n",
    "    print(\"\\nTesting encode_image...\")\n",
    "    batch_size = 2\n",
    "    dummy_pixel_values = torch.randn(batch_size, 3, 336, 336) # Example image batch\n",
    "    image_features = baseline_model.encode_image(dummy_pixel_values)\n",
    "\n",
    "    assert image_features is not None\n",
    "    expected_shape = (batch_size, 576, 1024)\n",
    "    assert image_features.shape == expected_shape, f\"Expected shape {expected_shape}, but got {image_features.shape}\"\n",
    "    print(f\"Image features shape: {image_features.shape}\")\n",
    "    print(\"encode_image test passed!\")\n",
    "\n",
    "    # Test embedding size\n",
    "    print(\"\\nTesting LLM embedding size...\")\n",
    "    llm_embeddings = baseline_model.get_input_embeddings()\n",
    "    llm_vocab_size = llm_embeddings.weight.size(0)\n",
    "    tokenizer_vocab_size = len(tokenizer)\n",
    "    print(f\"LLM embedding vocab size: {llm_vocab_size}\")\n",
    "    print(f\"Tokenizer vocab size: {tokenizer_vocab_size}\")\n",
    "    assert llm_vocab_size == tokenizer_vocab_size, \\\n",
    "        f\"Mismatch! LLM embedding size ({llm_vocab_size}) != Tokenizer size ({tokenizer_vocab_size})\"\n",
    "    print(\"LLM embedding size test passed!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file {config_path} not found. Skipping BaselineLLaVAModel test.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Skipping test due to ImportError: {e}. (Likely `peft` is missing)\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during BaselineLLaVAModel test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up large model from memory if running tests in a notebook\n",
    "# import gc # Already imported\n",
    "if 'baseline_model' in locals():\n",
    "    # Ensure model is moved to CPU before deleting if it was on GPU\n",
    "    if hasattr(baseline_model, 'vision_tower') and baseline_model.vision_tower is not None:\n",
    "        baseline_model.vision_tower.to('cpu')\n",
    "    if hasattr(baseline_model, 'language_model') and baseline_model.language_model is not None:\n",
    "        baseline_model.language_model.to('cpu')\n",
    "    if hasattr(baseline_model, 'projector') and baseline_model.projector is not None:\n",
    "        baseline_model.projector.to('cpu')\n",
    "    del baseline_model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Cleaned up baseline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n",
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n",
      "QLoRA enabled: Loading LLM in 4-bit with compute dtype torch.float16.\n",
      "  Setting device_map to {'': 0} for QLoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4d5d661e0743d9acd4126d94493de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Applying LoRA...\n",
      "LoRA applied. Config: r=8, alpha=16, dropout=0.05, modules=['q_proj', 'v_proj']\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n",
      "Resizing LLM token embeddings from 32000 to 32001 (tokenizer size)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM token embeddings resized.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Moving vision_tower to cuda:0\n",
      "Moving projector to cuda:0\n",
      "Language model's first parameter is on device: cuda:0\n",
      "Model is on device: cuda:0\n",
      "Running forward pass test...\n",
      "Created dummy inputs and moved to model device.\n",
      "Original input_ids shape: torch.Size([2, 15]), device: cuda:0\n",
      "Padded inputs_embeds shape (expected): (2, 590, 4096)\n",
      "Padded attention_mask shape (expected): (2, 590)\n",
      "Padded labels shape (expected): (2, 590)\n",
      "Output logits shape: torch.Size([2, 590, 32001])\n",
      "Output loss: 11.768798828125\n",
      "Forward pass test successful!\n",
      "Cleaned up forward_test_model\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# Test the implemented forward pass\n",
    "import torch, gc\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "try:\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # --- Configure for QLoRA and LoRA for this test ---\n",
    "    if 'model' not in config: config['model'] = {}\n",
    "    if 'peft' not in config['model']: config['model']['peft'] = {}\n",
    "    if 'quantization' not in config['model']: config['model']['quantization'] = {}\n",
    "    \n",
    "    config['model']['quantization']['load_in_4bit'] = True # Enable QLoRA\n",
    "    config['model']['quantization']['bnb_4bit_compute_dtype'] = \"float16\" # Ensure a compatible dtype\n",
    "    config['model']['peft']['use_lora'] = True # QLoRA implies LoRA\n",
    "    config['model']['use_activation_checkpointing'] = False # Keep disabled for this basic test\n",
    "\n",
    "    # Instantiate the model\n",
    "    forward_test_model = BaselineLLaVAModel(config)\n",
    "    forward_test_model.eval() # Set model to evaluation mode\n",
    "\n",
    "    # Determine the device of the model\n",
    "    # (assuming all parts of the model are on the same device after initialization,\n",
    "    # which device_map=\"auto\" or BitsAndBytesConfig usually handles for quantized models)\n",
    "    model_device = next(forward_test_model.parameters()).device\n",
    "    print(f\"Model is on device: {model_device}\")\n",
    "\n",
    "    print(\"Running forward pass test...\")\n",
    "    # Prepare dummy inputs\n",
    "    batch_size = 2\n",
    "    seq_len = 15 \n",
    "    img_size = 336 \n",
    "    num_patches = 576 \n",
    "    llm_hidden_dim = config['model']['projector']['output_dim']\n",
    "    \n",
    "    # Ensure tokenizer is loaded and get vocab size\n",
    "    if tokenizer is None:\n",
    "        raise RuntimeError(\"Tokenizer is not loaded. Cannot proceed with test.\")\n",
    "    tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "    # Move dummy tensors to the model's device\n",
    "    dummy_pixel_values = torch.randn(batch_size, 3, img_size, img_size, device=model_device)\n",
    "    dummy_input_ids = torch.randint(1, tokenizer_vocab_size, (batch_size, seq_len), dtype=torch.long, device=model_device)\n",
    "    placeholder_idx = 5 \n",
    "    dummy_input_ids[:, placeholder_idx] = IMAGE_TOKEN_INDEX_PLACEHOLDER # This value is int, device doesn't apply until it's in a tensor\n",
    "    \n",
    "    dummy_attention_mask = torch.ones_like(dummy_input_ids, device=model_device)\n",
    "    dummy_labels = dummy_input_ids.clone() # Will also be on model_device\n",
    "    dummy_labels[:, :placeholder_idx+1] = IGNORE_INDEX \n",
    "\n",
    "    print(\"Created dummy inputs and moved to model device.\")\n",
    "    print(f\"Original input_ids shape: {dummy_input_ids.shape}, device: {dummy_input_ids.device}\")\n",
    "\n",
    "    # Perform forward pass\n",
    "    with torch.no_grad(): \n",
    "         outputs = forward_test_model(\n",
    "            pixel_values=dummy_pixel_values,\n",
    "            input_ids=dummy_input_ids,\n",
    "            attention_mask=dummy_attention_mask,\n",
    "            labels=dummy_labels\n",
    "         )\n",
    "\n",
    "    # Check output type and attributes\n",
    "    assert isinstance(outputs, CausalLMOutputWithPast)\n",
    "    assert hasattr(outputs, 'logits')\n",
    "    assert outputs.logits is not None\n",
    "    assert hasattr(outputs, 'loss') \n",
    "    assert outputs.loss is not None\n",
    "\n",
    "    # Check output shapes\n",
    "    expected_seq_len = seq_len - 1 + num_patches\n",
    "    expected_logits_shape = (batch_size, expected_seq_len, tokenizer_vocab_size)\n",
    "\n",
    "    print(f\"Padded inputs_embeds shape (expected): ({batch_size}, {expected_seq_len}, {llm_hidden_dim})\")\n",
    "    print(f\"Padded attention_mask shape (expected): ({batch_size}, {expected_seq_len})\")\n",
    "    print(f\"Padded labels shape (expected): ({batch_size}, {expected_seq_len})\")\n",
    "\n",
    "    assert outputs.logits.shape == expected_logits_shape, \\\n",
    "        f\"Expected logits shape {expected_logits_shape}, but got {outputs.logits.shape}\"\n",
    "    print(f\"Output logits shape: {outputs.logits.shape}\")\n",
    "    print(f\"Output loss: {outputs.loss}\")\n",
    "    print(\"Forward pass test successful!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Config file {config_path} not found. Skipping forward pass test.\")\n",
    "except ImportError as e:\n",
    "     print(f\"Skipping forward pass test due to ImportError: {e}. (Likely `peft` or `bitsandbytes` is missing)\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during forward pass test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up\n",
    "if 'forward_test_model' in locals():\n",
    "    # Models with device_map=\"auto\" or BitsAndBytes quantization might not need explicit .to('cpu')\n",
    "    # as their components can be on different devices or offloaded.\n",
    "    # Simply deleting the reference should be sufficient for GC.\n",
    "    del forward_test_model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Cleaned up forward_test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
