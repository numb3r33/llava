{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 Training: Instruction Fine-tuning\n",
    "\n",
    "> Sets up and runs the second stage of LLaVA training: fine-tuning the LLM (using LoRA) and the projector on instruction-following data.\n",
    "> Also includes setup for training the Adaptive model variant and handling ablation configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc # For memory cleanup\n",
    "import argparse # For command-line execution\n",
    "import time # For timing\n",
    "import traceback # For detailed error printing\n",
    "import copy # For deep copying config\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "# If running as a script, the path might need adjustment relative to the script location\n",
    "# This assumes the script is run from the project root or `scripts/` directory\n",
    "if __name__ == \"__main__\" and \"get_ipython\" not in locals():\n",
    "     # If script is in 'scripts/', go up one level to project root\n",
    "     if project_root.name == 'scripts':\n",
    "         project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    # print(f\"Project root already in sys.path: {project_root_str}\") # Less verbose for script\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: peft library not found. LoRA functionality will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n",
      "CLIP Mean: [0.48145466, 0.4578275, 0.40821073]\n",
      "CLIP Std: [0.26862954, 0.26130258, 0.27577711]\n",
      "Fastai Normalize Transform: Normalize -- {'mean': tensor([[[[0.4815]],\n",
      "\n",
      "         [[0.4578]],\n",
      "\n",
      "         [[0.4082]]]]), 'std': tensor([[[[0.2686]],\n",
      "\n",
      "         [[0.2613]],\n",
      "\n",
      "         [[0.2758]]]]), 'axes': (0, 2, 3)}\n",
      "(enc:1,dec:1)\n",
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n",
      "V1 template assistant role tokens: [1792, 29889]\n",
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0, Template: v1\n",
      "LLaVADataBlockStage2 defined.\n",
      "LLaVALoss initialized, ignoring index: -100\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastai.learner import Learner\n",
    "from fastai.optimizer import AdamW\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.schedule import fit_one_cycle\n",
    "from fastai.callback.save import SaveModelCallback\n",
    "from fastai.callback.training import GradientAccumulation\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.vision.all import params # For splitter\n",
    "from fastai.text.all import Perplexity # Import Perplexity metric\n",
    "from fastai.data.core import DataLoaders\n",
    "from functools import partial\n",
    "import wandb # Import wandb directly for cleanup\n",
    "import json # For dummy data creation\n",
    "import PIL.Image # For dummy data creation\n",
    "from typing import List, Optional, Type # Added Type\n",
    "\n",
    "# Attempt to import peft, set flag\n",
    "try:\n",
    "    from peft import PeftModel, save_adapter # Removed save_adapter (not used here)\n",
    "    _peft_available = True\n",
    "except ImportError:\n",
    "    print(\"Warning: peft library not found. LoRA functionality will be disabled.\", file=sys.stderr)\n",
    "    PeftModel = None # Define as None if not available\n",
    "    _peft_available = False\n",
    "\n",
    "try:\n",
    "    from llava.utils import load_config, init_wandb\n",
    "    from llava.data.loading import get_stage2_dataloaders\n",
    "    from llava.model.baseline import BaselineLLaVAModel\n",
    "    from llava.model.adaptive import AdaptiveLLaVAModel # Added Adaptive model\n",
    "    from llava.training.core import LLaVALoss\n",
    "except ImportError as e:\n",
    "     print(f\"Error importing llava modules: {e}\")\n",
    "     print(\"Ensure that nbdev_export has been run and the llava library is installed/accessible.\")\n",
    "     # In a script context, it's better to exit if core modules are missing\n",
    "     if __name__ == \"__main__\" and \"get_ipython\" not in locals():\n",
    "          sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 Splitter Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stage 2, we fine-tune the `projector` and the `language_model`. If LoRA is enabled (`use_lora: true` in config), only the LoRA adapter parameters within the language model are trained. If LoRA is disabled, the entire language model is trained. The `vision_tower` remains frozen by default.\n",
    "\n",
    "For the adaptive model, we also include parameters from the `patcher` module if it is trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llava_stage2_splitter(model: nn.Module):\n",
    "    \"\"\"Splits the `BaselineLLaVAModel` parameters for Stage 2 training.\n",
    "\n",
    "    Trains the `projector` and LoRA adapters (if enabled) or the full `language_model`.\n",
    "    Keeps the `vision_tower` frozen by default.\n",
    "\n",
    "    Args:\n",
    "        model: An instance of `BaselineLLaVAModel`.\n",
    "\n",
    "    Returns:\n",
    "        A list containing parameter groups for trainable components.\n",
    "    \"\"\"\n",
    "    projector_params = []\n",
    "    llm_params = []\n",
    "    frozen_params = []\n",
    "\n",
    "    print(\"Applying Stage 2 splitter (Baseline)...\" + (\" (PEFT Available)\" if _peft_available else \"\"))\n",
    "\n",
    "    # Projector parameters are always trained in Stage 2\n",
    "    if hasattr(model, 'projector') and model.projector is not None:\n",
    "        print(\"  - Collecting projector parameters (trainable).\")\n",
    "        projector_params.extend(list(model.projector.parameters()))\n",
    "        for p in model.projector.parameters():\n",
    "             p.requires_grad = True\n",
    "    else:\n",
    "        print(\"Warning: Model has no projector attribute.\")\n",
    "\n",
    "    # Handle Language Model parameters based on LoRA configuration\n",
    "    if hasattr(model, 'language_model') and model.language_model is not None:\n",
    "        # Access config stored within the model instance\n",
    "        # Handle potential nested structure in config access more safely\n",
    "        use_lora = model.config.get('model', {}).get('peft', {}).get('use_lora', False)\n",
    "\n",
    "        if _peft_available and use_lora and isinstance(model.language_model, PeftModel):\n",
    "            print(\"  - LoRA enabled: Collecting LLM adapter parameters (trainable).\")\n",
    "            # PEFT model automatically handles requires_grad for adapters\n",
    "            llm_params.extend([p for p in model.language_model.parameters() if p.requires_grad])\n",
    "            # Base model parameters should already be frozen by PEFT\n",
    "            # We can double-check frozen status for verification\n",
    "            # frozen_params.extend([p for p in model.language_model.parameters() if not p.requires_grad])\n",
    "        elif use_lora and not _peft_available:\n",
    "             print(\"Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.\")\n",
    "             for p in model.language_model.parameters():\n",
    "                 p.requires_grad = False\n",
    "             # frozen_params.extend(list(model.language_model.parameters()))\n",
    "        else:\n",
    "            print(\"  - LoRA disabled: Collecting all LLM parameters (trainable).\")\n",
    "            llm_params.extend(list(model.language_model.parameters()))\n",
    "            for p in model.language_model.parameters():\n",
    "                 p.requires_grad = True # Ensure all LLM params are trainable if not using LoRA\n",
    "    else:\n",
    "        print(\"Warning: Model has no language_model attribute.\")\n",
    "\n",
    "    # Vision Tower parameters are frozen by default\n",
    "    if hasattr(model, 'vision_tower') and model.vision_tower is not None:\n",
    "        print(\"  - Collecting vision tower parameters (frozen).\")\n",
    "        # frozen_params.extend(list(model.vision_tower.parameters())) # Collect if needed for verification\n",
    "        for p in model.vision_tower.parameters():\n",
    "            p.requires_grad = False\n",
    "    else:\n",
    "        print(\"Warning: Model has no vision_tower attribute.\")\n",
    "\n",
    "    # Combine trainable parameters into one group for the optimizer\n",
    "    trainable_groups = projector_params + llm_params\n",
    "    # Count frozen parameters for verification\n",
    "    frozen_count = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    trainable_count = sum(p.numel() for p in trainable_groups)\n",
    "    print(f\"Splitter created groups: Trainable ({trainable_count} params), Frozen ({frozen_count} params)\")\n",
    "    \n",
    "    if not trainable_groups:\n",
    "         raise ValueError(\"Splitter function resulted in no trainable parameters. Check model structure and config.\")\n",
    "         \n",
    "    return [trainable_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def adaptive_llava_stage2_splitter(model: nn.Module):\n",
    "    \"\"\"Splits the `AdaptiveLLaVAModel` parameters for Stage 2 training.\n",
    "\n",
    "    Trains the `projector`, LoRA adapters (if enabled) or the full `language_model`,\n",
    "    and the `patcher` module (if it has trainable parameters).\n",
    "    Keeps the `vision_tower` frozen by default.\n",
    "\n",
    "    Args:\n",
    "        model: An instance of `AdaptiveLLaVAModel`.\n",
    "\n",
    "    Returns:\n",
    "        A list containing parameter groups for trainable components.\n",
    "    \"\"\"\n",
    "    projector_params = []\n",
    "    llm_params = []\n",
    "    patcher_params = []\n",
    "    frozen_params = []\n",
    "\n",
    "    print(\"Applying Stage 2 splitter (Adaptive)...\" + (\" (PEFT Available)\" if _peft_available else \"\"))\n",
    "\n",
    "    # Projector parameters\n",
    "    if hasattr(model, 'projector') and model.projector is not None:\n",
    "        print(\"  - Collecting projector parameters (trainable).\")\n",
    "        projector_params.extend(list(model.projector.parameters()))\n",
    "        for p in model.projector.parameters(): p.requires_grad = True\n",
    "    else: print(\"Warning: Model has no projector attribute.\")\n",
    "\n",
    "    # LLM parameters (LoRA or full)\n",
    "    if hasattr(model, 'language_model') and model.language_model is not None:\n",
    "        use_lora = model.config.get('model', {}).get('peft', {}).get('use_lora', False)\n",
    "        if _peft_available and use_lora and isinstance(model.language_model, PeftModel):\n",
    "            print(\"  - LoRA enabled: Collecting LLM adapter parameters (trainable).\")\n",
    "            llm_params.extend([p for p in model.language_model.parameters() if p.requires_grad])\n",
    "        elif use_lora and not _peft_available:\n",
    "            print(\"Warning: LoRA configured but PEFT library not found. Freezing LLM.\")\n",
    "            for p in model.language_model.parameters(): p.requires_grad = False\n",
    "        else:\n",
    "            print(\"  - LoRA disabled: Collecting all LLM parameters (trainable).\")\n",
    "            llm_params.extend(list(model.language_model.parameters()))\n",
    "            for p in model.language_model.parameters(): p.requires_grad = True\n",
    "    else: print(\"Warning: Model has no language_model attribute.\")\n",
    "\n",
    "    # Patcher parameters (if exists and has parameters)\n",
    "    if hasattr(model, 'patcher') and model.patcher is not None:\n",
    "        patcher_trainable_params = [p for p in model.patcher.parameters() if p.requires_grad]\n",
    "        if patcher_trainable_params:\n",
    "            print(\"  - Collecting patcher parameters (trainable).\")\n",
    "            patcher_params.extend(patcher_trainable_params)\n",
    "            # Ensure they are set to trainable (might be redundant but safe)\n",
    "            for p in patcher_params: p.requires_grad = True\n",
    "        else:\n",
    "            print(\"  - Patcher found, but has no trainable parameters.\")\n",
    "    else: print(\"  - No adaptive patcher found in model.\")\n",
    "\n",
    "    # Vision Tower (frozen)\n",
    "    if hasattr(model, 'vision_tower') and model.vision_tower is not None:\n",
    "        print(\"  - Collecting vision tower parameters (frozen).\")\n",
    "        for p in model.vision_tower.parameters(): p.requires_grad = False\n",
    "    else: print(\"Warning: Model has no vision_tower attribute.\")\n",
    "\n",
    "    # Combine trainable groups\n",
    "    trainable_groups = projector_params + llm_params + patcher_params\n",
    "    frozen_count = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    trainable_count = sum(p.numel() for p in trainable_groups)\n",
    "    print(f\"Splitter created groups: Trainable ({trainable_count} params), Frozen ({frozen_count} params)\")\n",
    "\n",
    "    if not trainable_groups:\n",
    "        raise ValueError(\"Splitter function resulted in no trainable parameters. Check model structure and config.\")\n",
    "\n",
    "    return [trainable_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(llava_stage2_splitter) # Omitted for script execution\n",
    "# show_doc(adaptive_llava_stage2_splitter) # Omitted for script execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3 & 7.1: Setup Learner Configuration (Stage 2 - Baseline & Adaptive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions set up the `Learner` object for Stage 2 instruction tuning, handling both the baseline and adaptive model variants.\n",
    "\n",
    "**Update for Step 8.1:** Modified `_get_stage2_learner_internal` to include ablation name in the W&B run name if an ablation is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stage2_learner(config: dict) -> Learner:\n",
    "    \"\"\"Configures and returns a fastai Learner for Stage 2 Instruction Fine-tuning (Baseline Model).\n",
    "\n",
    "    Loads Stage 1 projector weights, sets up the BaselineLLaVAModel (potentially with LoRA),\n",
    "    uses the llava_stage2_splitter, and includes relevant callbacks and metrics.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A configured fastai Learner instance for Stage 2 Baseline Training.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If DataLoaders or Model instantiation fails.\n",
    "        FileNotFoundError: If Stage 1 projector weights or data paths are invalid.\n",
    "        AttributeError: If the model is missing expected components.\n",
    "    \"\"\"\n",
    "    return _get_stage2_learner_internal(config, model_class=BaselineLLaVAModel, splitter=llava_stage2_splitter)\n",
    "\n",
    "#| export\n",
    "def get_adaptive_stage2_learner(config: dict) -> Learner:\n",
    "    \"\"\"Configures and returns a fastai Learner for Stage 2 Instruction Fine-tuning (Adaptive Model).\n",
    "\n",
    "    Loads Stage 1 projector weights, sets up the AdaptiveLLaVAModel (potentially with LoRA),\n",
    "    uses the adaptive_llava_stage2_splitter, and includes relevant callbacks and metrics.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A configured fastai Learner instance for Stage 2 Adaptive Training.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If DataLoaders or Model instantiation fails.\n",
    "        FileNotFoundError: If Stage 1 projector weights or data paths are invalid.\n",
    "        AttributeError: If the model is missing expected components.\n",
    "    \"\"\"\n",
    "    return _get_stage2_learner_internal(config, model_class=AdaptiveLLaVAModel, splitter=adaptive_llava_stage2_splitter)\n",
    "\n",
    "#| export\n",
    "# Internal function to handle common learner setup logic\n",
    "def _get_stage2_learner_internal(config: dict, model_class: Type[nn.Module], splitter: callable) -> Learner:\n",
    "    \"\"\"Internal function to set up Stage 2 Learner for baseline or adaptive models.\"\"\"\n",
    "    model_type_name = model_class.__name__\n",
    "    print(f\"--- Setting up Stage 2 Learner ({model_type_name}) ---\")\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ablation_config = config.get('ablation', {})\n",
    "    ablation_name = ablation_config.get('force_patcher_strategy') # e.g., 'baseline' or None\n",
    "\n",
    "    # 1. Load Stage 2 DataLoaders\n",
    "    print(\"Loading Stage 2 DataLoaders...\")\n",
    "    try:\n",
    "        dls = get_stage2_dataloaders(config)\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        print(f\"Error loading Stage 2 DataLoaders: {e}\")\n",
    "        raise RuntimeError(\"Failed to create Stage 2 DataLoaders.\") from e\n",
    "    if not dls:\n",
    "        raise RuntimeError(\"Stage 2 DataLoaders object is None.\")\n",
    "    print(f\"DataLoaders loaded. Train samples: {len(dls.train_ds)}, Valid samples: {len(dls.valid_ds)}\")\n",
    "\n",
    "    # 2. Instantiate Model\n",
    "    print(f\"Instantiating {model_type_name} for Stage 2...\" + (f\" (Ablation: {ablation_name})\" if ablation_name else \"\"))\n",
    "    try:\n",
    "        # Pass the potentially modified config (e.g., from command line) to the model\n",
    "        model = model_class(config)\n",
    "        if model.vision_tower is None or model.language_model is None or model.projector is None:\n",
    "            raise RuntimeError(f\"{model_type_name} initialization incomplete.\")\n",
    "        # For adaptive model, check if patcher initialized if expected\n",
    "        if model_type_name == 'AdaptiveLLaVAModel' and config.get('model', {}).get('adaptive_patcher', {}).get('enabled', False):\n",
    "            if not hasattr(model, 'patcher') or model.patcher is None:\n",
    "                print(\"Warning: Adaptive patcher was enabled in config but failed to initialize in the model.\")\n",
    "        print(\"Model instantiated successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error instantiating {model_type_name}: {e}\")\n",
    "        raise RuntimeError(f\"Failed to instantiate {model_type_name} for Stage 2.\") from e\n",
    "\n",
    "    # 3. Load Stage 1 Projector Weights\n",
    "    stage1_weights_fname = config['paths'].get('stage1_projector_weights', 'stage1_projector.pth')\n",
    "    stage1_weights_path = output_dir / 'models' / stage1_weights_fname\n",
    "    print(f\"Attempting to load Stage 1 projector weights from: {stage1_weights_path}\")\n",
    "    if stage1_weights_path.is_file():\n",
    "        try:\n",
    "            projector_state_dict = torch.load(stage1_weights_path, map_location='cpu')\n",
    "            model.projector.load_state_dict(projector_state_dict)\n",
    "            print(f\"Successfully loaded Stage 1 projector weights from {stage1_weights_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Stage 1 projector weights: {e}\")\n",
    "            raise RuntimeError(f\"Failed to load expected Stage 1 projector weights from {stage1_weights_path}\") from e\n",
    "    else:\n",
    "        print(f\"Warning: Stage 1 projector weights not found at {stage1_weights_path}. Projector will use initial weights.\")\n",
    "\n",
    "    # 4. Define Loss Function\n",
    "    loss_func = LLaVALoss()\n",
    "    print(f\"Loss function: {type(loss_func).__name__}\")\n",
    "\n",
    "    # 5. Define Optimizer\n",
    "    lr = config.get('training', {}).get('learning_rate_stage2', 2e-5)\n",
    "    wd = config.get('training', {}).get('weight_decay', 0.0)\n",
    "    opt_func = partial(AdamW, lr=lr, wd=wd, eps=1e-8)\n",
    "    print(f\"Optimizer: AdamW (lr={lr}, wd={wd})\")\n",
    "\n",
    "    # 6. Define Splitter (Passed as argument)\n",
    "    print(f\"Parameter splitter: {splitter.__name__}\")\n",
    "\n",
    "    # --- Add Metrics (Step 5.1) --- #\n",
    "    metrics = [Perplexity()]\n",
    "    print(f\"Metrics: {[m.name for m in metrics]}\")\n",
    "\n",
    "    # 7. Define Callbacks\n",
    "    cbs = []\n",
    "    if config.get('logging', {}).get('wandb', {}).get('enabled', False):\n",
    "        wandb_entity = config.get('logging', {}).get('wandb', {}).get('entity')\n",
    "        if wandb_entity and 'your_wandb_entity' not in str(wandb_entity):\n",
    "            project_name = config.get('logging', {}).get('wandb', {}).get('project', 'llava-adaptive-patching')\n",
    "            run_name_prefix = config.get('logging', {}).get('wandb', {}).get('run_name_prefix', 'stage2')\n",
    "            # Adapt run name based on model type and ablation status\n",
    "            model_tag = 'adaptive' if model_type_name == 'AdaptiveLLaVAModel' else 'baseline'\n",
    "            ablation_tag = f\"_abl-{ablation_name}\" if ablation_name else \"\"\n",
    "            stage2_model_name = Path(config['paths']['stage2_model_weights']).stem\n",
    "            run_name = f\"{run_name_prefix}_{model_tag}{ablation_tag}_{stage2_model_name}_{wandb.util.generate_id()}\"\n",
    "            init_wandb(config, job_type=\"stage2-training\", run_name=run_name)\n",
    "            cbs.append(WandbCallback(log_preds=False, log_model=False))\n",
    "            print(\"Added WandbCallback.\")\n",
    "        else:\n",
    "            print(\"W&B enabled in config, but entity not set or default. Skipping W&B init and callback.\")\n",
    "\n",
    "    # SaveModelCallback setup (remains commented out, manual save preferred)\n",
    "    stage2_model_fname = Path(config['paths']['stage2_model_weights']).stem\n",
    "    ablation_fname_tag = f\"_abl-{ablation_name}\" if ablation_name else \"\" # Add ablation tag to saved model name\n",
    "    save_cb = SaveModelCallback(\n",
    "        monitor='valid_loss',\n",
    "        min_delta=0.001,\n",
    "        fname=f\"{stage2_model_fname}{ablation_fname_tag}\", # Include ablation tag in filename\n",
    "        every_epoch=False,\n",
    "        with_opt=True,\n",
    "        reset_on_fit=True\n",
    "    )\n",
    "    print(f\"SaveModelCallback is configured but commented out. Manual saving of adapters/projector is preferred.\")\n",
    "\n",
    "    # Optimization Callbacks\n",
    "    grad_accum_steps = config.get('training', {}).get('gradient_accumulation_steps', 1)\n",
    "    if grad_accum_steps > 1:\n",
    "        cbs.append(GradientAccumulation(grad_accum_steps))\n",
    "        print(f\"Added GradientAccumulation callback with {grad_accum_steps} steps.\")\n",
    "    \n",
    "    use_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)\n",
    "    if use_mixed_precision:\n",
    "        cbs.append(MixedPrecision())\n",
    "        print(\"Added MixedPrecision callback.\")\n",
    "\n",
    "    # 8. Create Learner\n",
    "    try:\n",
    "        learner = Learner(\n",
    "            dls=dls,\n",
    "            model=model,\n",
    "            loss_func=loss_func,\n",
    "            opt_func=opt_func,\n",
    "            splitter=splitter,\n",
    "            cbs=cbs,\n",
    "            metrics=metrics,\n",
    "            path=output_dir,\n",
    "            train_bn=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Stage 2 Learner ({model_type_name}): {e}\")\n",
    "        if wandb.run is not None: wandb.finish(exit_code=1)\n",
    "        raise RuntimeError(f\"Failed to create the Stage 2 Learner object ({model_type_name}).\") from e\n",
    "\n",
    "    print(f\"--- Stage 2 Learner Setup Complete ({model_type_name}) ---\")\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(get_stage2_learner) # Omitted for script execution\n",
    "# show_doc(get_adaptive_stage2_learner) # Omitted for script execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Learner Configuration - Stage 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Creating dummy Stage 1 projector weights: /workspace/llava/output/models/stage1_projector.pth\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Creating dummy Stage 2 JSONL: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "--- Setting up Stage 2 Learner (BaselineLLaVAModel) ---\n",
      "Loading Stage 2 DataLoaders...\n",
      "Creating Stage 2 DataLoaders with batch size: 4, num_workers: 4\n",
      "Loading Stage 2 items from: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "Assuming images relative to: /workspace/llava/data\n",
      "Found 2 samples for Stage 2.\n",
      "Stage 2 DataLoaders created successfully.\n",
      "DataLoaders loaded. Train samples: 1, Valid samples: 1\n",
      "Instantiating BaselineLLaVAModel for Stage 2...\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af1425a4683437f866694368226fa1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM embedding size already matches tokenizer size. No resizing needed.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Model instantiated successfully.\n",
      "Attempting to load Stage 1 projector weights from: /workspace/llava/output/models/stage1_projector.pth\n",
      "Successfully loaded Stage 1 projector weights from /workspace/llava/output/models/stage1_projector.pth\n",
      "Loss function: LLaVALoss\n",
      "Optimizer: AdamW (lr=2e-05, wd=0.0)\n",
      "Parameter splitter: llava_stage2_splitter\n",
      "Metrics: ['perplexity']\n",
      "W&B enabled in config, but entity not set or default. Skipping W&B init and callback.\n",
      "SaveModelCallback is configured but commented out. Manual saving of adapters/projector is preferred.\n",
      "Added GradientAccumulation callback with 4 steps.\n",
      "Added MixedPrecision callback.\n",
      "Applying Stage 2 splitter (Baseline)...\n",
      "  - Collecting projector parameters (trainable).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Collecting vision tower parameters (frozen).\n",
      "Splitter created groups: Trainable (33554432 params), Frozen (6891018240 params)\n",
      "--- Stage 2 Learner Setup Complete (BaselineLLaVAModel) ---\n",
      "Baseline Stage 2 Learner created successfully.\n",
      "\n",
      "--- Setting up Stage 2 Learner (AdaptiveLLaVAModel) ---\n",
      "Loading Stage 2 DataLoaders...\n",
      "Creating Stage 2 DataLoaders with batch size: 4, num_workers: 4\n",
      "Loading Stage 2 items from: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "Assuming images relative to: /workspace/llava/data\n",
      "Found 2 samples for Stage 2.\n",
      "Stage 2 DataLoaders created successfully.\n",
      "DataLoaders loaded. Train samples: 1, Valid samples: 1\n",
      "Instantiating AdaptiveLLaVAModel for Stage 2...\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8c728c5b38417bafb3335d4b70f9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM embedding size already matches tokenizer size. No resizing needed.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Adaptive Patcher enabled with strategy: 'variable_resolution' (VariableResolutionPatcher)\n",
      "Initialized VariableResolutionPatcher with grid options: [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008], [336, 336]]\n",
      "Model instantiated successfully.\n",
      "Attempting to load Stage 1 projector weights from: /workspace/llava/output/models/stage1_projector.pth\n",
      "Successfully loaded Stage 1 projector weights from /workspace/llava/output/models/stage1_projector.pth\n",
      "Loss function: LLaVALoss\n",
      "Optimizer: AdamW (lr=2e-05, wd=0.0)\n",
      "Parameter splitter: adaptive_llava_stage2_splitter\n",
      "Metrics: ['perplexity']\n",
      "W&B enabled in config, but entity not set or default. Skipping W&B init and callback.\n",
      "SaveModelCallback is configured but commented out. Manual saving of adapters/projector is preferred.\n",
      "Added GradientAccumulation callback with 4 steps.\n",
      "Added MixedPrecision callback.\n",
      "Applying Stage 2 splitter (Adaptive)...\n",
      "  - Collecting projector parameters (trainable).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: LoRA configured but PEFT library not found. Freezing LLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Patcher found, but has no trainable parameters.\n",
      "  - Collecting vision tower parameters (frozen).\n",
      "Splitter created groups: Trainable (33554432 params), Frozen (6891018240 params)\n",
      "--- Stage 2 Learner Setup Complete (AdaptiveLLaVAModel) ---\n",
      "Adaptive Stage 2 Learner created successfully.\n",
      "\n",
      "Stage 2 Learner setup test passed.\n",
      "Cleaned up baseline_learner learner and model memory.\n",
      "Cleaned up adaptive_learner learner and model memory.\n"
     ]
    }
   ],
   "source": [
    "#| test \n",
    "import gc\n",
    "from fastai.callback.save import SaveModelCallback\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.training import GradientAccumulation\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.text.all import Perplexity # Import Perplexity for testing\n",
    "from llava.model.baseline import LLaVAProjector, BaselineLLaVAModel # Import required classes\n",
    "from llava.model.adaptive import AdaptiveLLaVAModel # Import adaptive model\n",
    "\n",
    "baseline_learner = None\n",
    "adaptive_learner = None\n",
    "\n",
    "try:\n",
    "    # Load config\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # --- Test Setup ---\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / 'models').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create dummy Stage 1 weights if they don't exist\n",
    "    stage1_weights_fname = config['paths'].get('stage1_projector_weights', 'stage1_projector.pth')\n",
    "    stage1_weights_path = output_dir / 'models' / stage1_weights_fname\n",
    "    if not stage1_weights_path.is_file():\n",
    "        print(f\"Creating dummy Stage 1 projector weights: {stage1_weights_path}\")\n",
    "        proj_input_dim = config['model']['projector']['input_dim']\n",
    "        proj_output_dim = config['model']['projector']['output_dim']\n",
    "        dummy_projector = LLaVAProjector(proj_input_dim, proj_output_dim)\n",
    "        torch.save(dummy_projector.state_dict(), stage1_weights_path)\n",
    "        del dummy_projector # Clean up\n",
    "        \n",
    "    # Create dummy Stage 2 data if needed\n",
    "    data_base = Path(config['paths']['data_base'])\n",
    "    stage2_json_rel = Path(config['paths']['stage2_data'])\n",
    "    stage1_img_rel = Path(config['paths']['stage1_images']) # Needed for dummy image paths\n",
    "    stage1_img_path = data_base / stage1_img_rel\n",
    "    stage2_json_path = data_base / stage2_json_rel\n",
    "    stage2_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    stage1_img_path.mkdir(parents=True, exist_ok=True) # Ensure image dir exists\n",
    "\n",
    "    img1_rel_path_str = str(stage1_img_rel.name + '/dummy_img1.jpg')\n",
    "    img2_rel_path_str = str(stage1_img_rel.name + '/dummy_img2.png')\n",
    "    if not (stage1_img_path / 'dummy_img1.jpg').exists():\n",
    "        PIL.Image.new('RGB', (60, 30), color = 'red').save(stage1_img_path / 'dummy_img1.jpg')\n",
    "    if not (stage1_img_path / 'dummy_img2.png').exists():\n",
    "        PIL.Image.new('RGB', (60, 30), color = 'green').save(stage1_img_path / 'dummy_img2.png')\n",
    "\n",
    "    if not stage2_json_path.exists() or stage2_json_path.stat().st_size < 10:\n",
    "        print(f\"Creating dummy Stage 2 JSONL: {stage2_json_path}\")\n",
    "        dummy_jsonl_content = [\n",
    "            {\"id\": \"s2_001\", \"image\": img1_rel_path_str, \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nDescribe image.\"}, {\"from\": \"gpt\", \"value\": \"It is a red object.\"}]}, \n",
    "            {\"id\": \"s2_002\", \"image\": img2_rel_path_str, \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nIs it green?\"}, {\"from\": \"gpt\", \"value\": \"Yes, it appears green.\"}]},\n",
    "        ]\n",
    "        with open(stage2_json_path, 'w') as f:\n",
    "            for item in dummy_jsonl_content:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "    # -----------------\n",
    "    \n",
    "    # Modify config for test\n",
    "    if 'model' not in config: config['model'] = {}\n",
    "    if 'peft' not in config['model']: config['model']['peft'] = {}\n",
    "    config['model']['peft']['use_lora'] = True # Enable LoRA for splitter test\n",
    "    config['model']['use_activation_checkpointing'] = False # Keep disabled for test\n",
    "    if 'adaptive_patcher' not in config['model']: config['model']['adaptive_patcher'] = {}\n",
    "    config['model']['adaptive_patcher']['enabled'] = True # Enable adaptive for test\n",
    "    config['model']['adaptive_patcher']['strategy'] = 'variable_resolution'\n",
    "    # Ensure ablation is not forced for this test\n",
    "    if 'ablation' not in config: config['ablation'] = {}\n",
    "    config['ablation']['force_patcher_strategy'] = None \n",
    "\n",
    "    # Disable W&B for testing unless entity is properly set\n",
    "    wandb_enabled = config.get('logging', {}).get('wandb', {}).get('enabled', False)\n",
    "    wandb_entity = config.get('logging', {}).get('wandb', {}).get('entity')\n",
    "    if wandb_enabled and (wandb_entity is None or 'your_wandb_entity' in str(wandb_entity)):\n",
    "        print(\"Warning: W&B is enabled but entity is not set or default. Disabling W&B for this test.\")\n",
    "        if 'logging' not in config: config['logging'] = {}\n",
    "        if 'wandb' not in config['logging']: config['logging']['wandb'] = {}\n",
    "        config['logging']['wandb']['enabled'] = False\n",
    "\n",
    "    # --- Test Baseline Learner --- \n",
    "    baseline_learner = get_stage2_learner(config)\n",
    "    print(\"Baseline Stage 2 Learner created successfully.\")\n",
    "    assert isinstance(baseline_learner, Learner)\n",
    "\n",
    "    # --- Test Adaptive Learner --- \n",
    "    adaptive_learner = get_adaptive_stage2_learner(config)\n",
    "    print(\"Adaptive Stage 2 Learner created successfully.\")\n",
    "    assert isinstance(adaptive_learner, Learner)\n",
    "    assert isinstance(adaptive_learner.model, AdaptiveLLaVAModel)\n",
    "    assert adaptive_learner.model.patcher is not None # Check patcher exists\n",
    "\n",
    "    print(\"\\nStage 2 Learner setup test passed.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Skipping test: FileNotFoundError - {e}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Skipping test: ImportError - {e}. (Likely `peft` is missing)\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Skipping test: RuntimeError - {e}. (Likely CUDA OOM or setup issue)\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred during Stage 2 learner setup test: {e}\")\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Clean up memory\n",
    "    def cleanup_learner(learner: Optional[Learner], name: str):\n",
    "        if learner is None: return\n",
    "        try:\n",
    "            if hasattr(learner, 'model') and learner.model is not None:\n",
    "                if hasattr(learner.model, 'vision_tower') and learner.model.vision_tower is not None: learner.model.vision_tower.to('cpu')\n",
    "                if hasattr(learner.model, 'language_model') and learner.model.language_model is not None: learner.model.language_model.to('cpu')\n",
    "                if hasattr(learner.model, 'projector') and learner.model.projector is not None: learner.model.projector.to('cpu')\n",
    "                if hasattr(learner.model, 'patcher') and learner.model.patcher is not None: learner.model.patcher.to('cpu')\n",
    "                del learner.model\n",
    "            learner.destroy()\n",
    "            print(f\"Cleaned up {name} learner and model memory.\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error during {name} learner cleanup: {e}\")\n",
    "        finally:\n",
    "            del learner\n",
    "        \n",
    "    cleanup_learner(baseline_learner, \"baseline_learner\")\n",
    "    cleanup_learner(adaptive_learner, \"adaptive_learner\")\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    \n",
    "    if wandb.run is not None:\n",
    "        try:\n",
    "            if wandb.run.id: wandb.finish()\n",
    "        except Exception as e: print(f\"Error finishing W&B run: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.5 & 7.2: Implement Stage 2 Training Script (Baseline & Adaptive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function orchestrates the Stage 2 training loop, including saving the final weights (projector + LoRA adapters). It can now handle both baseline and adaptive model training.\n",
    "\n",
    "**Update for Step 8.1:** Modified `_train_stage2_internal` to include ablation name in the saved filenames if an ablation is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_stage2(config_path: str | Path, ablation_mode: Optional[str] = None):\n",
    "    \"\"\"Loads config, sets up Stage 2 baseline learner, runs training, and saves weights.\n",
    "    Includes logging of efficiency metrics (Peak VRAM, Training Time).\n",
    "    Saves the projector weights and LoRA adapter weights (if used) separately.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the YAML configuration file.\n",
    "        ablation_mode: Optional string specifying the ablation mode (e.g., 'baseline'). \n",
    "                      Overrides config if provided.\n",
    "    \"\"\"\n",
    "    _train_stage2_internal(config_path, get_learner_func=get_stage2_learner, ablation_mode=ablation_mode)\n",
    "\n",
    "#| export\n",
    "def train_adaptive_stage2(config_path: str | Path, ablation_mode: Optional[str] = None):\n",
    "    \"\"\"Loads config, sets up Stage 2 adaptive learner, runs training, and saves weights.\n",
    "    Includes logging of efficiency metrics (Peak VRAM, Training Time).\n",
    "    Saves the projector weights and LoRA adapter weights (if used) separately.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the YAML configuration file.\n",
    "        ablation_mode: Optional string specifying the ablation mode (e.g., 'baseline'). \n",
    "                       Overrides config if provided.\n",
    "    \"\"\"\n",
    "    _train_stage2_internal(config_path, get_learner_func=get_adaptive_stage2_learner, ablation_mode=ablation_mode)\n",
    "\n",
    "\n",
    "#| export\n",
    "# Internal training function\n",
    "def _train_stage2_internal(config_path: str | Path, get_learner_func: callable, ablation_mode: Optional[str] = None):\n",
    "    \"\"\"Internal function to handle the Stage 2 training loop.\"\"\"\n",
    "    model_type_name = 'Adaptive' if get_learner_func == get_adaptive_stage2_learner else 'Baseline'\n",
    "    print(f\"--- Starting Stage 2 Training ({model_type_name} Model) --- \")\n",
    "    start_run_time = time.time()\n",
    "    print(f\"Loading configuration from: {config_path}\")\n",
    "    config = load_config(config_path)\n",
    "    \n",
    "    # --- Handle Ablation Override --- \n",
    "    ablation_name = ablation_mode # Use CLI override if provided\n",
    "    if ablation_name is None: # Otherwise use config setting\n",
    "         ablation_name = config.get('ablation', {}).get('force_patcher_strategy')\n",
    "    # Update config dictionary if CLI override was used or config has it\n",
    "    if ablation_name:\n",
    "         if 'ablation' not in config: config['ablation'] = {}\n",
    "         config['ablation']['force_patcher_strategy'] = ablation_name\n",
    "         if ablation_mode:\n",
    "              print(f\"Overriding ablation mode from command line: {ablation_mode}\")\n",
    "         else:\n",
    "              print(f\"Using ablation mode from config: {ablation_name}\")\n",
    "    # ----------------------------- \n",
    "\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    models_dir = output_dir / 'models'\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    learner = None # Initialize learner to None for finally block\n",
    "    run = None # Initialize wandb run object\n",
    "    try:\n",
    "        # --- Get Learner (using the passed function) --- \n",
    "        # Pass the potentially modified config\n",
    "        learner = get_learner_func(config)\n",
    "        run = wandb.run # Get the active run object if W&B was initialized\n",
    "\n",
    "        # --- Reset CUDA memory stats before training ---\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats() \n",
    "            print(\"Reset CUDA peak memory stats before training.\")\n",
    "            \n",
    "        # --- Start Training --- \n",
    "        epochs = config.get('training', {}).get('num_epochs_stage2', 3)\n",
    "        lr = config.get('training', {}).get('learning_rate_stage2', 2e-5)\n",
    "        print(f\"Starting training for {epochs} epochs with max_lr={lr}...\")\n",
    "        start_train_time = time.time()\n",
    "\n",
    "        learner.fit_one_cycle(epochs, lr_max=lr)\n",
    "        \n",
    "        end_train_time = time.time()\n",
    "        total_train_time_sec = end_train_time - start_train_time\n",
    "        print(f\"Training finished in {total_train_time_sec:.2f} seconds.\")\n",
    "        \n",
    "        # --- Log Efficiency Metrics (Step 5.4) --- \n",
    "        if run:\n",
    "            wandb.log({\"train/stage2_total_training_time_sec\": total_train_time_sec})\n",
    "        if torch.cuda.is_available():\n",
    "            peak_vram_gb = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "            print(f\"Peak Training VRAM used (Stage 2): {peak_vram_gb:.2f} GB\")\n",
    "            if run: wandb.log({\"train/stage2_peak_vram_gb\": peak_vram_gb})\n",
    "        \n",
    "        # --- Save final trained weights --- \n",
    "        # Define specific save names based on model type and ablation status\n",
    "        model_base_name = Path(config['paths']['stage2_model_weights']).stem\n",
    "        ablation_tag = f\"_abl-{ablation_name}\" if ablation_name else \"\" # Use determined ablation name\n",
    "        save_prefix = f\"{model_base_name}_{model_type_name.lower()}{ablation_tag}\"\n",
    "        \n",
    "        # 1. Save Projector Weights\n",
    "        projector_save_path = models_dir / f\"{save_prefix}_projector_final.pth\"\n",
    "        print(f\"Saving final projector weights to: {projector_save_path}\")\n",
    "        if hasattr(learner.model, 'projector') and learner.model.projector is not None:\n",
    "             torch.save(learner.model.projector.state_dict(), projector_save_path)\n",
    "             print(\"Projector weights saved.\")\n",
    "        else: print(\"Warning: Cannot save projector weights, model has no projector.\")\n",
    "             \n",
    "        # 2. Save LoRA Adapters (if LoRA was used)\n",
    "        use_lora_config = config.get('model', {}).get('peft', {}).get('use_lora', False)\n",
    "        lora_applied = _peft_available and use_lora_config and hasattr(learner.model, 'language_model') and isinstance(learner.model.language_model, PeftModel)\n",
    "        if lora_applied:\n",
    "            lora_save_dir = models_dir / f\"{save_prefix}_lora_adapters\"\n",
    "            print(f\"Saving LoRA adapters to: {lora_save_dir}\")\n",
    "            try:\n",
    "                lora_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "                # Use save_pretrained method from PeftModel\n",
    "                learner.model.language_model.save_pretrained(str(lora_save_dir))\n",
    "                print(\"LoRA adapters saved successfully.\")\n",
    "            except Exception as e: print(f\"Error saving LoRA adapters: {e}\"); traceback.print_exc()\n",
    "        elif use_lora_config: print(\"LoRA configured but not applied. Cannot save adapters.\")\n",
    "        else: print(\"LoRA was not enabled in config.\")\n",
    "        \n",
    "        # 3. Save Patcher Weights (if adaptive and trainable)\n",
    "        if model_type_name == 'Adaptive' and hasattr(learner.model, 'patcher') and learner.model.patcher is not None:\n",
    "            patcher_params = list(learner.model.patcher.parameters())\n",
    "            if any(p.requires_grad for p in patcher_params):\n",
    "                patcher_save_path = models_dir / f\"{save_prefix}_patcher_final.pth\"\n",
    "                print(f\"Saving final patcher weights to: {patcher_save_path}\")\n",
    "                torch.save(learner.model.patcher.state_dict(), patcher_save_path)\n",
    "                print(\"Patcher weights saved.\")\n",
    "            else:\n",
    "                print(\"Adaptive patcher exists but has no trainable parameters to save.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Stage 2 training ({model_type_name}): {e}\")\n",
    "        traceback.print_exc()\n",
    "        if run: run.finish(exit_code=1)\n",
    "        raise e\n",
    "    finally:\n",
    "        if learner is not None:\n",
    "             cleanup_learner(learner, model_type_name.lower()) # Use helper cleanup\n",
    "        if run and wandb.run and wandb.run.id == run.id: # Ensure we finish the correct run\n",
    "            try: wandb.finish()\n",
    "            except Exception as e: print(f\"Error finishing W&B run: {e}\")\n",
    "            \n",
    "    end_run_time = time.time()\n",
    "    total_script_time = end_run_time - start_run_time\n",
    "    print(f\"Total Stage 2 script execution time ({model_type_name}): {total_script_time:.2f} seconds.\")\n",
    "    print(f\"--- Stage 2 Training Complete ({model_type_name} Model) --- \")\n",
    "\n",
    "# Helper cleanup function (to avoid repetition)\n",
    "def cleanup_learner(learner: Optional[Learner], name: str):\n",
    "    if learner is None: return\n",
    "    try:\n",
    "        if hasattr(learner, 'model') and learner.model is not None:\n",
    "            if hasattr(learner.model, 'vision_tower') and learner.model.vision_tower is not None: learner.model.vision_tower.to('cpu')\n",
    "            if hasattr(learner.model, 'language_model') and learner.model.language_model is not None: learner.model.language_model.to('cpu')\n",
    "            if hasattr(learner.model, 'projector') and learner.model.projector is not None: learner.model.projector.to('cpu')\n",
    "            if hasattr(learner.model, 'patcher') and learner.model.patcher is not None: learner.model.patcher.to('cpu')\n",
    "            del learner.model\n",
    "        learner.destroy()\n",
    "        print(f\"Cleaned up {name} learner and model memory.\")\n",
    "    except Exception as e:\n",
    "         print(f\"Error during {name} learner cleanup: {e}\")\n",
    "    finally:\n",
    "        del learner\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(train_stage2) # Omitted for script execution\n",
    "# show_doc(train_adaptive_stage2) # Omitted for script execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage (Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Baseline Stage 2 Training --- \n",
      "--- Starting Stage 2 Training (Baseline Model) --- \n",
      "Loading configuration from: ../configs/temp_test_config.yaml\n",
      "Using ablation mode from config: None\n",
      "--- Setting up Stage 2 Learner (BaselineLLaVAModel) ---\n",
      "Loading Stage 2 DataLoaders...\n",
      "Creating Stage 2 DataLoaders with batch size: 4, num_workers: 4\n",
      "Loading Stage 2 items from: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "Assuming images relative to: /workspace/llava/data\n",
      "Found 2 samples for Stage 2.\n",
      "Stage 2 DataLoaders created successfully.\n",
      "DataLoaders loaded. Train samples: 1, Valid samples: 1\n",
      "Instantiating BaselineLLaVAModel for Stage 2...\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e879a5999a4651a14332b9ac1f185b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM embedding size already matches tokenizer size. No resizing needed.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Model instantiated successfully.\n",
      "Attempting to load Stage 1 projector weights from: /workspace/llava/output/models/stage1_projector.pth\n",
      "Successfully loaded Stage 1 projector weights from /workspace/llava/output/models/stage1_projector.pth\n",
      "Loss function: LLaVALoss\n",
      "Optimizer: AdamW (lr=2e-05, wd=0.0)\n",
      "Parameter splitter: llava_stage2_splitter\n",
      "Metrics: ['perplexity']\n",
      "W&B enabled in config, but entity not set or default. Skipping W&B init and callback.\n",
      "SaveModelCallback is configured but commented out. Manual saving of adapters/projector is preferred.\n",
      "Added GradientAccumulation callback with 4 steps.\n",
      "Added MixedPrecision callback.\n",
      "Applying Stage 2 splitter (Baseline)...\n",
      "  - Collecting projector parameters (trainable).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Collecting vision tower parameters (frozen).\n",
      "Splitter created groups: Trainable (33554432 params), Frozen (6891018240 params)\n",
      "--- Stage 2 Learner Setup Complete (BaselineLLaVAModel) ---\n",
      "Reset CUDA peak memory stats before training.\n",
      "Starting training for 1 epochs with max_lr=2e-05...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/fastai/torch_core.py:263: UserWarning: TensorBase is deprecated; please use Tensor instead (this warning will be removed in a future version)\n",
      "  warnings.warn(\"TensorBase is deprecated; please use Tensor instead (this warning will be removed in a future version)\")\n",
      "/opt/conda/lib/python3.10/site-packages/fastai/data/load.py:148: UserWarning: Your `DataLoader`'s `num_workers` is `4` but `batch_size` is small (`1`). Toggling `num_workers=0` for debug purposes.\nYou can ignore this warning if you are not debugging. Initial `num_workers` is restored in `__del__`\n",
      "  if self.num_workers > 0 and warn:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished in 6.75 seconds.\n",
      "Peak Training VRAM used (Stage 2): 11.80 GB\n",
      "Saving final projector weights to: /workspace/llava/output/models/stage2_llava_lora_baseline_projector_final.pth\n",
      "Projector weights saved.\n",
      "LoRA configured but not applied. Cannot save adapters.\n",
      "Cleaned up baseline learner and model memory.\n",
      "Total Stage 2 script execution time (Baseline): 35.79 seconds.\n",
      "--- Stage 2 Training Complete (Baseline Model) --- \n",
      "\n",
      "--- Running Adaptive Stage 2 Training --- \n",
      "--- Starting Stage 2 Training (Adaptive Model) --- \n",
      "Loading configuration from: ../configs/temp_test_config.yaml\n",
      "Using ablation mode from config: baseline\n",
      "--- Setting up Stage 2 Learner (AdaptiveLLaVAModel) ---\n",
      "Loading Stage 2 DataLoaders...\n",
      "Creating Stage 2 DataLoaders with batch size: 4, num_workers: 4\n",
      "Loading Stage 2 items from: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "Assuming images relative to: /workspace/llava/data\n",
      "Found 2 samples for Stage 2.\n",
      "Stage 2 DataLoaders created successfully.\n",
      "DataLoaders loaded. Train samples: 1, Valid samples: 1\n",
      "Instantiating AdaptiveLLaVAModel for Stage 2... (Ablation: baseline)\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4099d3b23849798064c93b3d2c9f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM embedding size already matches tokenizer size. No resizing needed.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Adaptive Patcher enabled with strategy: 'variable_resolution' (VariableResolutionPatcher)\n",
      "Initialized VariableResolutionPatcher: Ablation active - FORCING BASELINE grid (336x336).\n",
      "Model instantiated successfully.\n",
      "Attempting to load Stage 1 projector weights from: /workspace/llava/output/models/stage1_projector.pth\n",
      "Successfully loaded Stage 1 projector weights from /workspace/llava/output/models/stage1_projector.pth\n",
      "Loss function: LLaVALoss\n",
      "Optimizer: AdamW (lr=2e-05, wd=0.0)\n",
      "Parameter splitter: adaptive_llava_stage2_splitter\n",
      "Metrics: ['perplexity']\n",
      "W&B enabled in config, but entity not set or default. Skipping W&B init and callback.\n",
      "SaveModelCallback is configured but commented out. Manual saving of adapters/projector is preferred.\n",
      "Added GradientAccumulation callback with 4 steps.\n",
      "Added MixedPrecision callback.\n",
      "Applying Stage 2 splitter (Adaptive)...\n",
      "  - Collecting projector parameters (trainable).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: LoRA configured but PEFT library not found. Freezing LLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Patcher found, but has no trainable parameters.\n",
      "  - Collecting vision tower parameters (frozen).\n",
      "Splitter created groups: Trainable (33554432 params), Frozen (6891018240 params)\n",
      "--- Stage 2 Learner Setup Complete (AdaptiveLLaVAModel) ---\n",
      "Reset CUDA peak memory stats before training.\n",
      "Starting training for 1 epochs with max_lr=2e-05...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/fastai/torch_core.py:263: UserWarning: TensorBase is deprecated; please use Tensor instead (this warning will be removed in a future version)\n",
      "  warnings.warn(\"TensorBase is deprecated; please use Tensor instead (this warning will be removed in a future version)\")\n",
      "/opt/conda/lib/python3.10/site-packages/fastai/data/load.py:148: UserWarning: Your `DataLoader`'s `num_workers` is `4` but `batch_size` is small (`1`). Toggling `num_workers=0` for debug purposes.\nYou can ignore this warning if you are not debugging. Initial `num_workers` is restored in `__del__`\n",
      "  if self.num_workers > 0 and warn:\n",
      "/tmp/ipykernel_140/4271200700.py:121: UserWarning: Patcher is active and needs raw_images, but they were not provided or length mismatch. Patcher cannot run.\n",
      "  warnings.warn(\"Patcher is active and needs raw_images, but they were not provided or length mismatch. Patcher cannot run.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished in 6.66 seconds.\n",
      "Peak Training VRAM used (Stage 2): 11.80 GB\n",
      "Saving final projector weights to: /workspace/llava/output/models/stage2_llava_lora_adaptive_abl-baseline_projector_final.pth\n",
      "Projector weights saved.\n",
      "LoRA configured but not applied. Cannot save adapters.\n",
      "Adaptive patcher exists but has no trainable parameters to save.\n",
      "Cleaned up adaptive learner and model memory.\n",
      "Total Stage 2 script execution time (Adaptive): 34.81 seconds.\n",
      "--- Stage 2 Training Complete (Adaptive Model) --- \n",
      "Stage 2 Training Test Run Complete.\n"
     ]
    }
   ],
   "source": [
    "#| hide \n",
    "# Example of how to run this from within the notebook (for testing purposes)\n",
    "# Requires dummy data and Stage 1 weights to be set up as in the learner test cell\n",
    "\n",
    "try:\n",
    "    # Reduce epochs for testing\n",
    "    _test_config = load_config('../configs/config.yaml')\n",
    "    _test_config['training']['num_epochs_stage2'] = 1 # Just 1 epoch for testing\n",
    "    # Enable adaptive patcher and LoRA\n",
    "    if 'model' not in _test_config: _test_config['model'] = {}\n",
    "    if 'peft' not in _test_config['model']: _test_config['model']['peft'] = {}\n",
    "    if 'adaptive_patcher' not in _test_config['model']: _test_config['model']['adaptive_patcher'] = {}\n",
    "    _test_config['model']['peft']['use_lora'] = True\n",
    "    _test_config['model']['adaptive_patcher']['enabled'] = True\n",
    "    _test_config['model']['adaptive_patcher']['strategy'] = 'variable_resolution'\n",
    "    # Ensure ablation is OFF for the baseline run\n",
    "    if 'ablation' not in _test_config: _test_config['ablation'] = {}\n",
    "    _test_config['ablation']['force_patcher_strategy'] = None \n",
    "    \n",
    "    # Ensure dummy data exists (copied from learner test)\n",
    "    output_dir = Path(_test_config['paths']['output_dir'])\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / 'models').mkdir(parents=True, exist_ok=True)\n",
    "    stage1_weights_fname = _test_config['paths'].get('stage1_projector_weights', 'stage1_projector.pth')\n",
    "    stage1_weights_path = output_dir / 'models' / stage1_weights_fname\n",
    "    if not stage1_weights_path.is_file():\n",
    "        proj_input_dim = _test_config['model']['projector']['input_dim']\n",
    "        proj_output_dim = _test_config['model']['projector']['output_dim']\n",
    "        from llava.model.baseline import LLaVAProjector\n",
    "        dummy_projector = LLaVAProjector(proj_input_dim, proj_output_dim)\n",
    "        torch.save(dummy_projector.state_dict(), stage1_weights_path)\n",
    "        del dummy_projector\n",
    "    data_base = Path(_test_config['paths']['data_base'])\n",
    "    stage2_json_rel = Path(_test_config['paths']['stage2_data'])\n",
    "    stage1_img_rel = Path(_test_config['paths']['stage1_images'])\n",
    "    stage1_img_path = data_base / stage1_img_rel\n",
    "    stage2_json_path = data_base / stage2_json_rel\n",
    "    stage2_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    stage1_img_path.mkdir(parents=True, exist_ok=True)\n",
    "    img1_rel_path_str = str(stage1_img_rel.name + '/dummy_img1.jpg')\n",
    "    img2_rel_path_str = str(stage1_img_rel.name + '/dummy_img2.png')\n",
    "    if not (stage1_img_path / 'dummy_img1.jpg').exists(): PIL.Image.new('RGB', (60, 30), color = 'red').save(stage1_img_path / 'dummy_img1.jpg')\n",
    "    if not (stage1_img_path / 'dummy_img2.png').exists(): PIL.Image.new('RGB', (60, 30), color = 'green').save(stage1_img_path / 'dummy_img2.png')\n",
    "    if not stage2_json_path.exists() or stage2_json_path.stat().st_size < 10:\n",
    "        dummy_jsonl_content = [\n",
    "            {\"id\": \"s2_001\", \"image\": img1_rel_path_str, \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nDescribe image.\"}, {\"from\": \"gpt\", \"value\": \"It is a red object.\"}]}, \n",
    "            {\"id\": \"s2_002\", \"image\": img2_rel_path_str, \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nIs it green?\"}, {\"from\": \"gpt\", \"value\": \"Yes, it appears green.\"}]},\n",
    "        ]\n",
    "        with open(stage2_json_path, 'w') as f: [f.write(json.dumps(item) + '\\n') for item in dummy_jsonl_content]\n",
    "    \n",
    "    # Save the modified config for the training function to load\n",
    "    import yaml\n",
    "    temp_config_path = '../configs/temp_test_config.yaml'\n",
    "    with open(temp_config_path, 'w') as f_temp:\n",
    "         yaml.dump(_test_config, f_temp)\n",
    "    \n",
    "    # Run baseline training first (ablation=None by default)\n",
    "    print(\"--- Running Baseline Stage 2 Training --- \")\n",
    "    train_stage2(temp_config_path)\n",
    "    \n",
    "    # Run adaptive training with forced baseline ablation\n",
    "    print(\"\\n--- Running Adaptive Stage 2 Training --- \")\n",
    "    train_adaptive_stage2(temp_config_path, ablation_mode='baseline')\n",
    "    \n",
    "    # Clean up temp config\n",
    "    # os.remove(temp_config_path) \n",
    "    print(\"Stage 2 Training Test Run Complete.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Skipping training test: FileNotFoundError - {e}\")\n",
    "except RuntimeError as e:\n",
    "     print(f\"Skipping training test: RuntimeError - {e}. (Likely CUDA OOM or model setup issue)\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Stage 2 training test run: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Command-line execution block for Stage 2\n",
    "if __name__ == \"__main__\" and \"get_ipython\" not in locals():\n",
    "    parser = argparse.ArgumentParser(description=\"Run LLaVA Stage 2 Training\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"configs/config.yaml\", \n",
    "                        help=\"Path to the configuration YAML file (relative to project root or absolute).\")\n",
    "    parser.add_argument(\"--model_type\", type=str, default=\"baseline\", choices=['baseline', 'adaptive'],\n",
    "                        help=\"Choose model type to train: 'baseline' or 'adaptive'.\")\n",
    "    parser.add_argument(\"--ablation\", type=str, default=None, choices=[None, 'baseline'], # Add ablation choices\n",
    "                        help=\"Optional ablation mode (e.g., 'baseline' to force base grid). Overrides config.\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Resolve config path relative to project root (defined earlier in the script's import section)\n",
    "    config_file_path = project_root / args.config\n",
    "    \n",
    "    if not config_file_path.is_file():\n",
    "        print(f\"Error: Config file not found at {config_file_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        if args.model_type == 'baseline':\n",
    "            # Ablation doesn't apply to baseline model directly, but pass None\n",
    "            train_stage2(config_path=config_file_path, ablation_mode=None) \n",
    "        elif args.model_type == 'adaptive':\n",
    "            train_adaptive_stage2(config_path=config_file_path, ablation_mode=args.ablation)\n",
    "        else:\n",
    "             # This shouldn't happen due to choices in argparse\n",
    "             print(f\"Error: Invalid model_type '{args.model_type}'. Choose 'baseline' or 'adaptive'.\")\n",
    "             sys.exit(1)\n",
    "    except NotImplementedError as e:\n",
    "         print(f\"Exiting: {e}\") \n",
    "         sys.exit(0) \n",
    "    except Exception as e:\n",
    "        print(f\"Stage 2 training setup or execution failed for model type '{args.model_type}'{(' with ablation ' + args.ablation) if args.ablation else ''}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        # W&B run should be finished by the finally block in train_stage2/train_adaptive_stage2\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
