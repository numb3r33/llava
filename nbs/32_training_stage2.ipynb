{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 Training: Instruction Fine-tuning\n",
    "\n",
    "> Sets up and runs the second stage of LLaVA training: fine-tuning the LLM (using LoRA) and the projector on instruction-following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc # For memory cleanup\n",
    "import argparse # For command-line execution\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    # print(f\"Project root already in sys.path: {project_root_str}\") # Less verbose for script\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: peft library not found. LoRA functionality will be disabled.\n",
      "Loaded config from configs/config.yaml\n",
      "Successfully loaded tokenizer for: lmsys/vicuna-7b-v1.5\n",
      "Adding special token <image> to tokenizer.\n",
      "Added 1 token(s). New vocab size: 32001\n",
      "Using token ID for <image>: 32000\n",
      "Tokenizer already has pad token: <unk> (ID: 0)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Successfully loaded CLIP image processor for: openai/clip-vit-large-patch14-336\n",
      "LLaVABatchTransform initialized. Image Token ID: 32000, Pad Token ID: 0\n",
      "LLaVADataBlockStage2 defined.\n",
      "LLaVALoss initialized, ignoring index: -100\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastai.learner import Learner\n",
    "from fastai.optimizer import AdamW\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.schedule import fit_one_cycle\n",
    "from fastai.callback.save import SaveModelCallback\n",
    "from fastai.callback.training import GradientAccumulation\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "from fastai.vision.all import params # For splitter\n",
    "from fastai.data.core import DataLoaders\n",
    "from functools import partial\n",
    "import wandb # Import wandb directly for cleanup\n",
    "import json # For dummy data creation\n",
    "import PIL.Image # For dummy data creation\n",
    "from typing import List, Optional\n",
    "\n",
    "# Attempt to import peft, set flag\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    _peft_available = True\n",
    "except ImportError:\n",
    "    print(\"Warning: peft library not found. LoRA functionality will be disabled.\")\n",
    "    PeftModel = None # Define as None if not available\n",
    "    _peft_available = False\n",
    "\n",
    "try:\n",
    "    from llava.utils import load_config, init_wandb\n",
    "    from llava.data.loading import get_stage2_dataloaders\n",
    "    from llava.model.baseline import BaselineLLaVAModel\n",
    "    from llava.training.core import LLaVALoss\n",
    "except ImportError as e:\n",
    "     print(f\"Error importing llava modules: {e}\")\n",
    "     print(\"Ensure that nbdev_export has been run and the llava library is installed/accessible.\")\n",
    "     # In a script context, it's better to exit if core modules are missing\n",
    "     if __name__ == \"__main__\" and \"get_ipython\" not in locals():\n",
    "          sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 Splitter Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stage 2, we fine-tune the `projector` and the `language_model`. If LoRA is enabled (`use_lora: true` in config), only the LoRA adapter parameters within the language model are trained. If LoRA is disabled, the entire language model is trained. The `vision_tower` remains frozen by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llava_stage2_splitter(model: BaselineLLaVAModel):\n",
    "    \"\"\"Splits the `BaselineLLaVAModel` parameters for Stage 2 training.\n",
    "\n",
    "    Trains the `projector` and LoRA adapters (if enabled) or the full `language_model`.\n",
    "    Keeps the `vision_tower` frozen by default.\n",
    "\n",
    "    Args:\n",
    "        model: An instance of `BaselineLLaVAModel`.\n",
    "\n",
    "    Returns:\n",
    "        A list containing parameter groups for trainable components.\n",
    "    \"\"\"\n",
    "    projector_params = []\n",
    "    llm_params = []\n",
    "    frozen_params = []\n",
    "\n",
    "    print(\"Applying Stage 2 splitter...\")\n",
    "\n",
    "    # Projector parameters are always trained in Stage 2\n",
    "    if hasattr(model, 'projector') and model.projector is not None:\n",
    "        print(\"  - Collecting projector parameters (trainable).\")\n",
    "        projector_params.extend(list(model.projector.parameters()))\n",
    "        for p in model.projector.parameters():\n",
    "             p.requires_grad = True\n",
    "    else:\n",
    "        print(\"Warning: Model has no projector attribute.\")\n",
    "\n",
    "    # Handle Language Model parameters based on LoRA configuration\n",
    "    if hasattr(model, 'language_model') and model.language_model is not None:\n",
    "        # Access config stored within the model instance\n",
    "        # Handle potential nested structure in config access more safely\n",
    "        use_lora = model.config.get('model', {}).get('peft', {}).get('use_lora', False)\n",
    "\n",
    "        if _peft_available and use_lora and isinstance(model.language_model, PeftModel):\n",
    "            print(\"  - LoRA enabled: Collecting LLM adapter parameters (trainable).\")\n",
    "            # PEFT model automatically handles requires_grad for adapters\n",
    "            llm_params.extend([p for p in model.language_model.parameters() if p.requires_grad])\n",
    "            # Base model parameters should already be frozen by PEFT\n",
    "            # We can double-check frozen status for verification\n",
    "            # frozen_params.extend([p for p in model.language_model.parameters() if not p.requires_grad])\n",
    "        elif use_lora and not _peft_available:\n",
    "             print(\"Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.\")\n",
    "             for p in model.language_model.parameters():\n",
    "                 p.requires_grad = False\n",
    "             # frozen_params.extend(list(model.language_model.parameters()))\n",
    "        else:\n",
    "            print(\"  - LoRA disabled: Collecting all LLM parameters (trainable).\")\n",
    "            llm_params.extend(list(model.language_model.parameters()))\n",
    "            for p in model.language_model.parameters():\n",
    "                 p.requires_grad = True # Ensure all LLM params are trainable if not using LoRA\n",
    "    else:\n",
    "        print(\"Warning: Model has no language_model attribute.\")\n",
    "\n",
    "    # Vision Tower parameters are frozen by default\n",
    "    if hasattr(model, 'vision_tower') and model.vision_tower is not None:\n",
    "        print(\"  - Collecting vision tower parameters (frozen).\")\n",
    "        # frozen_params.extend(list(model.vision_tower.parameters())) # Collect if needed for verification\n",
    "        for p in model.vision_tower.parameters():\n",
    "            p.requires_grad = False\n",
    "    else:\n",
    "        print(\"Warning: Model has no vision_tower attribute.\")\n",
    "\n",
    "    # Combine trainable parameters into one group for the optimizer\n",
    "    trainable_groups = projector_params + llm_params\n",
    "    # Count frozen parameters for verification\n",
    "    frozen_count = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    trainable_count = sum(p.numel() for p in trainable_groups)\n",
    "    print(f\"Splitter created groups: Trainable ({trainable_count} params), Frozen ({frozen_count} params)\")\n",
    "    \n",
    "    if not trainable_groups:\n",
    "         raise ValueError(\"Splitter function resulted in no trainable parameters. Check model structure and config.\")\n",
    "         \n",
    "    return [trainable_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(llava_stage2_splitter) # Omitted for script execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3: Setup Learner Configuration (Stage 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function sets up the `Learner` object for Stage 2 instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stage2_learner(config: dict) -> Learner:\n",
    "    \"\"\"Configures and returns a fastai Learner for Stage 2 Instruction Fine-tuning.\n",
    "\n",
    "    Loads Stage 1 projector weights, sets up the model (potentially with LoRA),\n",
    "    uses the Stage 2 splitter, and includes relevant callbacks.\n",
    "\n",
    "    Args:\n",
    "        config: The main configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A configured fastai Learner instance for Stage 2.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If DataLoaders or Model instantiation fails.\n",
    "        FileNotFoundError: If Stage 1 projector weights or data paths are invalid.\n",
    "        AttributeError: If the model is missing expected components.\n",
    "    \"\"\"\n",
    "    print(\"--- Setting up Stage 2 Learner ---\")\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1. Load Stage 2 DataLoaders\n",
    "    print(\"Loading Stage 2 DataLoaders...\")\n",
    "    try:\n",
    "        dls = get_stage2_dataloaders(config)\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        print(f\"Error loading Stage 2 DataLoaders: {e}\")\n",
    "        raise RuntimeError(\"Failed to create Stage 2 DataLoaders.\") from e\n",
    "    if not dls:\n",
    "        raise RuntimeError(\"Stage 2 DataLoaders object is None.\")\n",
    "    print(f\"DataLoaders loaded. Train samples: {len(dls.train_ds)}, Valid samples: {len(dls.valid_ds)}\")\n",
    "\n",
    "    # 2. Instantiate Model (handles LoRA based on config)\n",
    "    print(\"Instantiating BaselineLLaVAModel for Stage 2...\")\n",
    "    try:\n",
    "        model = BaselineLLaVAModel(config)\n",
    "        if model.vision_tower is None or model.language_model is None or model.projector is None:\n",
    "            raise RuntimeError(\"BaselineLLaVAModel initialization incomplete.\")\n",
    "        print(\"Model instantiated successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error instantiating BaselineLLaVAModel: {e}\")\n",
    "        raise RuntimeError(\"Failed to instantiate baseline model for Stage 2.\") from e\n",
    "\n",
    "    # 3. Load Stage 1 Projector Weights\n",
    "    stage1_weights_fname = config['paths'].get('stage1_projector_weights', 'stage1_projector.pth')\n",
    "    # Look for weights relative to output_dir/models/\n",
    "    stage1_weights_path = output_dir / 'models' / stage1_weights_fname\n",
    "    print(f\"Attempting to load Stage 1 projector weights from: {stage1_weights_path}\")\n",
    "    if stage1_weights_path.is_file():\n",
    "        try:\n",
    "            # Load state dict onto CPU first to avoid device mismatches\n",
    "            projector_state_dict = torch.load(stage1_weights_path, map_location='cpu')\n",
    "            # Load into the model's projector\n",
    "            model.projector.load_state_dict(projector_state_dict)\n",
    "            print(f\"Successfully loaded Stage 1 projector weights from {stage1_weights_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Stage 1 projector weights: {e}\")\n",
    "            # Decide whether to raise error or continue with random init projector\n",
    "            # For reproducibility, it's usually better to raise if weights are expected.\n",
    "            raise RuntimeError(f\"Failed to load expected Stage 1 projector weights from {stage1_weights_path}\") from e\n",
    "    else:\n",
    "        print(f\"Warning: Stage 1 projector weights not found at {stage1_weights_path}. Projector will use initial weights.\")\n",
    "        # Optionally raise an error if pre-trained weights are strictly required\n",
    "        # raise FileNotFoundError(f\"Stage 1 projector weights not found: {stage1_weights_path}\")\n",
    "\n",
    "    # 4. Define Loss Function\n",
    "    loss_func = LLaVALoss()\n",
    "    print(f\"Loss function: {type(loss_func).__name__}\")\n",
    "\n",
    "    # 5. Define Optimizer\n",
    "    lr = config.get('training', {}).get('learning_rate_stage2', 2e-5) # Lower LR for fine-tuning\n",
    "    wd = config.get('training', {}).get('weight_decay', 0.0)\n",
    "    opt_func = partial(AdamW, lr=lr, wd=wd, eps=1e-8)\n",
    "    print(f\"Optimizer: AdamW (lr={lr}, wd={wd})\")\n",
    "\n",
    "    # 6. Define Splitter\n",
    "    splitter = llava_stage2_splitter\n",
    "    print(f\"Parameter splitter: {splitter.__name__}\")\n",
    "\n",
    "    # 7. Define Callbacks\n",
    "    cbs = []\n",
    "    if config.get('logging', {}).get('wandb', {}).get('enabled', False):\n",
    "        # Init W&B Run only if enabled and entity is likely configured\n",
    "        wandb_entity = config.get('logging', {}).get('wandb', {}).get('entity')\n",
    "        if wandb_entity and 'your_wandb_entity' not in str(wandb_entity):\n",
    "            project_name = config.get('logging', {}).get('wandb', {}).get('project', 'llava-adaptive-patching')\n",
    "            run_name_prefix = config.get('logging', {}).get('wandb', {}).get('run_name_prefix', 'stage2')\n",
    "            stage2_model_name = Path(config['paths']['stage2_model_weights']).stem\n",
    "            run_name = f\"{run_name_prefix}_{stage2_model_name}_{wandb.util.generate_id()}\"\n",
    "            init_wandb(config, job_type=\"stage2-training\", run_name=run_name)\n",
    "            cbs.append(WandbCallback(log_preds=False, log_model=False))\n",
    "            print(\"Added WandbCallback.\")\n",
    "        else:\n",
    "            print(\"W&B enabled in config, but entity not set or default. Skipping W&B init and callback.\")\n",
    "\n",
    "    # SaveModelCallback for Stage 2 (saves best full model state)\n",
    "    # Adapter saving might need custom handling in train loop or separate script (Step 4.5)\n",
    "    stage2_model_fname = Path(config['paths']['stage2_model_weights']).stem\n",
    "    save_cb = SaveModelCallback(\n",
    "        monitor='valid_loss',\n",
    "        min_delta=0.001,\n",
    "        fname=stage2_model_fname, # Saves best model state\n",
    "        every_epoch=False,\n",
    "        with_opt=True, # Save optimizer state to resume training if needed\n",
    "        reset_on_fit=True\n",
    "    )\n",
    "    # Note: SaveModelCallback saves the entire learner state, including the full model.\n",
    "    # For LoRA, we'll manually save adapters at the end of training in train_stage2.\n",
    "    # cbs.append(save_cb) # We might disable this if saving adapters manually is sufficient.\n",
    "    print(f\"SaveModelCallback is configured but commented out. Manual saving of adapters/projector in train_stage2 is preferred for LoRA.\")\n",
    "\n",
    "    # Optimization Callbacks\n",
    "    grad_accum_steps = config.get('training', {}).get('gradient_accumulation_steps', 1)\n",
    "    if grad_accum_steps > 1:\n",
    "        cbs.append(GradientAccumulation(grad_accum_steps))\n",
    "        print(f\"Added GradientAccumulation callback with {grad_accum_steps} steps.\")\n",
    "    \n",
    "    use_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)\n",
    "    if use_mixed_precision:\n",
    "        cbs.append(MixedPrecision())\n",
    "        print(\"Added MixedPrecision callback.\")\n",
    "\n",
    "    # 8. Create Learner\n",
    "    try:\n",
    "        learner = Learner(\n",
    "            dls=dls,\n",
    "            model=model,\n",
    "            loss_func=loss_func,\n",
    "            opt_func=opt_func,\n",
    "            splitter=splitter,\n",
    "            cbs=cbs,\n",
    "            path=output_dir,\n",
    "            train_bn=False # Typically False for LLM fine-tuning\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Stage 2 Learner: {e}\")\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish(exit_code=1)\n",
    "            print(\"Finished W&B run due to error during Learner creation.\")\n",
    "        raise RuntimeError(\"Failed to create the Stage 2 Learner object.\") from e\n",
    "\n",
    "    print(\"--- Stage 2 Learner Setup Complete ---\")\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(get_stage2_learner) # Omitted for script execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage & Test (Learner Configuration - Stage 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded config from ../configs/config.yaml\n",
      "Creating dummy Stage 1 projector weights: /workspace/llava/output/models/stage1_projector.pth\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Creating dummy Stage 2 JSONL: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "Warning: W&B is enabled but entity is not set or default. Disabling W&B for this test.\n",
      "--- Setting up Stage 2 Learner ---\n",
      "Loading Stage 2 DataLoaders...\n",
      "Creating Stage 2 DataLoaders with batch size: 4, num_workers: 4\n",
      "Loading Stage 2 items from: /workspace/llava/data/llava_instruct_150k/llava_v1_5_mix665k.jsonl\n",
      "Assuming images relative to: /workspace/llava/data\n",
      "Found 2 samples for Stage 2.\n",
      "Stage 2 DataLoaders created successfully.\n",
      "DataLoaders loaded. Train samples: 1, Valid samples: 1\n",
      "Instantiating BaselineLLaVAModel for Stage 2...\n",
      "Initializing Projector: Input Dim=1024, Output Dim=4096\n",
      "Loading Vision Tower: openai/clip-vit-large-patch14-336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_embedding.weight', 'text_model.final_layer_norm.bias', 'text_model.final_layer_norm.weight', 'text_projection.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Tower loaded successfully.\n",
      "Vision Tower weights frozen.\n",
      "Loading Language Model: lmsys/vicuna-7b-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2646864c336140a9801d49b80d996b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model loaded successfully.\n",
      "Base Language Model weights frozen.\n",
      "Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.\n",
      "LLM embedding size already matches tokenizer size. No resizing needed.\n",
      "Activation checkpointing is disabled in the configuration.\n",
      "Model instantiated successfully.\n",
      "Attempting to load Stage 1 projector weights from: /workspace/llava/output/models/stage1_projector.pth\n",
      "Successfully loaded Stage 1 projector weights from /workspace/llava/output/models/stage1_projector.pth\n",
      "Loss function: LLaVALoss\n",
      "Optimizer: AdamW (lr=2e-05, wd=0.0)\n",
      "Parameter splitter: llava_stage2_splitter\n",
      "W&B enabled in config, but entity not set or default. Skipping W&B init and callback.\n",
      "SaveModelCallback is configured but commented out. Manual saving of adapters/projector in train_stage2 is preferred for LoRA.\n",
      "Added GradientAccumulation callback with 4 steps.\n",
      "Added MixedPrecision callback.\n",
      "Applying Stage 2 splitter...\n",
      "  - Collecting projector parameters (trainable).\n",
      "Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.\n",
      "  - Collecting vision tower parameters (frozen).\n",
      "Splitter created groups: Trainable (33554432 params), Frozen (6891018240 params)\n",
      "--- Stage 2 Learner Setup Complete ---\n",
      "Stage 2 Learner created successfully.\n",
      "Checking parameter groups...\n",
      "  Group 0: 33554432 parameters (Trainable)\n",
      "Checking parameters in group 0 (first few):\n",
      "  - projector.model.0.weight: True\n",
      "  - projector.model.0.bias: True\n",
      "  - projector.model.2.weight: True\n",
      "  - projector.model.2.bias: True\n",
      "Splitter check passed: Trainable/Frozen status seems correct based on LoRA config.\n",
      "Checking frozen parameters (examples):\n",
      "  - vision_tower.vision_model.embeddings.patch_embedding.weight: False\n",
      "  - vision_tower.vision_model.embeddings.position_embedding.weight: False\n",
      "Callback check passed.\n",
      "\n",
      "Stage 2 Learner setup test passed.\n",
      "Cleaned up stage2_learner and model\n"
     ]
    }
   ],
   "source": [
    "#| test \n",
    "import gc\n",
    "from fastai.callback.save import SaveModelCallback\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.training import GradientAccumulation\n",
    "from fastai.callback.fp16 import MixedPrecision\n",
    "# import llava # Import base library for module check\n",
    "from llava.model.baseline import LLaVAProjector, BaselineLLaVAModel # Import required classes\n",
    "\n",
    "try:\n",
    "    # Load config\n",
    "    config_path = '../configs/config.yaml'\n",
    "    config = load_config(config_path)\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "    \n",
    "    # --- Test Setup ---\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / 'models').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create dummy Stage 1 weights if they don't exist\n",
    "    stage1_weights_fname = config['paths'].get('stage1_projector_weights', 'stage1_projector.pth')\n",
    "    stage1_weights_path = output_dir / 'models' / stage1_weights_fname\n",
    "    if not stage1_weights_path.is_file():\n",
    "        print(f\"Creating dummy Stage 1 projector weights: {stage1_weights_path}\")\n",
    "        # Create a dummy projector state dict matching the config\n",
    "        proj_input_dim = config['model']['projector']['input_dim']\n",
    "        proj_output_dim = config['model']['projector']['output_dim']\n",
    "        dummy_projector = LLaVAProjector(proj_input_dim, proj_output_dim)\n",
    "        torch.save(dummy_projector.state_dict(), stage1_weights_path)\n",
    "        del dummy_projector # Clean up\n",
    "        \n",
    "    # Create dummy Stage 2 data if needed\n",
    "    data_base = Path(config['paths']['data_base'])\n",
    "    stage2_json_rel = Path(config['paths']['stage2_data'])\n",
    "    stage1_img_rel = Path(config['paths']['stage1_images']) # Needed for dummy image paths\n",
    "    stage1_img_path = data_base / stage1_img_rel\n",
    "    stage2_json_path = data_base / stage2_json_rel\n",
    "    stage2_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    stage1_img_path.mkdir(parents=True, exist_ok=True) # Ensure image dir exists\n",
    "\n",
    "    img1_rel_path_str = str(stage1_img_rel.name + '/dummy_img1.jpg')\n",
    "    img2_rel_path_str = str(stage1_img_rel.name + '/dummy_img2.png')\n",
    "    if not (stage1_img_path / 'dummy_img1.jpg').exists():\n",
    "        PIL.Image.new('RGB', (60, 30), color = 'red').save(stage1_img_path / 'dummy_img1.jpg')\n",
    "    if not (stage1_img_path / 'dummy_img2.png').exists():\n",
    "        PIL.Image.new('RGB', (60, 30), color = 'green').save(stage1_img_path / 'dummy_img2.png')\n",
    "\n",
    "    if not stage2_json_path.exists() or stage2_json_path.stat().st_size < 10:\n",
    "        print(f\"Creating dummy Stage 2 JSONL: {stage2_json_path}\")\n",
    "        dummy_jsonl_content = [\n",
    "            {\"id\": \"s2_001\", \"image\": img1_rel_path_str, \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nDescribe image.\"}, {\"from\": \"gpt\", \"value\": \"It is a red object.\"}]}, \n",
    "            {\"id\": \"s2_002\", \"image\": img2_rel_path_str, \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nIs it green?\"}, {\"from\": \"gpt\", \"value\": \"Yes, it appears green.\"}]},\n",
    "        ]\n",
    "        with open(stage2_json_path, 'w') as f:\n",
    "            for item in dummy_jsonl_content:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "    # -----------------\n",
    "    \n",
    "    # Ensure LoRA is enabled in config for testing the splitter\n",
    "    # Safely modify the dictionary\n",
    "    if 'model' not in config: config['model'] = {}\n",
    "    if 'peft' not in config['model']: config['model']['peft'] = {}\n",
    "    config['model']['peft']['use_lora'] = True\n",
    "    config['model']['use_activation_checkpointing'] = False # Keep disabled for test\n",
    "\n",
    "    # Disable W&B for testing unless entity is properly set\n",
    "    wandb_enabled = config.get('logging', {}).get('wandb', {}).get('enabled', False)\n",
    "    wandb_entity = config.get('logging', {}).get('wandb', {}).get('entity')\n",
    "    if wandb_enabled and (wandb_entity is None or 'your_wandb_entity' in str(wandb_entity)):\n",
    "        print(\"Warning: W&B is enabled but entity is not set or default. Disabling W&B for this test.\")\n",
    "        if 'logging' not in config: config['logging'] = {}\n",
    "        if 'wandb' not in config['logging']: config['logging']['wandb'] = {}\n",
    "        config['logging']['wandb']['enabled'] = False\n",
    "\n",
    "    # Create learner\n",
    "    stage2_learner = get_stage2_learner(config)\n",
    "    print(\"Stage 2 Learner created successfully.\")\n",
    "\n",
    "    # Basic checks\n",
    "    assert isinstance(stage2_learner, Learner)\n",
    "    assert isinstance(stage2_learner.dls, DataLoaders)\n",
    "    assert isinstance(stage2_learner.model, BaselineLLaVAModel)\n",
    "    assert isinstance(stage2_learner.loss_func, LLaVALoss)\n",
    "    assert len(stage2_learner.opt.param_groups) == 1 # Splitter should return one group\n",
    "\n",
    "    # Check parameter freezing status based on splitter\n",
    "    print(\"Checking parameter groups...\")\n",
    "    trainable_param_count = 0\n",
    "    frozen_param_count = 0\n",
    "    trainable_param_names = []\n",
    "    frozen_param_names = []\n",
    "    for name, param in stage2_learner.model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_param_count += param.numel()\n",
    "            trainable_param_names.append(name)\n",
    "        else:\n",
    "            frozen_param_count += param.numel()\n",
    "            frozen_param_names.append(name)\n",
    "            \n",
    "    print(f\"  Group 0: {trainable_param_count} parameters (Trainable)\")\n",
    "    print(\"Checking parameters in group 0 (first few):\")\n",
    "    for i in range(min(10, len(trainable_param_names))):\n",
    "        name = trainable_param_names[i]\n",
    "        print(f\"  - {name}: {stage2_learner.model.get_parameter(name).requires_grad}\")\n",
    "\n",
    "    is_projector_trainable = any('projector' in name for name in trainable_param_names)\n",
    "    is_lora_trainable = any('lora_' in name for name in trainable_param_names)\n",
    "    is_vision_frozen = all('vision_tower' in name for name in frozen_param_names if 'vision_tower' in name)\n",
    "    is_base_llm_frozen = all('language_model.base_model' in name and 'lora_' not in name for name in frozen_param_names if 'language_model' in name)\n",
    "    \n",
    "    assert is_projector_trainable, \"Projector parameters not found in trainable group.\"\n",
    "    \n",
    "    if _peft_available and config['model']['peft']['use_lora']:\n",
    "        assert is_lora_trainable, \"LoRA adapter parameters not found in trainable group when LoRA is enabled.\"\n",
    "        assert is_base_llm_frozen, \"Base LLM parameters are not frozen when LoRA is enabled.\"\n",
    "    else:\n",
    "         # If LoRA disabled, all LLM params should be trainable\n",
    "         is_llm_trainable = any('language_model' in name for name in trainable_param_names)\n",
    "         # If PEFT is not available, LoRA cannot be applied, so the LLM should be frozen by the splitter's warning path\n",
    "         if not _peft_available and config['model']['peft']['use_lora']:\n",
    "              assert not is_llm_trainable, \"LLM parameters are trainable when LoRA was configured but PEFT is unavailable.\"\n",
    "         else:\n",
    "              assert is_llm_trainable, \"LLM parameters not found in trainable group when LoRA is disabled.\"\n",
    "         \n",
    "    assert is_vision_frozen, \"Vision tower parameters are not frozen.\"\n",
    "    print(\"Checking frozen parameters (examples):\")\n",
    "    if frozen_param_names:\n",
    "        print(f\"  - {frozen_param_names[0]}: {stage2_learner.model.get_parameter(frozen_param_names[0]).requires_grad}\")\n",
    "        if len(frozen_param_names) > 1: print(f\"  - {frozen_param_names[1]}: {stage2_learner.model.get_parameter(frozen_param_names[1]).requires_grad}\")\n",
    "    \n",
    "    print(\"Splitter check passed: Trainable/Frozen status seems correct based on LoRA config.\")\n",
    "\n",
    "    # Check callbacks\n",
    "    # has_save_cb = any(isinstance(cb, SaveModelCallback) for cb in stage2_learner.cbs)\n",
    "    expect_grad_accum = config.get('training', {}).get('gradient_accumulation_steps', 1) > 1\n",
    "    has_grad_accum = any(isinstance(cb, GradientAccumulation) for cb in stage2_learner.cbs)\n",
    "    expect_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)\n",
    "    has_mixed_precision = any(isinstance(cb, MixedPrecision) for cb in stage2_learner.cbs)\n",
    "    # Check W&B based on actual decision made in get_stage2_learner\n",
    "    wandb_init_decision = config.get('logging', {}).get('wandb', {}).get('enabled', False) and \\\n",
    "                          config.get('logging', {}).get('wandb', {}).get('entity') is not None and \\\n",
    "                         'your_wandb_entity' not in str(config.get('logging', {}).get('wandb', {}).get('entity'))\n",
    "    has_wandb_cb = any(isinstance(cb, WandbCallback) for cb in stage2_learner.cbs)\n",
    "    \n",
    "    # assert has_save_cb # Disabled check as SaveModelCallback might be commented out\n",
    "    assert has_grad_accum == expect_grad_accum\n",
    "    assert has_mixed_precision == expect_mixed_precision\n",
    "    assert has_wandb_cb == wandb_init_decision # Check if WandbCallback added matches decision\n",
    "    print(\"Callback check passed.\")\n",
    "    \n",
    "    # print(\"Learner summary:\") # Removed to avoid potential JSON errors in output\n",
    "    # stage2_learner.summary()\n",
    "\n",
    "    print(\"\\nStage 2 Learner setup test passed.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Skipping test: FileNotFoundError - {e}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Skipping test: ImportError - {e}. (Likely `peft` is missing)\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Skipping test: RuntimeError - {e}. (Likely CUDA OOM or setup issue)\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred during Stage 2 learner setup test: {e}\")\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Clean up memory\n",
    "    if 'stage2_learner' in locals() and stage2_learner is not None:\n",
    "        if hasattr(stage2_learner, 'model') and stage2_learner.model is not None:\n",
    "            # Move components to CPU before deleting\n",
    "            if hasattr(stage2_learner.model, 'vision_tower') and stage2_learner.model.vision_tower is not None:\n",
    "                stage2_learner.model.vision_tower.to('cpu')\n",
    "            if hasattr(stage2_learner.model, 'language_model') and stage2_learner.model.language_model is not None:\n",
    "                stage2_learner.model.language_model.to('cpu')\n",
    "            if hasattr(stage2_learner.model, 'projector') and stage2_learner.model.projector is not None:\n",
    "                stage2_learner.model.projector.to('cpu')\n",
    "            del stage2_learner.model\n",
    "        stage2_learner.destroy() # Clean up learner properly\n",
    "        del stage2_learner\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Cleaned up stage2_learner and model\")\n",
    "    # Terminate wandb run if it was initialized and not already finished\n",
    "    if wandb.run is not None:\n",
    "        try:\n",
    "            if wandb.run.id:\n",
    "                wandb.finish()\n",
    "                # print(\"Finished W&B run.\") # Less verbose in script\n",
    "        except Exception as e:\n",
    "            print(f\"Error finishing W&B run: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.5: Implement Stage 2 Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function orchestrates the Stage 2 training loop, including saving the final weights (projector + LoRA adapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_stage2(config_path: str | Path):\n",
    "    \"\"\"Loads config, sets up Stage 2 learner, runs training, and saves weights.\n",
    "    \n",
    "    Saves the projector weights and LoRA adapter weights (if used) separately.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the YAML configuration file.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Stage 2 Training --- \")\n",
    "    print(f\"Loading configuration from: {config_path}\")\n",
    "    config = load_config(config_path)\n",
    "    output_dir = Path(config['paths']['output_dir'])\n",
    "    models_dir = output_dir / 'models'\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    learner = None # Initialize learner to None for finally block\n",
    "    try:\n",
    "        # --- Get Learner (handles loading Stage 1 weights, LoRA setup, etc.) --- \n",
    "        learner = get_stage2_learner(config)\n",
    "        \n",
    "        # --- Start Training --- \n",
    "        epochs = config.get('training', {}).get('num_epochs_stage2', 3)\n",
    "        lr = config.get('training', {}).get('learning_rate_stage2', 2e-5)\n",
    "        print(f\"Starting training for {epochs} epochs with max_lr={lr}...\")\n",
    "        \n",
    "        learner.fit_one_cycle(epochs, lr_max=lr)\n",
    "        \n",
    "        print(\"Training finished.\")\n",
    "        \n",
    "        # --- Save final trained weights --- \n",
    "        \n",
    "        # 1. Save Projector Weights\n",
    "        projector_save_path = models_dir / (Path(config['paths']['stage2_model_weights']).stem + \"_projector_final.pth\")\n",
    "        print(f\"Saving final projector weights to: {projector_save_path}\")\n",
    "        if hasattr(learner.model, 'projector') and learner.model.projector is not None:\n",
    "             torch.save(learner.model.projector.state_dict(), projector_save_path)\n",
    "             print(\"Projector weights saved.\")\n",
    "        else:\n",
    "             print(\"Warning: Cannot save projector weights, model has no projector.\")\n",
    "             \n",
    "        # 2. Save LoRA Adapters (if LoRA was used)\n",
    "        use_lora = config.get('model', {}).get('peft', {}).get('use_lora', False)\n",
    "        if _peft_available and use_lora and isinstance(learner.model.language_model, PeftModel):\n",
    "            lora_save_dir = models_dir / (Path(config['paths']['stage2_model_weights']).stem + \"_lora_adapters\")\n",
    "            print(f\"Saving LoRA adapters to: {lora_save_dir}\")\n",
    "            try:\n",
    "                learner.model.language_model.save_pretrained(lora_save_dir)\n",
    "                print(\"LoRA adapters saved successfully.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error saving LoRA adapters: {e}\")\n",
    "        elif not use_lora:\n",
    "             print(\"LoRA was not enabled. Only projector weights saved explicitly.\")\n",
    "             # Note: The full model state might have been saved by SaveModelCallback if it was enabled.\n",
    "        else: # LoRA enabled in config but PEFT not available or not applied\n",
    "             print(\"LoRA configured but not applied (PEFT library issue?). Cannot save adapters.\")\n",
    "             \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Stage 2 training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Potentially re-raise or handle cleanup\n",
    "        raise e\n",
    "    finally:\n",
    "        # Clean up memory\n",
    "        if learner is not None and hasattr(learner, 'model') and learner.model is not None:\n",
    "            if hasattr(learner.model, 'vision_tower') and learner.model.vision_tower is not None:\n",
    "                learner.model.vision_tower.to('cpu')\n",
    "            if hasattr(learner.model, 'language_model') and learner.model.language_model is not None:\n",
    "                learner.model.language_model.to('cpu')\n",
    "            if hasattr(learner.model, 'projector') and learner.model.projector is not None:\n",
    "                learner.model.projector.to('cpu')\n",
    "            del learner.model\n",
    "            learner.destroy() # Release learner resources\n",
    "            del learner\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Cleaned up learner and model memory.\")\n",
    "            \n",
    "        # Ensure W&B run is finished if it was started\n",
    "        if wandb.run is not None:\n",
    "            try:\n",
    "                if wandb.run.id: # Check if run is still active\n",
    "                    wandb.finish()\n",
    "                    print(\"Finished W&B run.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error finishing W&B run: {e}\")\n",
    "            \n",
    "    print(f\"--- Stage 2 Training Complete --- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(train_stage2) # Omitted for script execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Command-line execution block for Stage 2\n",
    "if __name__ == \"__main__\" and \"get_ipython\" not in locals():\n",
    "    parser = argparse.ArgumentParser(description=\"Run LLaVA Stage 2 Training\")\n",
    "    # Assume script is run from project root\n",
    "    parser.add_argument(\"--config\", type=str, default=\"configs/config.yaml\", \n",
    "                        help=\"Path to the configuration YAML file (relative to project root or absolute).\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Resolve config path relative to project root (defined earlier in the script's import section)\n",
    "    config_file_path = project_root / args.config\n",
    "    \n",
    "    if not config_file_path.is_file():\n",
    "        print(f\"Error: Config file not found at {config_file_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        train_stage2(config_path=config_file_path) \n",
    "    except NotImplementedError as e:\n",
    "         print(f\"Exiting: {e}\") \n",
    "         sys.exit(0) \n",
    "    except Exception as e:\n",
    "        print(f\"Stage 2 training setup or execution failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/32_training_stage2.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "llava"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}