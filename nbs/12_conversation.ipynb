{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Templates\n",
    "\n",
    "> Defines structures and templates for handling conversations in LLaVA-style models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is run from the project root or one level down (e.g., nbs/)\n",
    "# Navigate up to the project root (where settings.ini or .git likely exists)\n",
    "project_root = Path(os.getcwd())\n",
    "# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level\n",
    "if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "project_root_str = str(project_root.resolve())\n",
    "\n",
    "if project_root_str not in sys.path:\n",
    "    print(f\"Adding project root to sys.path: {project_root_str}\")\n",
    "    sys.path.insert(0, project_root_str)\n",
    "else:\n",
    "    print(f\"Project root already in sys.path: {project_root_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root already in sys.path: /workspace/llava\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import dataclasses\n",
    "from enum import auto, Enum\n",
    "from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module defines the conversation structures and templates used for formatting model inputs, similar to the reference LLaVA implementation. It includes the `SeparatorStyle` enum and the `Conversation` dataclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SeparatorStyle(Enum):\n",
    "    \"\"\"Different separator styles for conversations.\"\"\"\n",
    "    # Generic styles\n",
    "    SINGLE = auto()\n",
    "    TWO = auto()\n",
    "    MPT = auto()\n",
    "    PLAIN = auto() # Special case for simple image-caption pairs\n",
    "    LLAMA_2 = auto()\n",
    "    # Specific model styles (add as needed)\n",
    "    VICUNA = auto() # Equivalent to TWO for older versions\n",
    "    CHATML = auto()\n",
    "    CHATGLM = auto()\n",
    "    DOLLY = auto()\n",
    "    RWKV = auto()\n",
    "    PHOENIX = auto()\n",
    "    ROBIN = auto()\n",
    "    FALCON_CHAT = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "An enumeration."
      ],
      "text/plain": [
       "llava.conversation.SeparatorStyle"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SeparatorStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A class that manages prompt generation and conversation history for different models.\"\"\"\n",
    "    # The system prompt message\n",
    "    system: str\n",
    "    # Roles for user and assistant\n",
    "    roles: List[str]\n",
    "    # The conversation history messages. List of lists, where each inner list contains [role, message]. Role is string, message is string or None.\n",
    "    messages: List[List[str]]\n",
    "    # Message offset\n",
    "    offset: int\n",
    "    # Separator style\n",
    "    sep_style: SeparatorStyle\n",
    "    # Separator token(s)\n",
    "    sep: str\n",
    "    # Optional second separator token\n",
    "    sep2: str = None\n",
    "    # Stop criteria (list of stop strings or token IDs)\n",
    "    stop_str: Union[str, List[str]] = None\n",
    "    # Stop token IDs (list of token IDs)\n",
    "    stop_token_ids: List[int] = None\n",
    "\n",
    "    def get_prompt(self) -> str:\n",
    "        \"\"\"Generates the prompt string based on the conversation history and style.\"\"\"\n",
    "        ret = \"\"\n",
    "        # Handle system prompt based on style\n",
    "        if self.sep_style == SeparatorStyle.PLAIN:\n",
    "            # Plain style only concatenates messages with sep\n",
    "            # Often used for image-caption tasks where system prompt is empty\n",
    "            pass # System prompt usually ignored or empty\n",
    "        elif self.system:\n",
    "            ret += self.system\n",
    "            # Add separator after system prompt if needed by the style\n",
    "            if self.sep_style not in [SeparatorStyle.CHATML]: # CHATML includes system in messages\n",
    "                 ret += self.sep\n",
    "\n",
    "        # Format messages\n",
    "        for i, (role, message) in enumerate(self.messages):\n",
    "            if message:\n",
    "                if self.sep_style == SeparatorStyle.PLAIN:\n",
    "                     # Append role (often empty) and message, followed by separator\n",
    "                     ret += role + message + self.sep\n",
    "                elif self.sep_style == SeparatorStyle.CHATML:\n",
    "                     # CHATML format: <|im_start|>role\\nmessage<|im_end|>sep\n",
    "                     ret += role + \"\\n\" + message + self.sep + (\"\\n\" if i < len(self.messages)-1 else \"\")\n",
    "                elif self.sep_style == SeparatorStyle.TWO or self.sep_style == SeparatorStyle.VICUNA:\n",
    "                     # Vicuna V1 format: ROLE: message<sep>\n",
    "                     # System prompt is handled above. 'sep' is space, 'sep2' is EOS.\n",
    "                     ret += role + \": \" + message + self.sep # Add space after message\n",
    "                else:\n",
    "                     # Default/Fallback: ROLE message SEP\n",
    "                     ret += role + message + self.sep \n",
    "            else:\n",
    "                # Handle cases where message is None (e.g., assistant prompt marker)\n",
    "                if self.sep_style == SeparatorStyle.PLAIN:\n",
    "                     ret += role + self.sep # Append role and separator (role might be empty)\n",
    "                elif self.sep_style == SeparatorStyle.CHATML:\n",
    "                     ret += role + \"\\n\" # Append role marker only, like <|im_start|>assistant\\n\n",
    "                elif self.sep_style == SeparatorStyle.TWO or self.sep_style == SeparatorStyle.VICUNA:\n",
    "                     ret += role + \":\" # Append role marker like ASSISTANT:\n",
    "                else:\n",
    "                     ret += role # Append role only\n",
    "                     \n",
    "        # Add final separator if required (e.g., Vicuna v1 EOS)\n",
    "        if self.sep_style == SeparatorStyle.TWO or self.sep_style == SeparatorStyle.VICUNA:\n",
    "            if ret.endswith(self.sep):\n",
    "                 # Remove the trailing space added by the loop if sep2 exists\n",
    "                 ret = ret[:-len(self.sep)] \n",
    "            if self.sep2:\n",
    "                ret += self.sep2\n",
    "        \n",
    "        # Remove trailing separators if style demands\n",
    "        # if self.sep_style != SeparatorStyle.PLAIN and ret.endswith(self.sep):\n",
    "        #      ret = ret[:-len(self.sep)]\n",
    "              \n",
    "        # Handle plain specifically: it should only have one final separator if needed\n",
    "        if self.sep_style == SeparatorStyle.PLAIN:\n",
    "             # Often expects a single newline at the end if sep is newline\n",
    "             # Let's ensure it ends with exactly one sep if sep is defined\n",
    "             if self.sep:\n",
    "                  if ret.endswith(self.sep):\n",
    "                       pass # Already ends with sep\n",
    "                  else:\n",
    "                       ret += self.sep\n",
    "             pass # Keep it simple, usually just image\\ncaption\\n\n",
    "             # Strip extra whitespace at the end for plain\n",
    "             ret = ret.rstrip()\n",
    "             # Re-add the single newline if it was the separator\n",
    "             if self.sep == \"\\n\": ret += \"\\n\"\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def append_message(self, role: str, message: str | None):\n",
    "        \"\"\"Appends a new message to the conversation history.\"\"\"\n",
    "        self.messages.append([role, message])\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"Creates a deep copy of the conversation object.\"\"\"\n",
    "        return Conversation(\n",
    "            system=self.system,\n",
    "            roles=list(self.roles), # Shallow copy roles list\n",
    "            messages=[list(msg) for msg in self.messages], # Deep copy messages\n",
    "            offset=self.offset,\n",
    "            sep_style=self.sep_style,\n",
    "            sep=self.sep,\n",
    "            sep2=self.sep2,\n",
    "            stop_str=self.stop_str,\n",
    "            stop_token_ids=list(self.stop_token_ids) if self.stop_token_ids else None,\n",
    "        )\n",
    "\n",
    "    def dict(self):\n",
    "        \"\"\"Converts the conversation object to a dictionary.\"\"\"\n",
    "        # Use dataclasses.asdict if needed, or manually construct\n",
    "        return {\n",
    "            \"system\": self.system,\n",
    "            \"roles\": self.roles,\n",
    "            \"messages\": self.messages,\n",
    "            \"offset\": self.offset,\n",
    "            \"sep_style\": self.sep_style.name, # Store enum name\n",
    "            \"sep\": self.sep,\n",
    "            \"sep2\": self.sep2,\n",
    "            \"stop_str\": self.stop_str,\n",
    "            \"stop_token_ids\": self.stop_token_ids,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Conversation\n",
       "\n",
       ">      Conversation (system:str, roles:List[str], messages:List[List[str]],\n",
       ">                    offset:int, sep_style:__main__.SeparatorStyle, sep:str,\n",
       ">                    sep2:str=None, stop_str:Union[str,List[str]]=None,\n",
       ">                    stop_token_ids:List[int]=None)\n",
       "\n",
       "*A class that manages prompt generation and conversation history for different models.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Conversation\n",
       "\n",
       ">      Conversation (system:str, roles:List[str], messages:List[List[str]],\n",
       ">                    offset:int, sep_style:__main__.SeparatorStyle, sep:str,\n",
       ">                    sep2:str=None, stop_str:Union[str,List[str]]=None,\n",
       ">                    stop_token_ids:List[int]=None)\n",
       "\n",
       "*A class that manages prompt generation and conversation history for different models.*"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define specific conversation templates for different models/stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Template for Stage 1 Pretraining ('plain' style)\n",
    "conv_llava_plain = Conversation(\n",
    "    system=\"\", # No system prompt\n",
    "    roles=(\"\", \"\"), # Roles are often ignored, but placeholders needed\n",
    "    messages=[], # History is built dynamically\n",
    "    offset=0,\n",
    "    sep_style=SeparatorStyle.PLAIN,\n",
    "    sep=\"\\n\", # Separator is a newline\n",
    "    stop_str=None,\n",
    "    stop_token_ids=None\n",
    ")\n",
    "\n",
    "# Template for Vicuna v1 (Instruction Tuning - Stage 2)\n",
    "conv_vicuna_v1 = Conversation(\n",
    "    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "           \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n",
    "    roles=(\"USER\", \"ASSISTANT\"),\n",
    "    messages=[],\n",
    "    offset=0,\n",
    "    sep_style=SeparatorStyle.TWO, # Uses two separators\n",
    "    sep=\" \", # Space separator between turns\n",
    "    sep2=\"</s>\", # EOS token as the second separator (end of conversation)\n",
    "    stop_str=\"</s>\",\n",
    "    # stop_token_ids=[2], # Example, assuming tokenizer.eos_token_id is 2\n",
    "    # Note: stop_token_ids should be set based on the actual tokenizer used later\n",
    "    stop_token_ids=None\n",
    ")\n",
    "\n",
    "# Add other templates as needed (e.g., 'v0', 'llama_2', 'chatml') based on LLaVA reference\n",
    "# ...\n",
    "\n",
    "# --- Template Dictionary --- \n",
    "conv_templates = {\n",
    "    \"plain\": conv_llava_plain,\n",
    "    \"v1\": conv_vicuna_v1,\n",
    "    # Add other templates here\n",
    "    # \"v0\": conv_vicuna_v0, \n",
    "    # \"vicuna_v1\": conv_vicuna_v1, # Alias\n",
    "}\n",
    "\n",
    "# --- Default Conversation --- \n",
    "# Set a default conversation template (can be overridden)\n",
    "default_conversation = conv_vicuna_v1 \n",
    "\n",
    "# --- Functions to get conversation templates ---\n",
    "def get_conv_template(name: str) -> Conversation:\n",
    "    \"\"\"Gets a conversation template by name.\n",
    "    \n",
    "    Args:\n",
    "        name: The name of the conversation template.\n",
    "        \n",
    "    Returns:\n",
    "        A deep copy of the requested conversation template.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the template name is not found.\n",
    "    \"\"\"\n",
    "    if name not in conv_templates:\n",
    "        raise ValueError(f\"Unknown conversation template: {name}. Available templates: {list(conv_templates.keys())}\")\n",
    "    return conv_templates[name].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Plain Template Test ---\n",
      "Formatted Prompt:\n",
      "<image>\n",
      "A red block.\n",
      "\n",
      "--- Vicuna V1 Template Test ---\n",
      "Formatted Prompt:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\n",
      "Describe it. ASSISTANT: It is red.</s>USER: What shape? ASSISTANT: It is square.</s>ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "#| test\n",
    "# --- Test Plain Template --- \n",
    "plain_conv = get_conv_template(\"plain\")\n",
    "plain_conv.append_message(\"\", \"<image>\") # Role is empty for plain image part\n",
    "plain_conv.append_message(\"\", \"A red block.\") # Role is empty for plain caption part\n",
    "plain_prompt = plain_conv.get_prompt()\n",
    "\n",
    "print(\"--- Plain Template Test ---\")\n",
    "print(\"Formatted Prompt:\")\n",
    "print(plain_prompt)\n",
    "assert plain_prompt == \"<image>\\nA red block.\\n\", f\"Plain template mismatch: '{plain_prompt}'\"\n",
    "\n",
    "# --- Test Vicuna V1 Template --- \n",
    "v1_conv = get_conv_template(\"v1\")\n",
    "v1_conv.append_message(v1_conv.roles[0], \"<image>\\nDescribe it.\") # User turn\n",
    "v1_conv.append_message(v1_conv.roles[1], \"It is red.\") # Assistant turn\n",
    "v1_conv.append_message(v1_conv.roles[0], \"What shape?\") # User turn\n",
    "v1_conv.append_message(v1_conv.roles[1], \"It is square.\") # Assistant turn\n",
    "v1_conv.append_message(v1_conv.roles[1], None) # Add prompt for next assistant turn\n",
    "v1_prompt = v1_conv.get_prompt()\n",
    "\n",
    "print(\"\\n--- Vicuna V1 Template Test ---\")\n",
    "print(\"Formatted Prompt:\")\n",
    "print(v1_prompt)\n",
    "\n",
    "# Expected output construction\n",
    "expected_v1 = (\n",
    "    \"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\" + v1_conv.sep +\n",
    "    \"USER: <image>\\nDescribe it.\" + v1_conv.sep + \n",
    "    \"ASSISTANT: It is red.\" + v1_conv.sep2 + # EOS after assistant turn\n",
    "    \"USER: What shape?\" + v1_conv.sep + \n",
    "    \"ASSISTANT: It is square.\" + v1_conv.sep2 + # EOS after assistant turn\n",
    "    \"ASSISTANT:\" # Prompt for next assistant turn\n",
    ")\n",
    "\n",
    "assert v1_prompt.strip() == expected_v1.strip(), f\"Vicuna v1 template mismatch. Expected:\\n'{expected_v1}'\\nGot:\\n'{v1_prompt}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "path": "nbs/12_conversation.ipynb",
   "skip_save": true
  },
  "nbdev": {
   "doc_path": "_docs",
   "lib_path": "llava"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}