"""Sets up and runs the first stage of LLaVA training, focusing on pre-training the projector module."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/31_training_stage1.ipynb.

# %% auto 0
__all__ = ['project_root', 'project_root_str', 'llava_stage1_splitter', 'get_stage1_learner', 'train_stage1']

# %% ../../nbs/31_training_stage1.ipynb 3
import sys
from pathlib import Path
import os
import gc # For memory cleanup
import argparse # For command-line execution


os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# Assumes the notebook is run from the project root or one level down (e.g., nbs/)
# Navigate up to the project root (where settings.ini or .git likely exists)
project_root = Path(os.getcwd())
# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level
if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():
    project_root = project_root.parent

project_root_str = str(project_root.resolve())

if project_root_str not in sys.path:
    print(f"Adding project root to sys.path: {project_root_str}")
    sys.path.insert(0, project_root_str)
else:
    print(f"Project root already in sys.path: {project_root_str}")

# %% ../../nbs/31_training_stage1.ipynb 4
import torch
import warnings

from fastai.learner import Learner
from fastai.vision.all import * # For splitter

from fastai.callback.wandb import WandbCallback
from fastai.callback.schedule import fit_one_cycle
from fastai.callback.training import GradientAccumulation # Import GradientAccumulation
from fastai.callback.fp16 import MixedPrecision # Import MixedPrecision
from fastai.data.core import DataLoaders
from functools import partial
import wandb # Import wandb directly for cleanup
import json # For dummy data creation
import PIL.Image # For dummy data creation

from ..utils import load_config, init_wandb
from ..data.loading import get_stage1_dataloaders
from ..model.baseline import BaselineLLaVAModel
from .core import LLaVALoss, LLaVAMixedPrecision, extract_loss_from_output, SafeGradientAccumulation

# %% ../../nbs/31_training_stage1.ipynb 7
def llava_stage1_splitter(model: BaselineLLaVAModel):
    """Splits the `BaselineLLaVAModel` parameters for Stage 1 training.
    
    Only the parameters of the `projector` module are marked as trainable for the optimizer.
    Vision tower is frozen. LLM's LoRA adapters (if any) remain trainable but won't be
    passed to the optimizer in Stage 1. Base LLM weights are frozen (by PEFT or explicitly).
    
    Args:
        model: An instance of `BaselineLLaVAModel`.
        
    Returns:
        A list containing a single parameter group for the projector.
    """
    if not hasattr(model, 'projector') or model.projector is None:
        raise AttributeError("Model does not have a 'projector' attribute or it is None.")
        
    print("Applying Stage 1 splitter: Ensuring only projector parameters are passed to optimizer.")
    
    trainable_params = []
    # Ensure projector parameters are trainable
    if hasattr(model, 'projector') and model.projector is not None:
        print("  - Setting projector parameters to require_grad=True.")
        for p in model.projector.parameters():
            p.requires_grad = True
        trainable_params.extend(list(model.projector.parameters()))
    
    # Ensure vision tower is frozen
    if hasattr(model, 'vision_tower') and model.vision_tower is not None:
        print("  - Setting vision_tower parameters to require_grad=False.")
        for p in model.vision_tower.parameters():
            p.requires_grad = False
            
    # For the language model in Stage 1:
    # - If PEFT/LoRA is used, get_peft_model has already frozen base weights and made adapters trainable.
    #   We do NOT want to turn off grads for LoRA adapters here.
    # - If no PEFT/LoRA, the base LLM should be frozen. This is handled in model.__init__
    #   by the `self.language_model.requires_grad_(False)` call if not QLoRA/LoRA.
    # So, the splitter's main job for Stage 1 regarding LLM is *not* to modify its requires_grad state
    # but to ensure only projector params are given to the optimizer.
    # The `model.__init__` handles initial freezing.

    if not trainable_params:
         raise ValueError("Splitter function resulted in no trainable parameters for Stage 1 (projector).")
    
    print(f"  - Stage 1 Splitter will provide {len(trainable_params)} projector parameters to the optimizer.")
    # The optimizer will only receive these parameters.
    return trainable_params # This was already correct, the issue was modifying LLM params.

# %% ../../nbs/31_training_stage1.ipynb 11
def get_stage1_learner(config: dict) -> Learner:
    """Configures and returns a fastai Learner for Stage 1 projector pre-training.
       Includes optimization callbacks (GradientAccumulation, MixedPrecision) based on config.

    Args:
        config: The main configuration dictionary.

    Returns:
        A configured fastai Learner instance for Stage 1.

    Raises:
        RuntimeError: If DataLoaders or Model instantiation fails.
        FileNotFoundError: If specified data paths in config are incorrect.
        AttributeError: If the model is missing expected components (e.g., projector).
    """
    print("--- Setting up Stage 1 Learner ---")
    
    # 1. Load DataLoaders
    print("Loading Stage 1 DataLoaders...")
    try:
        dls = get_stage1_dataloaders(config)
    except (FileNotFoundError, Exception) as e:
        print(f"Error loading DataLoaders: {e}")
        raise RuntimeError("Failed to create Stage 1 DataLoaders. Check config paths and data availability.") from e
    if not dls:
        raise RuntimeError("Stage 1 DataLoaders object is None.")
    print(f"DataLoaders loaded. Train samples: {len(dls.train_ds)}, Valid samples: {len(dls.valid_ds)}")

    # 2. Instantiate Model
    print("Instantiating BaselineLLaVAModel...")
    try:
        model = BaselineLLaVAModel(config)
        # Ensure model components loaded successfully
        if model.vision_tower is None or model.language_model is None or model.projector is None:
             raise RuntimeError("BaselineLLaVAModel initialization failed: one or more components are None.")
        print("Model instantiated successfully.")
    except Exception as e:
        print(f"Error instantiating BaselineLLaVAModel: {e}")
        raise RuntimeError("Failed to instantiate baseline model.") from e

    # 3. Define Loss Function
    # loss_func = LLaVALoss()
    loss_func = extract_loss_from_output # Use the extractor function
    print(f"Loss function: {loss_func.__name__}")
    print(f"Loss function: {type(loss_func).__name__}")

    # 4. Define Optimizer
    # AdamW is generally preferred for transformer models
    lr = config.get('training', {}).get('learning_rate_stage1', 1e-4)
    wd = config.get('training', {}).get('weight_decay', 0.0)
    
    
    opt_func = partial(Adam, lr=lr, wd=wd, eps=1e-8) # Added eps for numerical stability
    print(f"Optimizer: Adam (lr={lr}, wd={wd})")

    # opt_func = AdamW # Pass the optimizer class directly
    # print(f"Optimizer: AdamW (lr will be set by Learner/fit_one_cycle, default wd={wd})") # 
    
    # 5. Define Splitter
    splitter = llava_stage1_splitter
    print(f"Parameter splitter: {splitter.__name__}")

    # 6. Define Callbacks
    cbs = []
    # Weights & Biases Logging
    if config.get('logging', {}).get('wandb', {}).get('enabled', False):
        # Initialize W&B run here before creating WandbCallback
        project_name = config.get('logging', {}).get('wandb', {}).get('project', 'llava-adaptive-patching')
        entity = config.get('logging', {}).get('wandb', {}).get('entity') # Optional
        run_name_prefix = config.get('logging', {}).get('wandb', {}).get('run_name_prefix', 'stage1')
        
        # Create a unique run name (init_wandb will create one if not passed, but we can pre-define)
        run_name = f"{run_name_prefix}_{Path(config['paths']['stage1_projector_weights']).stem}_{wandb.util.generate_id()}"
        
        # Init W&B Run
        init_wandb(config, job_type="stage1-training", run_name=run_name)
        
        # Add W&B Callback
        cbs.append(WandbCallback(log_preds=False, log_model=False)) # Don't log model via W&B callback, use SaveModelCallback
        print("Added WandbCallback.")
        
    # Model Saving (Only Projector Weights)
    output_dir = Path(config['paths']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True) # Ensure output directory exists
    projector_weights_fname = Path(config['paths']['stage1_projector_weights']).stem # Get filename without extension
    save_cb = SaveModelCallback(
        monitor='valid_loss', 
        min_delta=0.001, # Avoid saving too often for tiny improvements
        fname=projector_weights_fname, # Saves as f'{fname}.pth' in learner.path/models/
        every_epoch=False, # Save only best based on monitor
        with_opt=False, # Don't save optimizer state for projector
        reset_on_fit=True # Ensures it checks from the start of training
    )
    cbs.append(save_cb)
    print(f"Added SaveModelCallback (saves best projector weights based on valid_loss to {output_dir/'models'/f'{projector_weights_fname}.pth'})")

    # --- Add Optimization Callbacks (Step 3.3 Implementation) --- 
    grad_accum_steps = config.get('training', {}).get('gradient_accumulation_steps', 1)
    if grad_accum_steps > 1:
        cbs.append(SafeGradientAccumulation(grad_accum_steps))
        print(f"Added SafeGradientAccumulation callback with {grad_accum_steps} steps.")
        
        # cbs.append(GradientAccumulation(grad_accum_steps))
        # print(f"Added GradientAccumulation callback with {grad_accum_steps} steps.")
    
    use_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)
    qlora_enabled = config.get('model', {}).get('quantization', {}).get('load_in_4bit', False)
    if use_mixed_precision and not qlora_enabled: # Only add if explicitly enabled AND QLoRA is OFF
        cbs.append(LLaVAMixedPrecision()) # Or your current custom MixedPrecision
        print("Added MixedPrecision callback (QLoRA is disabled).")
    elif use_mixed_precision and qlora_enabled:
        print("QLoRA is enabled, MixedPrecision callback will be skipped as QLoRA handles its own precision.")
    
    # if use_mixed_precision:
    #     cbs.append(LLaVAMixedPrecision())
    #     print("Added MixedPrecision callback.")
    
    # --------------------------------------------------------------
    
    # 7. Create Learner
    try:
        learner = Learner(
            dls=dls,
            model=model,
            loss_func=loss_func,
            opt_func=opt_func,
            splitter=splitter,
            cbs=cbs,
            path=output_dir, # Set Learner path for saving models
            train_bn=False, # Avoid issues with frozen batch norm layers in LLM/Vision Tower
            # wd=wd
        )
            
    except Exception as e:
        print(f"Error creating Learner: {e}")
        # Clean up wandb run if initialized
        if wandb.run is not None:
            wandb.finish(exit_code=1)
            print("Finished W&B run due to error during Learner creation.")
        raise RuntimeError("Failed to create the Learner object.") from e
    
    print("--- Stage 1 Learner Setup Complete ---")
    return learner

# %% ../../nbs/31_training_stage1.ipynb 17
def train_stage1(config_path: str | Path):
    """Loads config, sets up Stage 1 learner, and runs training.
    
    Args:
        config_path: Path to the YAML configuration file.
    """
    print(f"--- Starting Stage 1 Training --- ")
    print(f"Loading configuration from: {config_path}")
    config = load_config(config_path)
    
    learner = None # Initialize learner to None for finally block
    try:
        # --- Get Learner (including optimization callbacks) --- 
        learner = get_stage1_learner(config)
        
        # --- Start Training --- 
        epochs = config.get('training', {}).get('num_epochs_stage1', 1)
        lr = config.get('training', {}).get('learning_rate_stage1', 1e-4)
        print(f"Starting training for {epochs} epochs with max_lr={lr}...")
        
        # Use fit_one_cycle (common practice)
        # You could also use learner.fit(epochs, lr=lr) or other fine-tuning methods
        learner.fit(epochs, lr=lr)
        
        print("Training finished.")
        
        # --- Save final projector weights explicitly --- 
        # SaveModelCallback saves the *best* model during training.
        # It might be useful to save the *final* projector state as well.
        output_dir = Path(config['paths']['output_dir'])
        final_projector_filename = Path(config['paths']['stage1_projector_weights']).stem + "_final.pth"
        final_save_path = output_dir / 'models' / final_projector_filename 
        print(f"Saving final projector state to: {final_save_path}")
        # Save only the projector's state_dict
        torch.save(learner.model.projector.state_dict(), final_save_path)
        print("Final projector weights saved.")

    except Exception as e:
        print(f"An error occurred during Stage 1 training: {e}")
        import traceback
        traceback.print_exc()
        # Potentially re-raise or handle cleanup
        raise e
    finally:
        # Clean up memory
        if learner is not None and hasattr(learner, 'model') and learner.model is not None:
            if hasattr(learner.model, 'vision_tower') and learner.model.vision_tower is not None:
                learner.model.vision_tower.to('cpu')
            if hasattr(learner.model, 'language_model') and learner.model.language_model is not None:
                learner.model.language_model.to('cpu')
            if hasattr(learner.model, 'projector') and learner.model.projector is not None:
                learner.model.projector.to('cpu')
            del learner.model
            learner.destroy() # Release learner resources
            del learner
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print("Cleaned up learner and model memory.")
            
        # Ensure W&B run is finished if it was started
        if wandb.run is not None:
            try:
                if wandb.run.id: # Check if run is still active
                    wandb.finish()
                    print("Finished W&B run.")
            except Exception as e:
                 print(f"Error finishing W&B run: {e}")
            
    print(f"--- Stage 1 Training Complete --- ")

# %% ../../nbs/31_training_stage1.ipynb 19
# Add command-line execution block
if __name__ == "__main__" and "get_ipython" not in locals():
    parser = argparse.ArgumentParser(description="Run LLaVA Stage 1 Training")
    parser.add_argument("--config", type=str, default="../configs/config.yaml", 
                        help="Path to the configuration YAML file.")
    args = parser.parse_args()
    
    config_path = Path(args.config)
    if not config_path.is_file():
        print(f"Error: Config file not found at {config_path}")
        sys.exit(1)
        
    # Ensure the config path is absolute or relative to the script's execution dir
    # If running from the project root, '../configs/config.yaml' works.
    # If running from nbs/, 'configs/config.yaml' might be needed depending on cwd.
    # Using absolute paths or paths relative to a known root is safer.
    # Assuming execution from project root or script location within project:
    if not config_path.exists():
         # Try resolving relative to the script file itself if it doesn't exist relative to CWD
         script_dir = Path(__file__).parent.resolve()
         config_path = (script_dir / args.config).resolve()
         if not config_path.exists():
              print(f"Error: Config file not found at specified path or relative to script: {args.config}")
              sys.exit(1)

    try:
        train_stage1(config_path=config_path)
    except Exception as e:
        print(f"Stage 1 training failed: {e}")
        sys.exit(1)
