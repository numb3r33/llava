"""Sets up and runs the first stage of LLaVA training, focusing on pre-training the projector module."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/31_training_stage1.ipynb.

# %% auto 0
__all__ = ['project_root', 'project_root_str', 'llava_stage1_splitter', 'get_stage1_learner', 'train_stage1']

# %% ../../nbs/31_training_stage1.ipynb 3
import sys
from pathlib import Path
import os
import gc # For memory cleanup
import argparse # For command-line execution

# Assumes the notebook is run from the project root or one level down (e.g., nbs/)
# Navigate up to the project root (where settings.ini or .git likely exists)
project_root = Path(os.getcwd())
# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level
if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():
    project_root = project_root.parent

project_root_str = str(project_root.resolve())

if project_root_str not in sys.path:
    print(f"Adding project root to sys.path: {project_root_str}")
    sys.path.insert(0, project_root_str)
else:
    print(f"Project root already in sys.path: {project_root_str}")

# %% ../../nbs/31_training_stage1.ipynb 4
import torch
from fastai.learner import Learner
from fastai.optimizer import AdamW
from fastai.callback.wandb import WandbCallback
from fastai.callback.schedule import fit_one_cycle
from fastai.callback.save import SaveModelCallback
from fastai.callback.training import GradientAccumulation # Import GradientAccumulation
from fastai.callback.fp16 import MixedPrecision # Import MixedPrecision
from fastai.vision.all import params # For splitter
from fastai.data.core import DataLoaders
from functools import partial
import wandb # Import wandb directly for cleanup
import json # For dummy data creation
import PIL.Image # For dummy data creation

from ..utils import load_config, init_wandb
from ..data.loading import get_stage1_dataloaders
from ..model.baseline import BaselineLLaVAModel
from .core import LLaVALoss

# %% ../../nbs/31_training_stage1.ipynb 7
def llava_stage1_splitter(model: BaselineLLaVAModel):
    """Splits the `BaselineLLaVAModel` parameters for Stage 1 training.
    
    Only the parameters of the `projector` module are marked as trainable.
    The `vision_tower` and `language_model` parameters will remain frozen.
    
    Args:
        model: An instance of `BaselineLLaVAModel`.
        
    Returns:
        A list containing a single parameter group for the projector.
    """
    if not hasattr(model, 'projector') or model.projector is None:
        raise AttributeError("Model does not have a 'projector' attribute or it is None.")
        
    print("Applying Stage 1 splitter: Training only the projector.")
    # Fastai's `params` function selects parameters from the given module(s)
    # Only parameters returned by the splitter are trained.
    trainable_params = list(model.projector.parameters())
    
    # Verify that projector parameters require grad
    # They should by default unless explicitly frozen, but good to check
    for p in trainable_params:
         p.requires_grad = True # Ensure they are trainable
            
    # Ensure other parts are frozen (should be done during model init, but double-check)
    if hasattr(model, 'vision_tower') and model.vision_tower is not None:
        for p in model.vision_tower.parameters():
            p.requires_grad = False
    if hasattr(model, 'language_model') and model.language_model is not None:
        # Note: If PEFT is somehow applied here (it shouldn't be for stage 1),
        # this would wrongly freeze LoRA adapters. This assumes base LLM is frozen.
        for p in model.language_model.parameters():
            p.requires_grad = False
            
    # Return a list containing one group of trainable parameters (the projector's)
    return [trainable_params]

# %% ../../nbs/31_training_stage1.ipynb 11
def get_stage1_learner(config: dict) -> Learner:
    """Configures and returns a fastai Learner for Stage 1 projector pre-training.
       Includes optimization callbacks (GradientAccumulation, MixedPrecision) based on config.

    Args:
        config: The main configuration dictionary.

    Returns:
        A configured fastai Learner instance for Stage 1.

    Raises:
        RuntimeError: If DataLoaders or Model instantiation fails.
        FileNotFoundError: If specified data paths in config are incorrect.
        AttributeError: If the model is missing expected components (e.g., projector).
    """
    print("--- Setting up Stage 1 Learner ---")
    
    # 1. Load DataLoaders
    print("Loading Stage 1 DataLoaders...")
    try:
        dls = get_stage1_dataloaders(config)
    except (FileNotFoundError, Exception) as e:
        print(f"Error loading DataLoaders: {e}")
        raise RuntimeError("Failed to create Stage 1 DataLoaders. Check config paths and data availability.") from e
    if not dls:
        raise RuntimeError("Stage 1 DataLoaders object is None.")
    print(f"DataLoaders loaded. Train samples: {len(dls.train_ds)}, Valid samples: {len(dls.valid_ds)}")

    # 2. Instantiate Model
    print("Instantiating BaselineLLaVAModel...")
    try:
        model = BaselineLLaVAModel(config)
        # Ensure model components loaded successfully
        if model.vision_tower is None or model.language_model is None or model.projector is None:
             raise RuntimeError("BaselineLLaVAModel initialization failed: one or more components are None.")
        print("Model instantiated successfully.")
    except Exception as e:
        print(f"Error instantiating BaselineLLaVAModel: {e}")
        raise RuntimeError("Failed to instantiate baseline model.") from e

    # 3. Define Loss Function
    loss_func = LLaVALoss()
    print(f"Loss function: {type(loss_func).__name__}")

    # 4. Define Optimizer
    # AdamW is generally preferred for transformer models
    lr = config.get('training', {}).get('learning_rate_stage1', 1e-4)
    wd = config.get('training', {}).get('weight_decay', 0.0)
    opt_func = partial(AdamW, lr=lr, wd=wd, eps=1e-8) # Added eps for numerical stability
    print(f"Optimizer: AdamW (lr={lr}, wd={wd})")

    # 5. Define Splitter
    splitter = llava_stage1_splitter
    print(f"Parameter splitter: {splitter.__name__}")

    # 6. Define Callbacks
    cbs = []
    # Weights & Biases Logging
    if config.get('logging', {}).get('wandb', {}).get('enabled', False):
        # Initialize W&B run here before creating WandbCallback
        project_name = config.get('logging', {}).get('wandb', {}).get('project', 'llava-adaptive-patching')
        entity = config.get('logging', {}).get('wandb', {}).get('entity') # Optional
        run_name_prefix = config.get('logging', {}).get('wandb', {}).get('run_name_prefix', 'stage1')
        
        # Create a unique run name (init_wandb will create one if not passed, but we can pre-define)
        run_name = f"{run_name_prefix}_{Path(config['paths']['stage1_projector_weights']).stem}_{wandb.util.generate_id()}"
        
        # Init W&B Run
        init_wandb(config, job_type="stage1-training", run_name=run_name)
        
        # Add W&B Callback
        cbs.append(WandbCallback(log_preds=False, log_model=False)) # Don't log model via W&B callback, use SaveModelCallback
        print("Added WandbCallback.")
        
    # Model Saving (Only Projector Weights)
    output_dir = Path(config['paths']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True) # Ensure output directory exists
    projector_weights_fname = Path(config['paths']['stage1_projector_weights']).stem # Get filename without extension
    save_cb = SaveModelCallback(
        monitor='valid_loss', 
        min_delta=0.001, # Avoid saving too often for tiny improvements
        fname=projector_weights_fname, # Saves as f'{fname}.pth' in learner.path/models/
        every_epoch=False, # Save only best based on monitor
        with_opt=False, # Don't save optimizer state for projector
        reset_on_fit=True # Ensures it checks from the start of training
    )
    cbs.append(save_cb)
    print(f"Added SaveModelCallback (saves best projector weights based on valid_loss to {output_dir/'models'/f'{projector_weights_fname}.pth'})")

    # --- Add Optimization Callbacks (Step 3.3 Implementation) --- 
    grad_accum_steps = config.get('training', {}).get('gradient_accumulation_steps', 1)
    if grad_accum_steps > 1:
        cbs.append(GradientAccumulation(grad_accum_steps))
        print(f"Added GradientAccumulation callback with {grad_accum_steps} steps.")
    
    use_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)
    if use_mixed_precision:
        cbs.append(MixedPrecision())
        print("Added MixedPrecision callback.")
    # --------------------------------------------------------------
    
    # 7. Create Learner
    try:
        learner = Learner(
            dls=dls,
            model=model,
            loss_func=loss_func,
            opt_func=opt_func,
            splitter=splitter,
            cbs=cbs,
            path=output_dir, # Set Learner path for saving models
            train_bn=False # Avoid issues with frozen batch norm layers in LLM/Vision Tower
        )
            
    except Exception as e:
        print(f"Error creating Learner: {e}")
        # Clean up wandb run if initialized
        if wandb.run is not None:
            wandb.finish(exit_code=1)
            print("Finished W&B run due to error during Learner creation.")
        raise RuntimeError("Failed to create the Learner object.") from e
    
    print("--- Stage 1 Learner Setup Complete ---")
    return learner

# %% ../../nbs/31_training_stage1.ipynb 17
def train_stage1(config_path: str | Path):
    """Loads config, sets up Stage 1 learner, and runs training.
    
    Args:
        config_path: Path to the YAML configuration file.
    """
    print(f"--- Starting Stage 1 Training --- ")
    print(f"Loading configuration from: {config_path}")
    config = load_config(config_path)
    
    learner = None # Initialize learner to None for finally block
    try:
        # --- Get Learner (including optimization callbacks) --- 
        learner = get_stage1_learner(config)
        
        # --- Start Training --- 
        epochs = config.get('training', {}).get('num_epochs_stage1', 1)
        lr = config.get('training', {}).get('learning_rate_stage1', 1e-4)
        print(f"Starting training for {epochs} epochs with max_lr={lr}...")
        
        # Use fit_one_cycle (common practice)
        # You could also use learner.fit(epochs, lr=lr) or other fine-tuning methods
        learner.fit_one_cycle(epochs, lr_max=lr)
        
        print("Training finished.")
        
        # --- Save final projector weights explicitly --- 
        # SaveModelCallback saves the *best* model during training.
        # It might be useful to save the *final* projector state as well.
        output_dir = Path(config['paths']['output_dir'])
        final_projector_filename = Path(config['paths']['stage1_projector_weights']).stem + "_final.pth"
        final_save_path = output_dir / 'models' / final_projector_filename 
        print(f"Saving final projector state to: {final_save_path}")
        # Save only the projector's state_dict
        torch.save(learner.model.projector.state_dict(), final_save_path)
        print("Final projector weights saved.")

    except Exception as e:
        print(f"An error occurred during Stage 1 training: {e}")
        import traceback
        traceback.print_exc()
        # Potentially re-raise or handle cleanup
        raise e
    finally:
        # Clean up memory
        if learner is not None and hasattr(learner, 'model') and learner.model is not None:
            if hasattr(learner.model, 'vision_tower') and learner.model.vision_tower is not None:
                learner.model.vision_tower.to('cpu')
            if hasattr(learner.model, 'language_model') and learner.model.language_model is not None:
                learner.model.language_model.to('cpu')
            if hasattr(learner.model, 'projector') and learner.model.projector is not None:
                learner.model.projector.to('cpu')
            del learner.model
            learner.destroy() # Release learner resources
            del learner
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print("Cleaned up learner and model memory.")
            
        # Ensure W&B run is finished if it was started
        if wandb.run is not None:
            try:
                if wandb.run.id: # Check if run is still active
                    wandb.finish()
                    print("Finished W&B run.")
            except Exception as e:
                 print(f"Error finishing W&B run: {e}")
            
    print(f"--- Stage 1 Training Complete --- ")

# %% ../../nbs/31_training_stage1.ipynb 19
# Add command-line execution block
if __name__ == "__main__" and "get_ipython" not in locals():
    parser = argparse.ArgumentParser(description="Run LLaVA Stage 1 Training")
    parser.add_argument("--config", type=str, default="../configs/config.yaml", 
                        help="Path to the configuration YAML file.")
    args = parser.parse_args()
    
    config_path = Path(args.config)
    if not config_path.is_file():
        print(f"Error: Config file not found at {config_path}")
        sys.exit(1)
        
    # Ensure the config path is absolute or relative to the script's execution dir
    # If running from the project root, '../configs/config.yaml' works.
    # If running from nbs/, 'configs/config.yaml' might be needed depending on cwd.
    # Using absolute paths or paths relative to a known root is safer.
    # Assuming execution from project root or script location within project:
    if not config_path.exists():
         # Try resolving relative to the script file itself if it doesn't exist relative to CWD
         script_dir = Path(__file__).parent.resolve()
         config_path = (script_dir / args.config).resolve()
         if not config_path.exists():
              print(f"Error: Config file not found at specified path or relative to script: {args.config}")
              sys.exit(1)

    try:
        train_stage1(config_path=config_path)
    except Exception as e:
        print(f"Stage 1 training failed: {e}")
        sys.exit(1)
