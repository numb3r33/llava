"""Core components for the training pipeline, including loss functions and potentially shared callbacks or utilities."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/30_training_core.ipynb.

# %% auto 0
__all__ = ['project_root', 'project_root_str', 'LLaVALoss']

# %% ../../nbs/30_training_core.ipynb 3
import sys
from pathlib import Path
import os

# Assumes the notebook is run from the project root or one level down (e.g., nbs/)
# Navigate up to the project root (where settings.ini or .git likely exists)
project_root = Path(os.getcwd())
# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level
if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():
    project_root = project_root.parent

project_root_str = str(project_root.resolve())

if project_root_str not in sys.path:
    print(f"Adding project root to sys.path: {project_root_str}")
    sys.path.insert(0, project_root_str)
else:
    print(f"Project root already in sys.path: {project_root_str}")

# %% ../../nbs/30_training_core.ipynb 4
import torch
import torch.nn as nn
import torch.nn.functional as F
from fastai.callback.wandb import WandbCallback
from fastai.learner import Learner
from fastai.data.core import DataLoaders
from ..data.preprocessing import IGNORE_INDEX # Import ignore index constant

# %% ../../nbs/30_training_core.ipynb 7
class LLaVALoss(nn.Module):
    """ Custom CrossEntropyLoss that ignores indices where labels are IGNORE_INDEX (default -100).
    
    This loss function handles the standard autoregressive language modeling loss
    by shifting the logits and labels, ensuring the model predicts the next token.
    It specifically ignores tokens marked with `ignore_index` in the labels tensor,
    which is crucial for masking out prompt tokens, padding tokens, and image tokens
    during LLaVA training.
    """
    def __init__(self, ignore_index=IGNORE_INDEX):
        """ Initializes the loss function.
        
        Args:
            ignore_index (int): The label index to be ignored during loss calculation.
                                Defaults to the value imported from llava.data.preprocessing.
        """
        super().__init__()
        self.ignore_index = ignore_index
        # Initialize the standard CrossEntropyLoss with the specified ignore_index
        self.loss_fct = nn.CrossEntropyLoss(ignore_index=self.ignore_index)
        print(f"LLaVALoss initialized, ignoring index: {self.ignore_index}")

    def forward(self, output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """ Calculates the cross-entropy loss, ignoring specified indices.
        
        Args:
            output (torch.Tensor): The model's output logits.
                                   Shape: (batch_size, sequence_length, vocab_size).
            target (torch.Tensor): The target labels (token IDs).
                                   Shape: (batch_size, sequence_length).
                                   Should contain `ignore_index` for tokens to be ignored.
                                   
        Returns:
            torch.Tensor: A scalar tensor representing the calculated loss.
        """
        # --- Shift logits and labels for next token prediction --- 
        # Logits are shifted left (we predict the token *after* the current one)
        # output shape: (batch_size, sequence_length, vocab_size)
        shift_logits = output[..., :-1, :].contiguous()
        # Labels are shifted left (the target for the prediction at time t is the token at t+1)
        # target shape: (batch_size, sequence_length)
        shift_labels = target[..., 1:].contiguous()

        # --- Flatten the tokens for CrossEntropyLoss --- 
        # The CrossEntropyLoss expects input shape (N, C) where N is the number of samples
        # and C is the number of classes (vocab_size). The target shape should be (N).
        # Shift_logits flattened shape: (batch_size * (sequence_length - 1), vocab_size)
        # Shift_labels flattened shape: (batch_size * (sequence_length - 1))
        vocab_size = shift_logits.size(-1)
        loss = self.loss_fct(shift_logits.view(-1, vocab_size), 
                             shift_labels.view(-1))
        
        # Ensure loss is a scalar
        if loss.dim() > 0:
             # This might happen if the batch size or sequence length becomes 0 or 1 after masking/shifting.
             # Although CrossEntropyLoss typically returns a scalar, handle defensively.
             loss = loss.mean() 

        return loss
