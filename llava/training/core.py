"""Core components for the training pipeline, including loss functions and potentially shared callbacks or utilities."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/30_training_core.ipynb.

# %% auto 0
__all__ = ['project_root', 'project_root_str', 'extract_loss_from_output', 'LLaVALoss', 'SafeGradientAccumulation',
           'LLaVAMixedPrecision']

# %% ../../nbs/30_training_core.ipynb 3
import sys
from pathlib import Path
import os
import warnings

# Assumes the notebook is run from the project root or one level down (e.g., nbs/)
# Navigate up to the project root (where settings.ini or .git likely exists)
project_root = Path(os.getcwd())
# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level
if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():
    project_root = project_root.parent

project_root_str = str(project_root.resolve())

if project_root_str not in sys.path:
    print(f"Adding project root to sys.path: {project_root_str}")
    sys.path.insert(0, project_root_str)
else:
    print(f"Project root already in sys.path: {project_root_str}")

# %% ../../nbs/30_training_core.ipynb 4
import torch
import torch.nn as nn
import torch.nn.functional as F
from fastai.callback.wandb import WandbCallback
from fastai.learner import Learner
from fastai.data.core import DataLoaders
from ..data.preprocessing import IGNORE_INDEX # Import ignore index constant


from fastai.callback.fp16 import MixedPrecision
from fastai.callback.training import GradientAccumulation, find_bs
from fastai.torch_core import to_float
from transformers.modeling_outputs import ModelOutput, CausalLMOutputWithPast # Base class for HF outputs

# %% ../../nbs/30_training_core.ipynb 7
def extract_loss_from_output(model_output:CausalLMOutputWithPast, *args):
    if hasattr(model_output, 'loss') and model_output.loss is not None:
        return model_output.loss
    else:
        raise ValueError(
            "Loss attribute not found or is None in the model output object. "
            "Ensure 'labels' are correctly passed to the model's forward method during training, "
            "and the model computes loss internally. If this is learner.summary(), ensure dummy labels are passed if model expects them."
        )

# %% ../../nbs/30_training_core.ipynb 8
class LLaVALoss(nn.Module):
    """ Custom CrossEntropyLoss that ignores indices where labels are IGNORE_INDEX (default -100).
    
    This loss function handles the standard autoregressive language modeling loss
    by shifting the logits and labels, ensuring the model predicts the next token.
    It specifically ignores tokens marked with `ignore_index` in the labels tensor,
    which is crucial for masking out prompt tokens, padding tokens, and image tokens
    during LLaVA training.
    """
    def __init__(self, ignore_index=IGNORE_INDEX):
        """ Initializes the loss function.
        
        Args:
            ignore_index (int): The label index to be ignored during loss calculation.
                                Defaults to the value imported from llava.data.preprocessing.
        """
        super().__init__()
        self.ignore_index = ignore_index
        # Initialize the standard CrossEntropyLoss with the specified ignore_index
        self.loss_fct = nn.CrossEntropyLoss(ignore_index=self.ignore_index)
        print(f"LLaVALoss initialized, ignoring index: {self.ignore_index}")

    def forward(self, output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """ Calculates the cross-entropy loss, ignoring specified indices.
        
        Args:
            output (torch.Tensor): The model's output logits.
                                   Shape: (batch_size, sequence_length, vocab_size).
            target (torch.Tensor): The target labels (token IDs).
                                   Shape: (batch_size, sequence_length).
                                   Should contain `ignore_index` for tokens to be ignored.
                                   
        Returns:
            torch.Tensor: A scalar tensor representing the calculated loss.
        """
        # --- Shift logits and labels for next token prediction --- 
        # Logits are shifted left (we predict the token *after* the current one)
        # output shape: (batch_size, sequence_length, vocab_size)
        shift_logits = output[..., :-1, :].contiguous()
        # Labels are shifted left (the target for the prediction at time t is the token at t+1)
        # target shape: (batch_size, sequence_length)
        shift_labels = target[..., 1:].contiguous()

        # --- Flatten the tokens for CrossEntropyLoss --- 
        # The CrossEntropyLoss expects input shape (N, C) where N is the number of samples
        # and C is the number of classes (vocab_size). The target shape should be (N).
        # Shift_logits flattened shape: (batch_size * (sequence_length - 1), vocab_size)
        # Shift_labels flattened shape: (batch_size * (sequence_length - 1))
        vocab_size = shift_logits.size(-1)
        loss = self.loss_fct(shift_logits.view(-1, vocab_size), 
                             shift_labels.view(-1))
        
        # Ensure loss is a scalar
        if loss.dim() > 0:
             # This might happen if the batch size or sequence length becomes 0 or 1 after masking/shifting.
             # Although CrossEntropyLoss typically returns a scalar, handle defensively.
             loss = loss.mean() 

        return loss

# %% ../../nbs/30_training_core.ipynb 14
class SafeGradientAccumulation(GradientAccumulation):
    "A GradientAccumulation callback that clones loss if it requires grad before in-place division."
    def after_loss(self):
        "Divides `loss_grad` by `n_acc`."
        # If loss_grad is a tensor that requires grad, clone it before the in-place op
        if isinstance(self.learn.loss_grad, torch.Tensor) and self.learn.loss_grad.requires_grad:
            self.learn.loss_grad = self.learn.loss_grad.clone() / (self.n_acc/find_bs(self.learn.yb))
        else: # Otherwise, proceed as normal (handles cases where loss_grad might not require grad, e.g. after detach)
            self.learn.loss_grad /= (self.n_acc/find_bs(self.learn.yb))

# %% ../../nbs/30_training_core.ipynb 15
class LLaVAMixedPrecision(MixedPrecision):
    "Mixed precision training specifically handling HF model outputs"
    def after_pred(self):
        pred = self.learn.pred
        if isinstance(pred, ModelOutput):
            if hasattr(pred, 'logits'):
                logits_val = getattr(pred, 'logits')
                if isinstance(logits_val, torch.Tensor) and logits_val.is_floating_point():
                    try:
                        # This attempts to modify the logits attribute of the ModelOutput object
                        setattr(pred, 'logits', to_float(logits_val))
                        # self.learn.pred remains the ModelOutput object, but with float logits
                    except AttributeError:
                        # If pred is immutable (e.g. namedtuple, though HF usually uses dataclasses)
                        # This path is less likely for HF outputs but good for robustness
                        warnings.warn("LLaVAMixedPrecision: Could not set 'logits' attribute directly on model output. Applying to_float to the whole output.", UserWarning)
                        self.learn.pred = to_float(pred)
                # else: logits might not be float or not a tensor, do nothing to it.
            # else: pred is ModelOutput but no logits, do nothing.
        elif isinstance(pred, torch.Tensor) and pred.is_floating_point():
            self.learn.pred = to_float(pred)
        elif isinstance(pred, (list, tuple)): # Handle cases where pred might be a list/tuple of tensors
             self.learn.pred = apply(lambda x: to_float(x) if isinstance(x, torch.Tensor) and x.is_floating_point() else x, pred)
        # else: pred is some other type, do nothing.
