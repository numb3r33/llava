"""Defines adaptive components for the LLaVA model, starting with the Adaptive Patcher interface."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/21_model_adaptive.ipynb.

# %% auto 0
__all__ = ['project_root', 'project_root_str', 'PATCHER_STRATEGIES', 'AdaptivePatcher', 'VariableResolutionPatcher',
           'AdaptiveLLaVAModel']

# %% ../../nbs/21_model_adaptive.ipynb 3
import sys
from pathlib import Path
import os

# Assumes the notebook is run from the project root or one level down (e.g., nbs/)
# Navigate up to the project root (where settings.ini or .git likely exists)
project_root = Path(os.getcwd())
# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level
if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():
    project_root = project_root.parent

project_root_str = str(project_root.resolve())

if project_root_str not in sys.path:
    print(f"Adding project root to sys.path: {project_root_str}")
    sys.path.insert(0, project_root_str)
else:
    # print(f"Project root already in sys.path: {project_root_str}") # Less verbose
    pass

# %% ../../nbs/21_model_adaptive.ipynb 4
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, Optional, Tuple, List
from PIL import Image
from torch.nn.utils.rnn import pad_sequence # For padding variable length sequences
from transformers.modeling_outputs import CausalLMOutputWithPast # For type hints
import warnings
import copy

# Import base model and utilities
from .baseline import BaselineLLaVAModel
from ..utils import load_config
from ..data.preprocessing import tokenizer, IMAGE_TOKEN_INDEX_PLACEHOLDER, IGNORE_INDEX

# %% ../../nbs/21_model_adaptive.ipynb 7
class AdaptivePatcher(nn.Module):
    """Base interface for adaptive image patching modules.

    Subclasses should implement the `forward` method to dynamically process
    an input image (or its features) based on content or context (like text instructions)
    and return a structured representation of image features for the projector.
    """
    def __init__(self, config: Dict[str, Any]):
        """Initializes the Adaptive Patcher.

        Args:
            config: Dictionary containing configuration relevant to the patcher strategy.
        """
        super().__init__()
        self.config = config
        # Potentially load sub-modules or parameters based on config['strategy']

    def forward(
        self,
        pixel_values: Optional[torch.Tensor] = None, # Existing features (e.g., from base processing)
        text_features: Optional[torch.Tensor] = None, # For text-guided strategies
        raw_image: Optional[Image.Image] = None, # Original image for properties like aspect ratio
        **kwargs
    ) -> Tuple[Optional[torch.Tensor], Optional[Dict[str, Any]]]:
        """Processes the input image adaptively or determines processing strategy.

        The exact inputs used and outputs returned depend on the specific strategy.
        For example, a strategy predictor might only need pooled features,
        while a variable resolution strategy primarily needs the raw image aspect ratio.
        A text-guided strategy needs text_features and potentially patch features.

        Args:
            pixel_values: Optional preprocessed image tensor (e.g., B x C x H x W).
                           Could represent the full image or specific patches.
            text_features: Optional tensor containing embeddings of the instruction text.
                           Needed for text-guided patching strategies.
            raw_image: Optional PIL image, potentially needed for calculating aspect ratio
                       or other properties not easily derived from pixel_values alone.
            **kwargs: Additional keyword arguments specific to the patching strategy.

        Returns:
            A tuple containing:
            - Optional[torch.Tensor]: Processed/selected image features, if the patcher
                                    directly outputs features. Shape might vary.
                                    Can be None if the patcher only outputs metadata.
            - Optional[Dict[str, Any]]: Metadata about the patching process or decision.
                                      (e.g., strategy used, number of patches, selected grid).
                                      Can be None if no metadata is generated.

        Raises:
            NotImplementedError: This base method must be implemented by subclasses.
        """
        raise NotImplementedError("Subclasses must implement the forward method.")

# %% ../../nbs/21_model_adaptive.ipynb 11
class VariableResolutionPatcher(AdaptivePatcher):
    """Adaptive patcher that selects an optimal grid resolution based on image aspect ratio,
    with an option to force baseline behavior for ablation studies.

    Inspired by the LLaVA-NeXT 'anyres' logic. It determines the target processing
    dimensions but does not perform the actual image processing or feature extraction.
    The main model's forward pass should use the output metadata to handle the image.
    """
    def __init__(self, config: Dict[str, Any]):
        """Initializes the VariableResolutionPatcher.

        Args:
            config: Dictionary containing configuration. Expected keys:
                    'model.adaptive_patcher.image_grid_pinpoints': List of [H, W] grids.
                    'model.vision_config.image_size': Base image size (e.g., 336).
                    'model.vision_config.patch_size': Patch size (e.g., 14).
                    'ablation.force_patcher_strategy': Optional string ('baseline' or null).
        """
        super().__init__(config)
        # Default grid from LLaVA-NeXT / Project Spec
        default_grid = [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]
        # Retrieve nested config safely
        patcher_config = self.config.get('model', {}).get('adaptive_patcher', {})
        vision_config = self.config.get('model', {}).get('vision_config', {})
        self.ablation_config = self.config.get('ablation', {})

        self.image_grid_pinpoints = patcher_config.get('image_grid_pinpoints', default_grid)
        self.base_image_size = vision_config.get('image_size', 336)
        self.patch_size = vision_config.get('patch_size', 14)

        # Ensure base resolution is included as an option (implicitly or explicitly)
        self.base_grid = (self.base_image_size, self.base_image_size)
        if list(self.base_grid) not in self.image_grid_pinpoints:
             self.image_grid_pinpoints.append(list(self.base_grid))
             
        self.force_baseline = self.ablation_config.get('force_patcher_strategy', None) == 'baseline'
        if self.force_baseline:
            print("Initialized VariableResolutionPatcher: Ablation active - FORCING BASELINE grid (336x336).")
        else:
             print(f"Initialized VariableResolutionPatcher with grid options: {self.image_grid_pinpoints}")

    def select_best_resolution(self, original_height: int, original_width: int) -> Tuple[int, int]:
        """Selects the best grid resolution based on aspect ratio and minimizing waste.

        Args:
            original_height: Height of the raw input image.
            original_width: Width of the raw input image.

        Returns:
            Tuple[int, int]: The selected best grid dimensions (height, width).
        """
        original_aspect_ratio = original_height / original_width

        best_fit_grid = None
        min_wasted_pixels = float('inf')

        for grid_h, grid_w in self.image_grid_pinpoints:
            grid_aspect_ratio = grid_h / grid_w

            # Calculate the dimensions if the image were scaled to fit this grid
            # Scale based on the limiting dimension
            scale_h = grid_h / original_height
            scale_w = grid_w / original_width

            if original_aspect_ratio > grid_aspect_ratio: # Scale based on height
                scaled_h = grid_h
                scaled_w = int(original_width * scale_h)
            else: # Scale based on width
                scaled_w = grid_w
                scaled_h = int(original_height * scale_w)
            
            # Ensure scaled dimensions do not exceed grid dimensions (due to int conversion)
            scaled_h = min(scaled_h, grid_h)
            scaled_w = min(scaled_w, grid_w)
            
            # Calculate wasted area (pixels in the grid not covered by the scaled image)
            grid_area = grid_h * grid_w
            scaled_image_area = scaled_h * scaled_w
            wasted_pixels = grid_area - scaled_image_area

            # Prefer grids with less waste
            # Among grids with similar waste, the reference code doesn't specify tie-breaking.
            # Let's pick the first one encountered with the minimum waste.
            # A slightly better tie-breaker might be aspect ratio closeness, but min waste is simpler.
            if wasted_pixels < min_wasted_pixels:
                min_wasted_pixels = wasted_pixels
                best_fit_grid = (grid_h, grid_w)
            # Simple tie-breaking: if waste is equal, prefer larger area (less scaling down)
            elif wasted_pixels == min_wasted_pixels:
                 if best_fit_grid is None or (grid_h * grid_w > best_fit_grid[0] * best_fit_grid[1]):
                       best_fit_grid = (grid_h, grid_w)
                       
        # Fallback to base resolution if something went wrong
        if best_fit_grid is None:
            print(f"Warning: Could not determine best fit grid for H={original_height}, W={original_width}. Defaulting to base {self.base_image_size}x{self.base_image_size}.")
            best_fit_grid = self.base_grid
            
        return best_fit_grid

    def forward(self,
                *args: Any, # Accept positional args for flexibility
                pixel_values: Optional[torch.Tensor] = None,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None,
                # Add raw_images potentially needed by patcher (extract from kwargs or batch dict)
                raw_images: Optional[List[Image.Image]] = None,
                **kwargs: Any # Accept keyword args
               ) -> CausalLMOutputWithPast: # Return type from transformers
        """Defines the forward pass of the Adaptive LLaVA model.

        Handles input flexibility. If an adaptive patcher is enabled, it's called first
        to potentially determine processing strategy metadata. Currently, the image features
        used still come from the standard `pixel_values` input, pending full adaptive implementation.

        Args:
            *args: Positional arguments. If args[0] is a dictionary, it's treated as the batch. If args[0] is a Tensor, it might be pixel_values.
            pixel_values (Optional[torch.Tensor]): Tensor of image pixel values.
            input_ids (Optional[torch.Tensor]): Tensor of input token IDs, potentially containing image markers (-200).
            attention_mask (Optional[torch.Tensor]): Attention mask for input_ids.
            labels (Optional[torch.Tensor]): Labels for language modeling loss (shifted internally).
            raw_images (Optional[List[PIL.Image.Image]]): List of raw PIL images for the batch, if needed by the patcher.
            **kwargs: Keyword arguments, potentially containing the input tensors or 'raw_images'.

        Returns:
            Output dictionary from the language model (transformers.CausalLMOutputWithPast).
        """
        # --- Input Parsing Logic (Handles *args, **kwargs, dict in args[0], tensor in args[0]) ---
        _pixel_values, _input_ids, _attention_mask, _labels, _raw_images = None, None, None, None, None
        batch_dict = None

        # Case 1: fastai standard case (batch dict in args[0])
        if len(args) == 1 and isinstance(args[0], dict):
            batch_dict = args[0]
            _pixel_values = batch_dict.get('pixel_values')
            _input_ids = batch_dict.get('input_ids')
            _attention_mask = batch_dict.get('attention_mask')
            _labels = batch_dict.get('labels')
            _raw_images = batch_dict.get('raw_images') # Try getting raw_images from dict
        # Case 2: Try kwargs next
        elif kwargs:
            _pixel_values = kwargs.get('pixel_values', pixel_values)
            _input_ids = kwargs.get('input_ids', input_ids)
            _attention_mask = kwargs.get('attention_mask', attention_mask)
            _labels = kwargs.get('labels', labels)
            _raw_images = kwargs.get('raw_images', raw_images) # Get raw_images from kwargs
        # Case 3: Check if only pixel_values was passed positionally (e.g., summary)
        elif len(args) == 1 and isinstance(args[0], torch.Tensor) and pixel_values is None:
             _pixel_values = args[0]
             # Attempt to get others from formal params (likely None)
             _input_ids = input_ids
             _attention_mask = attention_mask
             _labels = labels
             _raw_images = raw_images # Get raw_images from formal params
        # Case 4: Fallback to formal parameters if nothing else worked
        else:
             _pixel_values = pixel_values
             _input_ids = input_ids
             _attention_mask = attention_mask
             _labels = labels
             _raw_images = raw_images

        # --- Handle learner.summary() potentially needing dummy text inputs ---
        if _pixel_values is not None and _input_ids is None:
            warnings.warn("Adaptive forward() received pixel_values but not input_ids after parsing. "
                          "Creating dummy text inputs (likely for learner.summary()).", UserWarning)
            batch_size = _pixel_values.shape[0]
            dummy_seq_len = 1
            target_device = _pixel_values.device
            if tokenizer is None: raise RuntimeError("Tokenizer not available for dummy inputs.")

            _input_ids = torch.full(
                (batch_size, dummy_seq_len),
                tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0,
                dtype=torch.long,
                device=target_device
            )
            _input_ids[:, 0] = self.image_token_index_marker
            _attention_mask = torch.ones_like(_input_ids)
            _labels = None
            _raw_images = None # No raw images needed/available for dummy case

        # --- Final check for essential inputs ---
        if _pixel_values is None or _input_ids is None:
            err_msg_parts = ["Adaptive forward() missing required arguments after parsing: pixel_values and input_ids must be provided."]
            # ... (add more details as needed) ...
            raise ValueError("\n".join(err_msg_parts))

        # Use resolved values from here on
        pixel_values, input_ids, attention_mask, labels, raw_images = _pixel_values, _input_ids, _attention_mask, _labels, _raw_images
        # --- End Input Parsing & Dummy Input Handling ---


        # --- Patcher Logic (Get Metadata) ---
        patcher_metadata_batch = [None] * pixel_values.shape[0] # Initialize metadata list
        if self.patcher is not None:
            # Pass necessary inputs to the patcher
            # Currently VariableResolutionPatcher only needs raw_image if not forced baseline
            requires_raw = not getattr(self.patcher, 'force_baseline', False)
            if requires_raw and (raw_images is None or len(raw_images) != pixel_values.shape[0]):
                 warnings.warn(f"Patcher '{self.patcher.__class__.__name__}' needs raw_images, but they were not provided or length mismatch.", UserWarning)
                 # Decide behavior: Proceed without metadata, or raise error? For now, proceed.
            elif requires_raw or getattr(self.patcher, 'force_baseline', False): # Run if raw needed or baseline forced
                for i in range(pixel_values.shape[0]):
                    try:
                        current_raw_image = raw_images[i] if raw_images else None
                        # Pass only the necessary inputs to the patcher's forward
                        _, patcher_metadata_sample = self.patcher(raw_image=current_raw_image)
                        patcher_metadata_batch[i] = patcher_metadata_sample
                    except Exception as e:
                        warnings.warn(f"Error running patcher for batch index {i}: {e}")
                self.current_patcher_metadata = patcher_metadata_batch # Store for inspection
        # --- End Patcher Logic ---

        # --- Feature Processing (Currently Still Baseline Behavior) ---
        # TODO: Modify this section based on `patcher_metadata_batch`.
        # This currently ignores the patcher output metadata and uses baseline features.

        if self.language_model is None or self.vision_tower is None or self.projector is None:
            raise RuntimeError("Model components (LLM, Vision Tower, Projector) are not fully loaded.")

        image_features = self.encode_image(pixel_values) # (B, P_base, D_clip)
        if image_features is None:
            raise RuntimeError("Image encoding failed.")

        projector_device = next(self.projector.parameters()).device
        projector_dtype = next(self.projector.parameters()).dtype
        image_features = image_features.to(projector_device, dtype=projector_dtype)
        projected_image_features = self.projector(image_features) # (B, P_base, D_llm)
        num_image_patches = projected_image_features.shape[1]

        # --- Prepare LLM inputs (Same as baseline) ---
        embedding_layer = self.get_input_embeddings()
        target_device = embedding_layer.weight.device
        target_dtype = embedding_layer.weight.dtype

        input_ids_clone = input_ids.clone().to(target_device)
        input_ids_clone[input_ids_clone == self.image_token_index_marker] = 0

        text_embeddings = embedding_layer(input_ids_clone)
        projected_image_features = projected_image_features.to(target_device, dtype=target_dtype)

        new_input_embeds = []
        new_labels = [] if labels is not None else None
        new_attention_mask = []

        for batch_idx in range(input_ids.shape[0]):
            current_input_ids_slice = input_ids[batch_idx].to(target_device)
            image_token_indices = torch.where(current_input_ids_slice == self.image_token_index_marker)[0]

            if len(image_token_indices) == 0:
                warnings.warn(f"Image token placeholder {self.image_token_index_marker} not found in batch index {batch_idx}. Using text embeddings only.", UserWarning)
                new_input_embeds.append(text_embeddings[batch_idx])
                current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()
                new_attention_mask.append(current_attention_mask_slice)
                if new_labels is not None and labels is not None:
                    new_labels.append(labels[batch_idx].to(target_device))
                continue

            image_token_start_index = image_token_indices[0].item()
            text_emb_before = text_embeddings[batch_idx, :image_token_start_index]
            text_emb_after = text_embeddings[batch_idx, image_token_start_index + 1:]

            cur_new_embed = torch.cat([
                text_emb_before,
                projected_image_features[batch_idx],
                text_emb_after
            ], dim=0)
            new_input_embeds.append(cur_new_embed)

            current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()
            mask_before = current_attention_mask_slice[:image_token_start_index]
            mask_image = torch.ones(num_image_patches, dtype=torch.long, device=target_device)
            mask_after = current_attention_mask_slice[image_token_start_index + 1:]
            cur_new_mask = torch.cat([mask_before, mask_image, mask_after], dim=0)
            new_attention_mask.append(cur_new_mask)

            if new_labels is not None and labels is not None:
                current_labels_slice = labels[batch_idx].to(target_device)
                label_before = current_labels_slice[:image_token_start_index]
                label_image = torch.full((num_image_patches,), self.ignore_index, dtype=torch.long, device=target_device)
                label_after = current_labels_slice[image_token_start_index + 1:]
                cur_new_label = torch.cat([label_before, label_image, label_after], dim=0)
                new_labels.append(cur_new_label)

        # --- Padding (Same as baseline) ---
        padded_input_embeds = pad_sequence(new_input_embeds, batch_first=True, padding_value=0.0)
        padded_attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)
        padded_labels = None
        if new_labels is not None and len(new_labels) > 0:
            padded_labels = pad_sequence(new_labels, batch_first=True, padding_value=self.ignore_index)

        # --- Pass to LLM (Same as baseline) ---
        outputs: CausalLMOutputWithPast = self.language_model(
            inputs_embeds=padded_input_embeds,
            attention_mask=padded_attention_mask,
            labels=padded_labels, # Pass potentially None labels
            return_dict=True
        )

        return outputs

# %% ../../nbs/21_model_adaptive.ipynb 16
# Placeholder for other potential patcher implementations
# class PredictorPatcher(AdaptivePatcher): ...
# class TextGuidedPatcher(AdaptivePatcher): ...

# Mapping from strategy name in config to Patcher class
PATCHER_STRATEGIES = {
    'variable_resolution': VariableResolutionPatcher,
    # 'predictor': PredictorPatcher, # Add when implemented
    # 'text_guided': TextGuidedPatcher, # Add when implemented
}

class AdaptiveLLaVAModel(BaselineLLaVAModel):
    """LLaVA Model extended with an Adaptive Patcher module.

    Inherits from BaselineLLaVAModel and adds an adaptive patcher component
    based on the configuration. The forward pass needs to be overridden
    to incorporate the patcher's logic.
    """
    def __init__(self, config: Dict[str, Any]):
        """Initializes the Adaptive LLaVA model.

        Loads baseline components and instantiates the specified adaptive patcher.

        Args:
            config: The main configuration dictionary.
        """
        # Initialize baseline components (Vision Tower, LLM, Projector)
        super().__init__(config)

        self.patcher = None
        patcher_config = self.config.get('model', {}).get('adaptive_patcher', {})
        patcher_enabled = patcher_config.get('enabled', False)
        patcher_strategy = patcher_config.get('strategy')

        if patcher_enabled and patcher_strategy:
            if patcher_strategy in PATCHER_STRATEGIES:
                PatcherClass = PATCHER_STRATEGIES[patcher_strategy]
                try:
                    # Pass the full config, including ablation settings
                    self.patcher = PatcherClass(config)
                    print(f"Adaptive Patcher enabled with strategy: '{patcher_strategy}' ({PatcherClass.__name__})")
                except Exception as e:
                    print(f"Error initializing patcher '{patcher_strategy}': {e}. Disabling patcher.")
                    self.patcher = None
            else:
                print(f"Warning: Unknown adaptive patcher strategy '{patcher_strategy}'. Disabling patcher.")
                self.patcher = None
        else:
            print("Adaptive Patcher is disabled in the configuration.")

    # --- Step 6.5: Implement Adaptive Forward Pass ---
    # Override the forward pass to integrate the patcher logic
    def forward(self,
                *args: Any, # Accept positional args for flexibility
                pixel_values: Optional[torch.Tensor] = None,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None,
                # Add raw_images potentially needed by patcher (extract from kwargs or batch dict)
                raw_images: Optional[List[Image.Image]] = None,
                **kwargs: Any # Accept keyword args
               ) -> CausalLMOutputWithPast: # Return type from transformers
        """Defines the forward pass of the Adaptive LLaVA model.

        If an adaptive patcher is enabled, it's called to determine patching strategy
        (metadata stored). The actual image features used currently come from the standard
        `pixel_values` input (base resolution), regardless of patcher output. This allows
        the structural integration without implementing complex variable feature handling yet.

        Handles input flexibility for compatibility with fastai Learner.

        Args:
            *args: Positional arguments. If args[0] is a dictionary, it's treated as the batch. If args[0] is a Tensor, it might be pixel_values.
            pixel_values (Optional[torch.Tensor]): Tensor of image pixel values.
            input_ids (Optional[torch.Tensor]): Tensor of input token IDs, potentially containing image markers (-200).
            attention_mask (Optional[torch.Tensor]): Attention mask for input_ids.
            labels (Optional[torch.Tensor]): Labels for language modeling loss (shifted internally).
            raw_images (Optional[List[PIL.Image.Image]]): List of raw PIL images for the batch, if needed by the patcher.
            **kwargs: Keyword arguments, potentially containing the input tensors or 'raw_images'.

        Returns:
            Output dictionary from the language model (transformers.CausalLMOutputWithPast).
        """
        # --- Input Parsing Logic (Handles *args, **kwargs, dict in args[0], tensor in args[0]) ---
        _pixel_values, _input_ids, _attention_mask, _labels, _raw_images = None, None, None, None, None
        batch_dict = {}

        if len(args) == 1 and isinstance(args[0], dict):
            batch_dict = args[0]
            _pixel_values = batch_dict.get('pixel_values')
            _input_ids = batch_dict.get('input_ids')
            _attention_mask = batch_dict.get('attention_mask')
            _labels = batch_dict.get('labels')
            _raw_images = batch_dict.get('raw_images')
        elif len(args) == 1 and isinstance(args[0], torch.Tensor):
            _pixel_values = args[0]
        
        # Prioritize kwargs, then formal params if not found elsewhere
        if _pixel_values is None: _pixel_values = kwargs.get('pixel_values', pixel_values)
        if _input_ids is None: _input_ids = kwargs.get('input_ids', input_ids)
        if _attention_mask is None: _attention_mask = kwargs.get('attention_mask', attention_mask)
        if _labels is None: _labels = kwargs.get('labels', labels)
        if _raw_images is None: _raw_images = kwargs.get('raw_images', raw_images)

        # --- Handle learner.summary() case: Create dummy text inputs ---
        is_summary_call = (_pixel_values is not None and _input_ids is None)
        if is_summary_call:
            # print("Warning: input_ids not provided, creating dummy inputs for summary/tracing.") # Optional warning
            batch_size = _pixel_values.shape[0]
            dummy_seq_len = 1 # Minimal sequence length
            target_device = _pixel_values.device 
            if tokenizer is None: raise RuntimeError("Tokenizer not available to create dummy inputs.")
            
            _input_ids = torch.full(
                (batch_size, dummy_seq_len),
                tokenizer.pad_token_id,
                dtype=torch.long,
                device=target_device
            )
            _attention_mask = torch.zeros_like(_input_ids)
            _labels = None
        
        # --- Final check for essential inputs ---
        if _pixel_values is None or _input_ids is None:
            err_msg_parts = ["Adaptive forward() missing required arguments: pixel_values and input_ids must be provided."]
            err_msg_parts.append(f"  Resolved pixel_values is None: {_pixel_values is None}")
            err_msg_parts.append(f"  Resolved input_ids is None: {_input_ids is None}")
            err_msg_parts.append(f"  len(args): {len(args)}, type(args[0]) if args else 'N/A': {type(args[0]) if args else 'N/A'}")
            err_msg_parts.append(f"  kwargs keys: {list(kwargs.keys())}")
            raise ValueError("\n".join(err_msg_parts))

        # Use resolved values from here on
        pixel_values, input_ids, attention_mask, labels, raw_images = _pixel_values, _input_ids, _attention_mask, _labels, _raw_images
        # --- End Input Parsing & Dummy Input Handling ---


        # --- Patcher Logic (Get Metadata) ---
        patcher_metadata_batch = [None] * pixel_values.shape[0] # Initialize metadata list
        if self.patcher is not None:
            requires_raw = not getattr(self.patcher, 'force_baseline', False)
            if requires_raw and (raw_images is None or len(raw_images) != pixel_values.shape[0]):
                 warnings.warn(f"Patcher '{self.patcher.__class__.__name__}' is active and needs raw_images, but they were not provided or length mismatch. Patcher cannot run effectively.", UserWarning)
                 # Proceed without patcher metadata if raw images are missing when needed
            else:
                # Iterate through batch to get metadata for each sample
                for i in range(pixel_values.shape[0]):
                    try:
                        current_raw_image = raw_images[i] if raw_images else None
                        _, patcher_metadata_sample = self.patcher(raw_image=current_raw_image)
                        patcher_metadata_batch[i] = patcher_metadata_sample
                    except Exception as e:
                        warnings.warn(f"Error running patcher for batch index {i}: {e}")
                self.current_patcher_metadata = patcher_metadata_batch # Store for potential inspection/logging

        # --- Feature Processing (Currently Still Baseline Behavior) ---
        # TODO: Modify this section based on `patcher_metadata_batch`.
        # For now, it proceeds exactly like the baseline model using the standard pixel_values input.

        if self.language_model is None or self.vision_tower is None or self.projector is None:
            raise RuntimeError("Model components (LLM, Vision Tower, Projector) are not fully loaded.")

        image_features = self.encode_image(pixel_values) # (B, P_base, D_clip)
        if image_features is None:
            raise RuntimeError("Image encoding failed.")

        projector_device = next(self.projector.parameters()).device
        image_features = image_features.to(projector_device)
        projected_image_features = self.projector(image_features) # (B, P_base, D_llm)
        num_image_patches = projected_image_features.shape[1]

        # --- Prepare LLM inputs (Same as baseline) ---
        embedding_layer = self.get_input_embeddings()
        target_device = embedding_layer.weight.device

        input_ids_clone = input_ids.clone().to(target_device)
        input_ids_clone[input_ids_clone == self.image_token_index_marker] = 0

        text_embeddings = embedding_layer(input_ids_clone)
        projected_image_features = projected_image_features.to(target_device, dtype=text_embeddings.dtype)

        new_input_embeds = []
        new_labels = [] if labels is not None else None
        new_attention_mask = []

        for batch_idx in range(input_ids.shape[0]):
            current_input_ids_slice = input_ids[batch_idx].to(target_device)
            image_token_indices = torch.where(current_input_ids_slice == self.image_token_index_marker)[0]

            if len(image_token_indices) == 0:
                # If using dummy inputs, image marker won't be found, handle gracefully.
                if is_summary_call:
                    new_input_embeds.append(text_embeddings[batch_idx])
                    current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else torch.zeros_like(current_input_ids_slice)
                    new_attention_mask.append(current_attention_mask_slice)
                else: # Normal operation, warn if marker is missing
                    warnings.warn(f"Image token placeholder {self.image_token_index_marker} not found in batch index {batch_idx}. Skipping image features.")
                    new_input_embeds.append(text_embeddings[batch_idx])
                    current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()
                    new_attention_mask.append(current_attention_mask_slice)
                    if new_labels is not None and labels is not None:
                        new_labels.append(labels[batch_idx].to(target_device))
                continue # Move to next item in batch

            image_token_start_index = image_token_indices[0].item()
            # Ensure indices are valid before slicing
            if image_token_start_index >= text_embeddings.shape[1]:
                 warnings.warn(f"Calculated image_token_start_index {image_token_start_index} is out of bounds for text_embeddings shape {text_embeddings.shape}. Skipping.")
                 new_input_embeds.append(text_embeddings[batch_idx])
                 new_attention_mask.append(attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long())
                 if new_labels is not None and labels is not None: new_labels.append(labels[batch_idx].to(target_device))
                 continue


            text_emb_before = text_embeddings[batch_idx, :image_token_start_index]
            text_emb_after = text_embeddings[batch_idx, image_token_start_index + 1:]

            cur_new_embed = torch.cat([
                text_emb_before,
                projected_image_features[batch_idx],
                text_emb_after
            ], dim=0)
            new_input_embeds.append(cur_new_embed)

            current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()
            mask_before = current_attention_mask_slice[:image_token_start_index]
            mask_image = torch.ones(num_image_patches, dtype=torch.long, device=target_device)
            mask_after = current_attention_mask_slice[image_token_start_index + 1:]
            cur_new_mask = torch.cat([mask_before, mask_image, mask_after], dim=0)
            new_attention_mask.append(cur_new_mask)

            if new_labels is not None and labels is not None:
                current_labels_slice = labels[batch_idx].to(target_device)
                label_before = current_labels_slice[:image_token_start_index]
                label_image = torch.full((num_image_patches,), self.ignore_index, dtype=torch.long, device=target_device)
                label_after = current_labels_slice[image_token_start_index + 1:]
                cur_new_label = torch.cat([label_before, label_image, label_after], dim=0)
                new_labels.append(cur_new_label)

        # --- Padding (Same as baseline) ---
        padded_input_embeds = pad_sequence(new_input_embeds, batch_first=True, padding_value=0.0)
        padded_attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)
        padded_labels = None
        if new_labels is not None:
            padded_labels = pad_sequence(new_labels, batch_first=True, padding_value=self.ignore_index)

        # --- Pass to LLM (Same as baseline) ---
        outputs: CausalLMOutputWithPast = self.language_model(
            inputs_embeds=padded_input_embeds,
            attention_mask=padded_attention_mask,
            labels=padded_labels,
            return_dict=True
        )

        return outputs
