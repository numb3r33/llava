"""Defines the baseline LLaVA 1.5 model architecture, including the projector and the combined model structure."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/20_model_baseline.ipynb.

# %% auto 0
__all__ = ['project_root', 'project_root_str', 'LLaVAProjector', 'BaselineLLaVAModel']

# %% ../../nbs/20_model_baseline.ipynb 3
import sys
from pathlib import Path
import os
from typing import Any

# Assumes the notebook is run from the project root or one level down (e.g., nbs/)
# Navigate up to the project root (where settings.ini or .git likely exists)
project_root = Path(os.getcwd())
# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level
if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():
    project_root = project_root.parent

project_root_str = str(project_root.resolve())

if project_root_str not in sys.path:
    print(f"Adding project root to sys.path: {project_root_str}")
    sys.path.insert(0, project_root_str)
else:
    print(f"Project root already in sys.path: {project_root_str}")

# %% ../../nbs/20_model_baseline.ipynb 4
import torch
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence # For padding variable length sequences
from transformers import CLIPVisionModel, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig
from transformers.modeling_outputs import BaseModelOutputWithPooling, CausalLMOutputWithPast # For type hints
import warnings
from typing import Optional # Added for type hinting

try:
    from peft import LoraConfig, get_peft_model, PeftModel # Import PEFT components
    _peft_available = True
except ImportError:
    print("Warning: peft library not found. LoRA functionality will be disabled.")
    LoraConfig, get_peft_model, PeftModel = None, None, None # Define as None if not available
    _peft_available = False

from ..utils import load_config # Assuming utils notebook is created
from ..data.preprocessing import IMAGE_TOKEN_INDEX_PLACEHOLDER, IGNORE_INDEX, tokenizer, DEFAULT_IMAGE_TOKEN # Import constants and tokenizer

# Ensure tokenizer is loaded (it should be from the import)
if tokenizer is None:
    print("Warning: Tokenizer could not be imported from data.preprocessing. Trying to load it again...")
    try:
        # Need to load config first to get LLM name
        _config_temp = load_config('configs/config.yaml')
        _llm_name_temp = _config_temp.get('model', {}).get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')
        _tokenizer_max_len_temp = _config_temp.get('data', {}).get('tokenizer_model_max_length', 2048)
        _tokenizer_padding_side_temp = _config_temp.get('data', {}).get('tokenizer_padding_side', 'right')

        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            _llm_name_temp,
            model_max_length=_tokenizer_max_len_temp,
            padding_side=_tokenizer_padding_side_temp,
            use_fast=True,
        )
        # Re-add special token logic if necessary (should match preprocessing step)
        # from llava.data.preprocessing import DEFAULT_IMAGE_TOKEN # Already imported
        if DEFAULT_IMAGE_TOKEN not in tokenizer.get_vocab():
             num_added = tokenizer.add_special_tokens({'additional_special_tokens': [DEFAULT_IMAGE_TOKEN]})
             print(f"Added {num_added} token(s) during re-load. New vocab size: {len(tokenizer)}")
        if tokenizer.pad_token is None:
             tokenizer.pad_token = tokenizer.eos_token
             print(f"Set pad token to EOS during re-load.")
        print(f"Tokenizer re-loaded successfully for {_llm_name_temp}.")

    except Exception as e:
        print(f"Fatal Error: Could not load tokenizer. {e}")
        # Depending on the context, might raise an error or exit
        raise ImportError("Tokenizer is essential and could not be loaded.") from e

# %% ../../nbs/20_model_baseline.ipynb 7
class LLaVAProjector(nn.Module):
    """A simple 2-layer MLP projector to map vision features to LLM embedding space.

    Maps input_dim (e.g., CLIP ViT-L output dim = 1024) to
    output_dim (e.g., Vicuna-7B hidden dim = 4096).

    Architecture: Linear -> GELU -> Linear
    """
    def __init__(self, input_dim: int, output_dim: int):
        """Initializes the MLP projector.

        Args:
            input_dim: Dimension of the input vision features.
            output_dim: Dimension of the LLM's hidden space.
        """
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, output_dim, bias=True),
            nn.GELU(),
            nn.Linear(output_dim, output_dim, bias=True)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Projects the input features.

        Args:
            x: Input tensor of shape (batch_size, num_patches, input_dim) or similar.

        Returns:
            Projected tensor of shape (batch_size, num_patches, output_dim).
        """
        return self.model(x)

# %% ../../nbs/20_model_baseline.ipynb 13
class BaselineLLaVAModel(nn.Module):
    """Baseline LLaVA 1.5 model combining Vision Encoder, Projector, and LLM."""
    def __init__(self, config: dict):
        super().__init__()
        self.config = config 
        self.model_config = config.get('model', {}) 

        self.vision_tower = None
        self.language_model = None
        self.projector = None

        self.image_token_index_marker = self.model_config.get('image_token_index_marker', IMAGE_TOKEN_INDEX_PLACEHOLDER)
        self.ignore_index = IGNORE_INDEX 
        self.vision_feature_layer = self.model_config.get('vision_feature_layer', -2)
        self.vision_encoder_name = self.model_config.get('vision_encoder_name_or_path', 'openai/clip-vit-large-patch14-336')
        self.llm_name = self.model_config.get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')

        llm_hf_config = AutoConfig.from_pretrained(self.llm_name, trust_remote_code=True)
        vision_hf_config = AutoConfig.from_pretrained(self.vision_encoder_name, trust_remote_code=True)

        projector_cfg_from_yaml = self.model_config.get('projector', {})
        
        proj_input_dim_fallback = vision_hf_config.vision_config.hidden_size if hasattr(vision_hf_config, 'vision_config') else vision_hf_config.hidden_size
        proj_input_dim = projector_cfg_from_yaml.get('input_dim', proj_input_dim_fallback)
        proj_output_dim = projector_cfg_from_yaml.get('output_dim', llm_hf_config.hidden_size)

        print(f"Initializing Projector: Input Dim={proj_input_dim}, Output Dim={proj_output_dim}")
        self.projector = LLaVAProjector(proj_input_dim, proj_output_dim)

        self.load_vision_tower(expected_output_dim=proj_input_dim)
        self.load_language_model() 
        self.resize_llm_embeddings()
        self.apply_activation_checkpointing()

        if torch.cuda.is_available():
            target_device = torch.device("cuda:0")
            if self.vision_tower is not None and next(self.vision_tower.parameters()).device != target_device:
                print(f"Moving vision_tower to {target_device}")
                self.vision_tower.to(target_device)
            if self.projector is not None and next(self.projector.parameters()).device != target_device:
                print(f"Moving projector to {target_device}")
                self.projector.to(target_device)
            if self.language_model is not None:
                 llm_device = next(self.language_model.parameters()).device
                 print(f"Language model's first parameter is on device: {llm_device}")


    def load_vision_tower(self, expected_output_dim: int):
        try:
            print(f"Loading Vision Tower: {self.vision_encoder_name}...")
            self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_encoder_name, trust_remote_code=True)
            print(f"Vision Tower loaded successfully.")
            self.vision_tower.requires_grad_(False)
            print(f"Vision Tower weights frozen.")
            vision_output_dim = self.vision_tower.config.hidden_size
            if vision_output_dim != expected_output_dim:
                warnings.warn(
                    f"Vision Tower output dimension ({vision_output_dim}) does not match "
                    f"Projector input dimension ({expected_output_dim})."
                )
        except Exception as e:
            print(f"Error loading Vision Tower ({self.vision_encoder_name}): {e}")
            self.vision_tower = None

    def load_language_model(self):
        try:
            print(f"Loading Language Model: {self.llm_name}...")
            
            quantization_config_dict = self.model_config.get('quantization', {})
            load_in_4bit = quantization_config_dict.get('load_in_4bit', False)
            bnb_config = None
            extra_kwargs = {"trust_remote_code": True}

            if load_in_4bit:
                if BitsAndBytesConfig is None:
                    raise ImportError("bitsandbytes library is required for 4-bit quantization but not found.")
                
                bnb_4bit_quant_type = quantization_config_dict.get('bnb_4bit_quant_type', "nf4")
                bnb_4bit_compute_dtype_str = quantization_config_dict.get('bnb_4bit_compute_dtype', "float16")
                compute_dtype = getattr(torch, bnb_4bit_compute_dtype_str)

                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type=bnb_4bit_quant_type,
                    bnb_4bit_compute_dtype=compute_dtype,
                    bnb_4bit_use_double_quant=quantization_config_dict.get('bnb_4bit_use_double_quant', False),
                )
                print(f"QLoRA enabled: Loading LLM in 4-bit with compute dtype {compute_dtype}.")
                if torch.cuda.is_available():
                    extra_kwargs["device_map"] = {"": 0} 
                    print(f"  Setting device_map to {{'': 0}} for QLoRA.")
                else:
                    print("  Warning: CUDA not available, QLoRA device_map will not be set to GPU. Model will load on CPU if possible.")
                extra_kwargs["quantization_config"] = bnb_config
            
            self.language_model = AutoModelForCausalLM.from_pretrained(
                self.llm_name,
                **extra_kwargs 
            )
            print(f"Language Model loaded successfully.")
            
            if not load_in_4bit: 
                 self.language_model.requires_grad_(False)
                 print(f"Base Language Model weights frozen (non-QLoRA path).")

            peft_config_dict = self.model_config.get('peft', {}) 
            use_lora = peft_config_dict.get('use_lora', False)

            if use_lora:
                if not _peft_available:
                    print("Warning: `use_lora` is true in config, but peft library is not installed. Skipping LoRA.")
                else:
                    if isinstance(self.language_model, PeftModel):
                        print("Language model is already a PeftModel. Ensure LoRA config matches if re-applying.")
                    
                    print("Applying LoRA...")
                    lora_r = peft_config_dict.get('lora_r', 8)
                    lora_alpha = peft_config_dict.get('lora_alpha', 16)
                    lora_dropout = peft_config_dict.get('lora_dropout', 0.05)
                    target_modules = peft_config_dict.get('target_modules', ['q_proj', 'v_proj'])
                    
                    current_lora_config = LoraConfig( 
                        r=lora_r,
                        lora_alpha=lora_alpha,
                        target_modules=target_modules,
                        lora_dropout=lora_dropout,
                        bias="none",
                    )
                    self.language_model = get_peft_model(self.language_model, current_lora_config)
                    print(f"LoRA applied. Config: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}, modules={target_modules}")
                    self.language_model.print_trainable_parameters()
            else:
                print("LoRA is disabled in the configuration.")
        except Exception as e:
            print(f"Error loading Language Model ({self.llm_name}): {e}")
            import traceback
            traceback.print_exc()
            self.language_model = None

    def resize_llm_embeddings(self):
        if self.language_model is None or tokenizer is None:
            print("Warning: Cannot resize LLM embeddings. LLM or tokenizer not available.")
            return

        current_embeddings = self.get_input_embeddings()
        current_vocab_size = current_embeddings.weight.size(0)
        target_vocab_size = len(tokenizer)

        if current_vocab_size != target_vocab_size:
            print(f"Resizing LLM token embeddings from {current_vocab_size} to {target_vocab_size} (tokenizer size)...")
            self.language_model.resize_token_embeddings(target_vocab_size)
            print("LLM token embeddings resized.")
        else:
            print("LLM embedding size already matches tokenizer size. No resizing needed.")
            
    def apply_activation_checkpointing(self):
        use_checkpointing = self.model_config.get('use_activation_checkpointing', False)
        if use_checkpointing:
            if self.language_model is not None:
                 model_to_checkpoint = self.language_model 
                 if hasattr(model_to_checkpoint, 'gradient_checkpointing_enable'):
                     try:
                         model_to_checkpoint.gradient_checkpointing_enable()
                         print("Activation checkpointing enabled for the language model.")
                     except Exception as e:
                         print(f"Warning: Failed to enable activation checkpointing: {e}")
                 else:
                     print("Warning: Language model does not have 'gradient_checkpointing_enable' method.")
            else:
                 print("Warning: Activation checkpointing enabled in config, but language model is not loaded.")
        else:
             print("Activation checkpointing is disabled in the configuration.")

    def get_input_embeddings(self) -> nn.Embedding:
        if self.language_model is None:
            raise ValueError("Language model not loaded.")
        return self.language_model.get_input_embeddings()

    def encode_image(self, pixel_values: torch.Tensor) -> torch.Tensor | None:
        if self.vision_tower is None:
            print("Error: Vision tower not loaded, cannot encode image.")
            return None
        try:
            vision_device = next(self.vision_tower.parameters()).device
            vision_dtype = next(self.vision_tower.parameters()).dtype
            pixel_values = pixel_values.to(vision_device, dtype=vision_dtype)

            vision_outputs: BaseModelOutputWithPooling = self.vision_tower(
                pixel_values,
                output_hidden_states=True
            )
            image_features = vision_outputs.hidden_states[self.vision_feature_layer]
            image_features = image_features[:, 1:, :] 
            return image_features
        except Exception as e:
            print(f"Error during image encoding: {e}")
            import traceback
            traceback.print_exc()
            return None

    def forward(self, 
                *args: Any, # To catch positional arguments like a single dict from Learner.summary
                pixel_values: Optional[torch.Tensor] = None,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None, 
                labels: Optional[torch.Tensor] = None,
                **kwargs: Any
               ) -> CausalLMOutputWithPast:
        
        # Handle cases where inputs might be packed in a dictionary (e.g., from Learner)
        # or passed as keyword arguments directly.
        batch_dict = {}
        if len(args) == 1 and isinstance(args[0], dict):
            batch_dict = args[0]
        elif len(args) > 0: # Attempt to map positional args if not a dict; less robust
            # This path is less ideal; prefer keyword args or a single dict
            if pixel_values is None and isinstance(args[0], torch.Tensor): pixel_values = args[0]
            if input_ids is None and len(args) > 1 and isinstance(args[1], torch.Tensor): input_ids = args[1]
            if attention_mask is None and len(args) > 2 and (args[2] is None or isinstance(args[2], torch.Tensor)): attention_mask = args[2]
            if labels is None and len(args) > 3 and (args[3] is None or isinstance(args[3], torch.Tensor)): labels = args[3]

        # Update from kwargs if they were provided
        if 'pixel_values' in kwargs: pixel_values = kwargs['pixel_values']
        if 'input_ids' in kwargs: input_ids = kwargs['input_ids']
        if 'attention_mask' in kwargs: attention_mask = kwargs['attention_mask']
        if 'labels' in kwargs: labels = kwargs['labels']
        
        # If batch_dict was populated, use its values preferentially or if others are None
        if batch_dict:
            pixel_values = batch_dict.get('pixel_values', pixel_values)
            input_ids = batch_dict.get('input_ids', input_ids)
            attention_mask = batch_dict.get('attention_mask', attention_mask)
            labels = batch_dict.get('labels', labels)

        if pixel_values is None or input_ids is None:
            # This error will be caught by the Learner.summary() or training loop if inputs are malformed.
            raise ValueError("forward() missing required arguments: pixel_values and input_ids must be provided.")

        if self.language_model is None or self.vision_tower is None or self.projector is None:
            raise RuntimeError("Model components (LLM, Vision Tower, Projector) are not fully loaded.")

        image_features = self.encode_image(pixel_values)
        if image_features is None:
            raise RuntimeError("Image encoding failed.")
        
        projector_device = next(self.projector.parameters()).device
        image_features = image_features.to(projector_device)
        projected_image_features = self.projector(image_features)
        num_image_patches = projected_image_features.shape[1]
        
        embedding_layer = self.get_input_embeddings()
        target_device = embedding_layer.weight.device
        
        input_ids_clone = input_ids.clone().to(target_device) 
        input_ids_clone[input_ids_clone == self.image_token_index_marker] = 0 
        
        text_embeddings = embedding_layer(input_ids_clone)
        projected_image_features = projected_image_features.to(target_device, dtype=text_embeddings.dtype)

        new_input_embeds = []
        new_labels = [] if labels is not None else None
        new_attention_mask = []

        for batch_idx in range(input_ids.shape[0]):
            current_input_ids_slice = input_ids[batch_idx].to(target_device)
            image_token_indices = torch.where(current_input_ids_slice == self.image_token_index_marker)[0]
            
            if len(image_token_indices) == 0:
                warnings.warn(f"Image token placeholder {self.image_token_index_marker} not found in batch index {batch_idx}. Skipping image features.")
                new_input_embeds.append(text_embeddings[batch_idx])
                current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()
                new_attention_mask.append(current_attention_mask_slice)
                if new_labels is not None and labels is not None:
                    new_labels.append(labels[batch_idx].to(target_device))
                continue

            image_token_start_index = image_token_indices[0].item()
            text_emb_before = text_embeddings[batch_idx, :image_token_start_index]
            text_emb_after = text_embeddings[batch_idx, image_token_start_index + 1:]

            cur_new_embed = torch.cat([
                text_emb_before,
                projected_image_features[batch_idx], 
                text_emb_after
            ], dim=0)
            new_input_embeds.append(cur_new_embed)

            current_attention_mask_slice = attention_mask[batch_idx].to(target_device) if attention_mask is not None else (current_input_ids_slice != tokenizer.pad_token_id).long()
            mask_before = current_attention_mask_slice[:image_token_start_index]
            mask_image = torch.ones(num_image_patches, dtype=torch.long, device=target_device)
            mask_after = current_attention_mask_slice[image_token_start_index + 1:]
            cur_new_mask = torch.cat([mask_before, mask_image, mask_after], dim=0)
            new_attention_mask.append(cur_new_mask)

            if new_labels is not None and labels is not None:
                current_labels_slice = labels[batch_idx].to(target_device)
                label_before = current_labels_slice[:image_token_start_index]
                label_image = torch.full((num_image_patches,), self.ignore_index, dtype=torch.long, device=target_device)
                label_after = current_labels_slice[image_token_start_index + 1:]
                cur_new_label = torch.cat([label_before, label_image, label_after], dim=0)
                new_labels.append(cur_new_label)

        padded_input_embeds = pad_sequence(new_input_embeds, batch_first=True, padding_value=0.0)
        padded_attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)
        padded_labels = None
        if new_labels is not None:
            padded_labels = pad_sequence(new_labels, batch_first=True, padding_value=self.ignore_index)
        
        outputs: CausalLMOutputWithPast = self.language_model(
            inputs_embeds=padded_input_embeds, 
            attention_mask=padded_attention_mask, 
            labels=padded_labels, 
            return_dict=True
        )
        return outputs
