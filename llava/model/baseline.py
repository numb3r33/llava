"""Defines the baseline LLaVA 1.5 model architecture, including the projector and the combined model structure."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/20_model_baseline.ipynb.

# %% auto 0
__all__ = ['project_root', 'project_root_str', 'LLaVAProjector', 'BaselineLLaVAModel']

# %% ../../nbs/20_model_baseline.ipynb 3
import sys
from pathlib import Path
import os

# Assumes the notebook is run from the project root or one level down (e.g., nbs/)
# Navigate up to the project root (where settings.ini or .git likely exists)
project_root = Path(os.getcwd())
# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level
if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():
    project_root = project_root.parent

project_root_str = str(project_root.resolve())

if project_root_str not in sys.path:
    print(f"Adding project root to sys.path: {project_root_str}")
    sys.path.insert(0, project_root_str)
else:
    print(f"Project root already in sys.path: {project_root_str}")

# %% ../../nbs/20_model_baseline.ipynb 4
import torch
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence # For padding variable length sequences
from transformers import CLIPVisionModel, AutoModelForCausalLM, AutoConfig
from transformers.modeling_outputs import BaseModelOutputWithPooling, CausalLMOutputWithPast # For type hints
import warnings
from typing import Optional # Added for type hinting

try:
    from peft import LoraConfig, get_peft_model, PeftModel # Import PEFT components
    _peft_available = True
except ImportError:
    print("Warning: peft library not found. LoRA functionality will be disabled.")
    LoraConfig, get_peft_model, PeftModel = None, None, None # Define as None if not available
    _peft_available = False

from ..utils import load_config # Assuming utils notebook is created
from ..data.preprocessing import IMAGE_TOKEN_INDEX_PLACEHOLDER, IGNORE_INDEX, tokenizer, DEFAULT_IMAGE_TOKEN # Import constants and tokenizer

# Ensure tokenizer is loaded (it should be from the import)
if tokenizer is None:
    print("Warning: Tokenizer could not be imported from data.preprocessing. Trying to load it again...")
    try:
        # Need to load config first to get LLM name
        _config_temp = load_config('configs/config.yaml')
        _llm_name_temp = _config_temp.get('model', {}).get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')
        _tokenizer_max_len_temp = _config_temp.get('data', {}).get('tokenizer_model_max_length', 2048)
        _tokenizer_padding_side_temp = _config_temp.get('data', {}).get('tokenizer_padding_side', 'right')

        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            _llm_name_temp,
            model_max_length=_tokenizer_max_len_temp,
            padding_side=_tokenizer_padding_side_temp,
            use_fast=True,
        )
        # Re-add special token logic if necessary (should match preprocessing step)
        # from llava.data.preprocessing import DEFAULT_IMAGE_TOKEN # Already imported
        if DEFAULT_IMAGE_TOKEN not in tokenizer.get_vocab():
             num_added = tokenizer.add_special_tokens({'additional_special_tokens': [DEFAULT_IMAGE_TOKEN]})
             print(f"Added {num_added} token(s) during re-load. New vocab size: {len(tokenizer)}")
        if tokenizer.pad_token is None:
             tokenizer.pad_token = tokenizer.eos_token
             print(f"Set pad token to EOS during re-load.")
        print(f"Tokenizer re-loaded successfully for {_llm_name_temp}.")

    except Exception as e:
        print(f"Fatal Error: Could not load tokenizer. {e}")
        # Depending on the context, might raise an error or exit
        raise ImportError("Tokenizer is essential and could not be loaded.") from e

# %% ../../nbs/20_model_baseline.ipynb 7
class LLaVAProjector(nn.Module):
    """A simple 2-layer MLP projector to map vision features to LLM embedding space.

    Maps input_dim (e.g., CLIP ViT-L output dim = 1024) to
    output_dim (e.g., Vicuna-7B hidden dim = 4096).

    Architecture: Linear -> GELU -> Linear
    """
    def __init__(self, input_dim: int, output_dim: int):
        """Initializes the MLP projector.

        Args:
            input_dim: Dimension of the input vision features.
            output_dim: Dimension of the LLM's hidden space.
        """
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, output_dim, bias=True),
            nn.GELU(),
            nn.Linear(output_dim, output_dim, bias=True)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Projects the input features.

        Args:
            x: Input tensor of shape (batch_size, num_patches, input_dim) or similar.

        Returns:
            Projected tensor of shape (batch_size, num_patches, output_dim).
        """
        return self.model(x)

# %% ../../nbs/20_model_baseline.ipynb 13
class BaselineLLaVAModel(nn.Module):
    """Baseline LLaVA 1.5 model combining Vision Encoder, Projector, and LLM.

    Loads the specified Vision Encoder (CLIP) and LLM (Vicuna) from Hugging Face Hub,
    along with the Projector. Handles freezing components, LLM embedding resizing,
    and optionally applies LoRA to the LLM based on the configuration.
    Implements `encode_image`, activation checkpointing, and the main `forward` pass.
    """
    def __init__(self, config: dict):
        """Initializes the Baseline LLaVA model structure.

        Loads Vision Encoder, LLM, and Projector based on the config.
        Resizes LLM embeddings to match the tokenizer vocabulary size.
        Optionally applies LoRA to the LLM.
        Optionally applies activation checkpointing to the LLM.

        Args:
            config: Dictionary containing model configuration, including paths/names
                    for vision encoder, LLM, projector dimensions, LoRA settings, checkpointing, etc.
        """
        super().__init__()
        self.config = config 
        self.model_config = config.get('model', {}) 

        self.vision_tower = None
        self.language_model = None
        self.projector = None

        self.image_token_index_marker = self.model_config.get('image_token_index_marker', IMAGE_TOKEN_INDEX_PLACEHOLDER)
        self.ignore_index = IGNORE_INDEX 
        self.vision_feature_layer = self.model_config.get('vision_feature_layer', -2)
        self.vision_encoder_name = self.model_config.get('vision_encoder_name_or_path', 'openai/clip-vit-large-patch14-336')
        self.llm_name = self.model_config.get('llm_name_or_path', 'lmsys/vicuna-7b-v1.5')

        llm_hf_config = AutoConfig.from_pretrained(self.llm_name)
        vision_hf_config = AutoConfig.from_pretrained(self.vision_encoder_name)

        projector_cfg_from_yaml = self.model_config.get('projector', {})
        
        # Corrected: Access vision_config.hidden_size from CLIPConfig
        proj_input_dim_fallback = vision_hf_config.vision_config.hidden_size if hasattr(vision_hf_config, 'vision_config') else vision_hf_config.hidden_size
        proj_input_dim = projector_cfg_from_yaml.get('input_dim', proj_input_dim_fallback)
        proj_output_dim = projector_cfg_from_yaml.get('output_dim', llm_hf_config.hidden_size)

        print(f"Initializing Projector: Input Dim={proj_input_dim}, Output Dim={proj_output_dim}")
        self.projector = LLaVAProjector(proj_input_dim, proj_output_dim)

        self.load_vision_tower(expected_output_dim=proj_input_dim)
        self.load_language_model()
        self.resize_llm_embeddings()
        self.apply_activation_checkpointing()

    def load_vision_tower(self, expected_output_dim: int):
        """Loads the CLIP Vision Model based on the configuration."""
        try:
            print(f"Loading Vision Tower: {self.vision_encoder_name}...")
            self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_encoder_name)
            print(f"Vision Tower loaded successfully.")
            self.vision_tower.requires_grad_(False)
            print(f"Vision Tower weights frozen.")

            # For CLIPVisionModel, the relevant config is self.vision_tower.config
            vision_output_dim = self.vision_tower.config.hidden_size
            if vision_output_dim != expected_output_dim:
                warnings.warn(
                    f"Vision Tower output dimension ({vision_output_dim}) does not match "
                    f"Projector input dimension ({expected_output_dim}). Check vision model and config['model']['projector']['input_dim']."
                )
        except Exception as e:
            print(f"Error loading Vision Tower ({self.vision_encoder_name}): {e}")
            self.vision_tower = None

    def load_language_model(self):
        """Loads the Language Model and optionally applies LoRA based on the configuration."""
        try:
            print(f"Loading Language Model: {self.llm_name}...")
            self.language_model = AutoModelForCausalLM.from_pretrained(self.llm_name)
            print(f"Language Model loaded successfully.")
            self.language_model.requires_grad_(False)
            print(f"Base Language Model weights frozen.")

            peft_config = self.model_config.get('peft', {})
            use_lora = peft_config.get('use_lora', False)

            if use_lora:
                if not _peft_available:
                    print("Warning: `use_lora` is true in config, but peft library is not installed. Skipping LoRA.")
                else:
                    print("Applying LoRA...")
                    lora_r = peft_config.get('lora_r', 8)
                    lora_alpha = peft_config.get('lora_alpha', 16)
                    lora_dropout = peft_config.get('lora_dropout', 0.05)
                    target_modules = peft_config.get('target_modules', ['q_proj', 'v_proj'])
                    
                    lora_config_obj = LoraConfig( # Renamed to avoid conflict
                        r=lora_r,
                        lora_alpha=lora_alpha,
                        target_modules=target_modules,
                        lora_dropout=lora_dropout,
                        bias="none",
                    )
                    
                    self.language_model = get_peft_model(self.language_model, lora_config_obj)
                    print(f"LoRA applied. Config: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}, modules={target_modules}")
                    self.language_model.print_trainable_parameters()
            else:
                print("LoRA is disabled in the configuration.")
        except Exception as e:
            print(f"Error loading Language Model ({self.llm_name}): {e}")
            self.language_model = None

    def resize_llm_embeddings(self):
        """Resizes the LLM's token embeddings to match the tokenizer size."""
        if self.language_model is None or tokenizer is None:
            print("Warning: Cannot resize LLM embeddings. LLM or tokenizer not available.")
            return

        current_embeddings = self.get_input_embeddings()
        current_vocab_size = current_embeddings.weight.size(0)
        target_vocab_size = len(tokenizer)

        if current_vocab_size != target_vocab_size:
            print(f"Resizing LLM token embeddings from {current_vocab_size} to {target_vocab_size} (tokenizer size)...")
            self.language_model.resize_token_embeddings(target_vocab_size)
            print("LLM token embeddings resized.")
        else:
            print("LLM embedding size already matches tokenizer size. No resizing needed.")
            
    def apply_activation_checkpointing(self):
        """Applies gradient checkpointing if enabled in the config."""
        use_checkpointing = self.model_config.get('use_activation_checkpointing', False)
        if use_checkpointing:
            if self.language_model is not None:
                 model_to_checkpoint = self.language_model 
                 if hasattr(model_to_checkpoint, 'gradient_checkpointing_enable'):
                     try:
                         model_to_checkpoint.gradient_checkpointing_enable()
                         print("Activation checkpointing enabled for the language model.")
                     except Exception as e:
                         print(f"Warning: Failed to enable activation checkpointing: {e}")
                 else:
                     print("Warning: Language model does not have 'gradient_checkpointing_enable' method.")
            else:
                 print("Warning: Activation checkpointing enabled in config, but language model is not loaded.")
        else:
             print("Activation checkpointing is disabled in the configuration.")

    def get_input_embeddings(self) -> nn.Embedding:
        """Helper to get the LLM's input embedding layer."""
        if self.language_model is None:
            raise ValueError("Language model not loaded.")
        return self.language_model.get_input_embeddings()

    def encode_image(self, pixel_values: torch.Tensor) -> torch.Tensor | None:
        """Encodes images using the vision tower and extracts features from the specified layer."""
        if self.vision_tower is None:
            print("Error: Vision tower not loaded, cannot encode image.")
            return None
        try:
            device = self.vision_tower.device
            pixel_values = pixel_values.to(device, dtype=next(self.vision_tower.parameters()).dtype) # Match vision tower dtype

            vision_outputs: BaseModelOutputWithPooling = self.vision_tower(
                pixel_values,
                output_hidden_states=True
            )
            image_features = vision_outputs.hidden_states[self.vision_feature_layer]
            image_features = image_features[:, 1:, :] 
            return image_features
        except Exception as e:
            print(f"Error during image encoding: {e}")
            import traceback
            traceback.print_exc()
            return None

    def forward(self,
                pixel_values: torch.Tensor,
                input_ids: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None, 
                labels: Optional[torch.Tensor] = None 
               ) -> CausalLMOutputWithPast:
        if self.language_model is None or self.vision_tower is None or self.projector is None:
            raise RuntimeError("Model components (LLM, Vision Tower, Projector) are not fully loaded.")

        image_features = self.encode_image(pixel_values)
        if image_features is None:
            raise RuntimeError("Image encoding failed.")
        projected_image_features = self.projector(image_features)
        num_image_patches = projected_image_features.shape[1]

        input_ids_clone = input_ids.clone()
        input_ids_clone[input_ids_clone == self.image_token_index_marker] = 0 
        text_embeddings = self.get_input_embeddings()(input_ids_clone)

        new_input_embeds = []
        new_labels = [] if labels is not None else None
        new_attention_mask = []

        for batch_idx in range(input_ids.shape[0]):
            image_token_indices = torch.where(input_ids[batch_idx] == self.image_token_index_marker)[0]
            if len(image_token_indices) == 0:
                warnings.warn(f"Image token placeholder {self.image_token_index_marker} not found in batch index {batch_idx}. Skipping image features.")
                new_input_embeds.append(text_embeddings[batch_idx])
                current_attention_mask = attention_mask[batch_idx] if attention_mask is not None else (input_ids[batch_idx] != tokenizer.pad_token_id).long()
                new_attention_mask.append(current_attention_mask)
                if new_labels is not None and labels is not None:
                    new_labels.append(labels[batch_idx])
                continue

            image_token_start_index = image_token_indices[0].item()
            text_emb_before = text_embeddings[batch_idx, :image_token_start_index]
            text_emb_after = text_embeddings[batch_idx, image_token_start_index + 1:]

            cur_new_embed = torch.cat([
                text_emb_before,
                projected_image_features[batch_idx].to(text_embeddings.device, dtype=text_embeddings.dtype),
                text_emb_after
            ], dim=0)
            new_input_embeds.append(cur_new_embed)

            current_attention_mask = attention_mask[batch_idx] if attention_mask is not None else (input_ids[batch_idx] != tokenizer.pad_token_id).long()
            mask_before = current_attention_mask[:image_token_start_index]
            mask_image = torch.ones(num_image_patches, dtype=torch.long, device=current_attention_mask.device)
            mask_after = current_attention_mask[image_token_start_index + 1:]
            cur_new_mask = torch.cat([mask_before, mask_image, mask_after], dim=0)
            new_attention_mask.append(cur_new_mask)

            if new_labels is not None and labels is not None:
                label_before = labels[batch_idx, :image_token_start_index]
                label_image = torch.full((num_image_patches,), self.ignore_index, dtype=torch.long, device=labels.device)
                label_after = labels[batch_idx, image_token_start_index + 1:]
                cur_new_label = torch.cat([label_before, label_image, label_after], dim=0)
                new_labels.append(cur_new_label)

        padded_input_embeds = pad_sequence(new_input_embeds, batch_first=True, padding_value=0.0)
        padded_attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)
        padded_labels = None
        if new_labels is not None:
            padded_labels = pad_sequence(new_labels, batch_first=True, padding_value=self.ignore_index)

        outputs: CausalLMOutputWithPast = self.language_model(
            inputs_embeds=padded_input_embeds,
            attention_mask=padded_attention_mask,
            labels=padded_labels,
            return_dict=True
        )
        return outputs
