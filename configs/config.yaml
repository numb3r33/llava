# Configuration for Adaptive Patching VIT project

project_name: llava
version: 1.0

# --- Paths ---
paths:
  # Base directory where datasets are stored
  data_base: /workspace/llava/data/  # TODO: Update this path
  # Directory to save outputs like trained models, logs, predictions
  output_dir: /workspace/llava/output/ # TODO: Update this path

  # Specific dataset paths (relative to data_base or absolute)
  # Stage 1: Projector Pre-training Data (e.g., CC3M 595K subset)
  stage1_data: llava_pretrain/llava_pretrain.jsonl # Example JSON Lines file for metadata
  stage1_images: llava_pretrain/images # Example directory for images

  # Stage 2: Instruction Fine-tuning Data (e.g., LLaVA-Instruct-150K)
  stage2_data: llava_instruct_150k/llava_v1_5_mix665k.jsonl # Example JSON Lines file
  stage2_images: llava_instruct_150k/images # Base dir for images referenced in stage 2 data (can be multiple sources, resolved via path in JSONL)

  # Evaluation dataset paths (relative to data_base or absolute)
  # Keys should match dataset_name used in evaluation scripts
  vqav2_test:
    annotations: vqav2/v2_mscoco_test2015_annotations.json # Placeholder VQAv2 test annotations
    images: vqav2/test2015 # Placeholder VQAv2 test images path
    # Add questions file path if needed for evaluation script
    questions: vqav2/v2_OpenEnded_mscoco_test2015_questions.json
  textvqa_val:
    annotations: textvqa/TextVQA_0.5.1_val.json # Placeholder TextVQA val annotations
    images: textvqa/train_images # Placeholder TextVQA val images path (often same as train)

  # Custom Evaluation Set (New in Step 7.4)
  custom_eval:
    annotations: custom_eval/annotations.jsonl # TODO: Update path to custom eval annotations
    images: custom_eval/images # TODO: Update path to custom eval images

  # Example entries for other potential datasets (placeholders)
  # docvqa_val:
  #   annotations: docvqa/val_v1.0.json
  #   images: docvqa/val_images
  # chartqa_test:
  #   annotations: chartqa/test_human.json
  #   images: chartqa/test/png

  # Saved model weights paths (relative to output_dir/models or absolute)
  # stage1_projector_weights defines the *input* weights for stage 2
  stage1_projector_weights: stage1_projector.pth # Filename for stage 1 projector weights
  # stage2_model_weights defines the *output* filename for the stage 2 learner state or base name for adapters
  stage2_model_weights: stage2_llava_lora # Base name for saved stage 2 model/adapters

# --- Model Configuration ---
model:
  # Base LLM identifier from Hugging Face Hub
  llm_name_or_path: lmsys/vicuna-7b-v1.5
  # Base Vision Encoder identifier from Hugging Face Hub
  vision_encoder_name_or_path: openai/clip-vit-large-patch14-336
  # Layer index from vision encoder to extract features (LLaVA 1.5 uses -2)
  vision_feature_layer: -2
  # Image token placeholder used in text templates before replacement
  image_token: "<image>"
  # Special token index used internally to mark image feature insertion points
  image_token_index_marker: -200 # Marker used in input_ids before embedding replacement

  # Projector configuration
  projector:
    type: mlp_2x # Simple 2-layer MLP
    # Input dim should match vision encoder output dim (e.g., 1024 for ViT-L)
    input_dim: 1024
    # Output dim should match LLM hidden dim (e.g., 4096 for Vicuna-7B)
    output_dim: 4096

  quantization:
    load_in_4bit: true
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: "float16" # float16 is safer for older GPUs
    bnb_4bit_use_double_quant: false

  # PEFT (LoRA) configuration (for Stage 2) - Step 4.2
  peft:
    use_lora: true          # Enable/disable LoRA for Stage 2 Fine-tuning
    lora_r: 8               # LoRA rank (e.g., 8, 16, 32, 64) - higher rank = more params, potentially better perf.
    lora_alpha: 16          # LoRA scaling factor (often 2*r)
    lora_dropout: 0.05      # Dropout probability for LoRA layers
    # Modules to apply LoRA to. Check the LLM architecture (e.g., Vicuna-7B) for valid module names.
    # Common targets for attention mechanisms:
    target_modules: ["q_proj", "v_proj"]
    # You might also target "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj" depending on the model and experimental results.
    # bias: "none"            # Typically set to "none" for LoRA
    # task_type: "CAUSAL_LM"  # Specify if required by peft version

  # Adaptive Patcher configuration - Step 6.2
  adaptive_patcher:
    enabled: true # Set to true to use the adaptive model
    strategy: 'variable_resolution' # Strategy implemented in Step 6.2
    # Strategy-specific parameters
    image_grid_pinpoints: [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]] # Default from LLaVA-NeXT

  # Vision config details needed by patcher (shared with base model)
  vision_config:
    image_size: 336 # Base image size for patcher reference
    patch_size: 14  # Vision model patch size

  # Memory optimization - Step 4.4
  use_activation_checkpointing: false # Enable gradient checkpointing for LLM (Default to False, enable if needed)

# --- Data Handling ---
data:
  # Image preprocessing parameters
  image_size: 336 # Standard resize target for baseline/stage1/stage2
  image_aspect_ratio_padding: pad # Method for resizing/padding ('pad' is common)
  # Use CLIP's processor for stats by default
  image_mean: [0.48145466, 0.4578275, 0.40821073]
  image_std: [0.26862954, 0.26130258, 0.27577711]

  # Text processing parameters
  tokenizer_padding_side: right
  tokenizer_model_max_length: 2048 # Max sequence length for tokenizer

  # Stage 1 data template
  stage1_text_template: plain # Special template for projector pre-training

  # Stage 2 data template (matches LLM fine-tuning)
  stage2_text_template: v1 # Vicuna v1 chat template

  # Dataloader parameters
  batch_size_per_device_stage1: 2 # Adjust based on VRAM
  batch_size_per_device_stage2: 4 # Adjust based on VRAM
  num_workers: 16

# --- Training Parameters ---
training:
  # General
  seed: 42
  num_epochs_stage1: 1      # Reduce for faster runs during dev
  num_epochs_stage2: 1      # Reduce for faster runs during dev
  learning_rate_stage1: 1e-4 # For projector in Stage 1
  learning_rate_stage2: 2e-5 # For projector & LoRA adapters in Stage 2 (typically lower)
  weight_decay: 0.
  optimizer: AdamW
  scheduler: cosine # Learning rate scheduler type
  warmup_ratio: 0.03 # Ratio of steps for linear warmup

  # Optimization
  gradient_accumulation_steps: 4 # Accumulate gradients over N steps
  use_mixed_precision: true # Use float16/bfloat16 (if supported)

  # Stage 1 Specifics
  freeze_vision_encoder_stage1: true
  freeze_llm_stage1: true

  # Stage 2 Specifics
  freeze_vision_encoder_stage2: true # Default LLaVA 1.5 keeps vision frozen

  # Saving checkpoints
  save_strategy: "epoch" # Or "steps"
  save_total_limit: 1 # Keep only the last checkpoint

# --- Evaluation Parameters ---
evaluation:
  eval_batch_size_per_device: 2 # Batch size for evaluation runs
  # Add specific benchmark evaluation settings if needed
  generation_max_length: 200 # Max tokens for generate_predictions
  generation_temperature: 0.2
  generation_top_p: null # Set to e.g. 0.9 for nucleus sampling

# --- Logging ---
logging:
  wandb:
    enabled: false # Disable W&B by default for local testing, enable for actual runs
    project: llava-adaptive-patching-fastai # TODO: Update W&B project name
    entity: numb3r33 # TODO: Update W&B entity (username or team) - Leave blank or null to use default entity
    log_model: false # Whether to log model checkpoints as W&B artifacts ("checkpoint" or false)
    run_name_prefix: llava # Prefix for generated run names
    # Add run tags/notes templates if desired

# --- Ablation Settings (New for Step 8.1) ---
ablation:
  # Set to 'baseline' to force VariableResolutionPatcher to always output 336x336 grid info.
  # Set to null or remove to allow normal variable resolution behavior.
  force_patcher_strategy: null # Options: null, 'baseline'

# --- Environment ---
environment:
  # Set to true if using DeepSpeed (lower priority for single GPU)
  use_deepspeed: false
  # Add DeepSpeed config path if used