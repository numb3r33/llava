# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/32_training_stage2.ipynb.

# %% auto 0
__all__ = ['llava_stage2_splitter', 'get_stage2_learner', 'train_stage2']

# %% ../nbs/32_training_stage2.ipynb 3
import sys
from pathlib import Path
import os
import gc # For memory cleanup
import argparse # For command-line execution

# Assumes the notebook is run from the project root or one level down (e.g., nbs/)
# Navigate up to the project root (where settings.ini or .git likely exists)
project_root = Path(os.getcwd())
# Simple check: If settings.ini is not in cwd, assume we are in nbs/ and go up one level
if not (project_root / 'settings.ini').exists() and (project_root.parent / 'settings.ini').exists():
    project_root = project_root.parent

# If running as a script, the path might need adjustment relative to the script location
# This assumes the script is run from the project root or `scripts/` directory
if __name__ == "__main__" and "get_ipython" not in locals():
     # If script is in 'scripts/', go up one level to project root
     if project_root.name == 'scripts':
         project_root = project_root.parent

project_root_str = str(project_root.resolve())

if project_root_str not in sys.path:
    print(f"Adding project root to sys.path: {project_root_str}")
    sys.path.insert(0, project_root_str)
else:
    # print(f"Project root already in sys.path: {project_root_str}") # Less verbose for script
    pass

# %% ../nbs/32_training_stage2.ipynb 4
import torch
import torch.nn as nn
from fastai.learner import Learner
from fastai.optimizer import AdamW
from fastai.callback.wandb import WandbCallback
from fastai.callback.schedule import fit_one_cycle
from fastai.callback.save import SaveModelCallback
from fastai.callback.training import GradientAccumulation
from fastai.callback.fp16 import MixedPrecision
from fastai.vision.all import params # For splitter
from fastai.data.core import DataLoaders
from functools import partial
import wandb # Import wandb directly for cleanup
import json # For dummy data creation
import PIL.Image # For dummy data creation
from typing import List, Optional

# Attempt to import peft, set flag
try:
    from peft import PeftModel
    _peft_available = True
except ImportError:
    print("Warning: peft library not found. LoRA functionality will be disabled.")
    PeftModel = None # Define as None if not available
    _peft_available = False

try:
    from llava.utils import load_config, init_wandb
    from llava.data.loading import get_stage2_dataloaders
    from llava.model.baseline import BaselineLLaVAModel
    from llava.training.core import LLaVALoss
except ImportError as e:
     print(f"Error importing llava modules: {e}")
     print("Ensure that nbdev_export has been run and the llava library is installed/accessible.")
     # In a script context, it's better to exit if core modules are missing
     if __name__ == "__main__" and "get_ipython" not in locals():
          sys.exit(1)

# %% ../nbs/32_training_stage2.ipynb 5
def llava_stage2_splitter(model: BaselineLLaVAModel):
    """Splits the `BaselineLLaVAModel` parameters for Stage 2 training.

    Trains the `projector` and LoRA adapters (if enabled) or the full `language_model`.
    Keeps the `vision_tower` frozen by default.

    Args:
        model: An instance of `BaselineLLaVAModel`.

    Returns:
        A list containing parameter groups for trainable components.
    """
    projector_params = []
    llm_params = []
    frozen_params = []

    print("Applying Stage 2 splitter...")

    # Projector parameters are always trained in Stage 2
    if hasattr(model, 'projector') and model.projector is not None:
        print("  - Collecting projector parameters (trainable).")
        projector_params.extend(list(model.projector.parameters()))
        for p in model.projector.parameters():
             p.requires_grad = True
    else:
        print("Warning: Model has no projector attribute.")

    # Handle Language Model parameters based on LoRA configuration
    if hasattr(model, 'language_model') and model.language_model is not None:
        # Access config stored within the model instance
        # Handle potential nested structure in config access more safely
        use_lora = model.config.get('model', {}).get('peft', {}).get('use_lora', False)

        if _peft_available and use_lora and isinstance(model.language_model, PeftModel):
            print("  - LoRA enabled: Collecting LLM adapter parameters (trainable).")
            # PEFT model automatically handles requires_grad for adapters
            llm_params.extend([p for p in model.language_model.parameters() if p.requires_grad])
            # Base model parameters should already be frozen by PEFT
            # We can double-check frozen status for verification
            # frozen_params.extend([p for p in model.language_model.parameters() if not p.requires_grad])
        elif use_lora and not _peft_available:
             print("Warning: LoRA configured but PEFT library not found. Cannot train LoRA adapters. Freezing LLM.")
             for p in model.language_model.parameters():
                 p.requires_grad = False
             # frozen_params.extend(list(model.language_model.parameters()))
        else:
            print("  - LoRA disabled: Collecting all LLM parameters (trainable).")
            llm_params.extend(list(model.language_model.parameters()))
            for p in model.language_model.parameters():
                 p.requires_grad = True # Ensure all LLM params are trainable if not using LoRA
    else:
        print("Warning: Model has no language_model attribute.")

    # Vision Tower parameters are frozen by default
    if hasattr(model, 'vision_tower') and model.vision_tower is not None:
        print("  - Collecting vision tower parameters (frozen).")
        # frozen_params.extend(list(model.vision_tower.parameters())) # Collect if needed for verification
        for p in model.vision_tower.parameters():
            p.requires_grad = False
    else:
        print("Warning: Model has no vision_tower attribute.")

    # Combine trainable parameters into one group for the optimizer
    trainable_groups = projector_params + llm_params
    # Count frozen parameters for verification
    frozen_count = sum(p.numel() for p in model.parameters() if not p.requires_grad)
    trainable_count = sum(p.numel() for p in trainable_groups)
    print(f"Splitter created groups: Trainable ({trainable_count} params), Frozen ({frozen_count} params)")
    
    if not trainable_groups:
         raise ValueError("Splitter function resulted in no trainable parameters. Check model structure and config.")
         
    return [trainable_groups]

# %% ../nbs/32_training_stage2.ipynb 7
def get_stage2_learner(config: dict) -> Learner:
    """Configures and returns a fastai Learner for Stage 2 Instruction Fine-tuning.

    Loads Stage 1 projector weights, sets up the model (potentially with LoRA),
    uses the Stage 2 splitter, and includes relevant callbacks.

    Args:
        config: The main configuration dictionary.

    Returns:
        A configured fastai Learner instance for Stage 2.

    Raises:
        RuntimeError: If DataLoaders or Model instantiation fails.
        FileNotFoundError: If Stage 1 projector weights or data paths are invalid.
        AttributeError: If the model is missing expected components.
    """
    print("--- Setting up Stage 2 Learner ---")
    output_dir = Path(config['paths']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)

    # 1. Load Stage 2 DataLoaders
    print("Loading Stage 2 DataLoaders...")
    try:
        dls = get_stage2_dataloaders(config)
    except (FileNotFoundError, Exception) as e:
        print(f"Error loading Stage 2 DataLoaders: {e}")
        raise RuntimeError("Failed to create Stage 2 DataLoaders.") from e
    if not dls:
        raise RuntimeError("Stage 2 DataLoaders object is None.")
    print(f"DataLoaders loaded. Train samples: {len(dls.train_ds)}, Valid samples: {len(dls.valid_ds)}")

    # 2. Instantiate Model (handles LoRA based on config)
    print("Instantiating BaselineLLaVAModel for Stage 2...")
    try:
        model = BaselineLLaVAModel(config)
        if model.vision_tower is None or model.language_model is None or model.projector is None:
            raise RuntimeError("BaselineLLaVAModel initialization incomplete.")
        print("Model instantiated successfully.")
    except Exception as e:
        print(f"Error instantiating BaselineLLaVAModel: {e}")
        raise RuntimeError("Failed to instantiate baseline model for Stage 2.") from e

    # 3. Load Stage 1 Projector Weights
    stage1_weights_fname = config['paths'].get('stage1_projector_weights', 'stage1_projector.pth')
    # Look for weights relative to output_dir/models/
    stage1_weights_path = output_dir / 'models' / stage1_weights_fname
    print(f"Attempting to load Stage 1 projector weights from: {stage1_weights_path}")
    if stage1_weights_path.is_file():
        try:
            # Load state dict onto CPU first to avoid device mismatches
            projector_state_dict = torch.load(stage1_weights_path, map_location='cpu')
            # Load into the model's projector
            model.projector.load_state_dict(projector_state_dict)
            print(f"Successfully loaded Stage 1 projector weights from {stage1_weights_path}")
        except Exception as e:
            print(f"Error loading Stage 1 projector weights: {e}")
            # Decide whether to raise error or continue with random init projector
            # For reproducibility, it's usually better to raise if weights are expected.
            raise RuntimeError(f"Failed to load expected Stage 1 projector weights from {stage1_weights_path}") from e
    else:
        print(f"Warning: Stage 1 projector weights not found at {stage1_weights_path}. Projector will use initial weights.")
        # Optionally raise an error if pre-trained weights are strictly required
        # raise FileNotFoundError(f"Stage 1 projector weights not found: {stage1_weights_path}")

    # 4. Define Loss Function
    loss_func = LLaVALoss()
    print(f"Loss function: {type(loss_func).__name__}")

    # 5. Define Optimizer
    lr = config.get('training', {}).get('learning_rate_stage2', 2e-5) # Lower LR for fine-tuning
    wd = config.get('training', {}).get('weight_decay', 0.0)
    opt_func = partial(AdamW, lr=lr, wd=wd, eps=1e-8)
    print(f"Optimizer: AdamW (lr={lr}, wd={wd})")

    # 6. Define Splitter
    splitter = llava_stage2_splitter
    print(f"Parameter splitter: {splitter.__name__}")

    # 7. Define Callbacks
    cbs = []
    if config.get('logging', {}).get('wandb', {}).get('enabled', False):
        # Init W&B Run only if enabled and entity is likely configured
        wandb_entity = config.get('logging', {}).get('wandb', {}).get('entity')
        if wandb_entity and 'your_wandb_entity' not in str(wandb_entity):
            project_name = config.get('logging', {}).get('wandb', {}).get('project', 'llava-adaptive-patching')
            run_name_prefix = config.get('logging', {}).get('wandb', {}).get('run_name_prefix', 'stage2')
            stage2_model_name = Path(config['paths']['stage2_model_weights']).stem
            run_name = f"{run_name_prefix}_{stage2_model_name}_{wandb.util.generate_id()}"
            init_wandb(config, job_type="stage2-training", run_name=run_name)
            cbs.append(WandbCallback(log_preds=False, log_model=False))
            print("Added WandbCallback.")
        else:
            print("W&B enabled in config, but entity not set or default. Skipping W&B init and callback.")

    # SaveModelCallback for Stage 2 (saves best full model state)
    # Adapter saving might need custom handling in train loop or separate script (Step 4.5)
    stage2_model_fname = Path(config['paths']['stage2_model_weights']).stem
    save_cb = SaveModelCallback(
        monitor='valid_loss',
        min_delta=0.001,
        fname=stage2_model_fname, # Saves best model state
        every_epoch=False,
        with_opt=True, # Save optimizer state to resume training if needed
        reset_on_fit=True
    )
    # Note: SaveModelCallback saves the entire learner state, including the full model.
    # For LoRA, we'll manually save adapters at the end of training in train_stage2.
    # cbs.append(save_cb) # We might disable this if saving adapters manually is sufficient.
    print(f"SaveModelCallback is configured but commented out. Manual saving of adapters/projector in train_stage2 is preferred for LoRA.")

    # Optimization Callbacks
    grad_accum_steps = config.get('training', {}).get('gradient_accumulation_steps', 1)
    if grad_accum_steps > 1:
        cbs.append(GradientAccumulation(grad_accum_steps))
        print(f"Added GradientAccumulation callback with {grad_accum_steps} steps.")
    
    use_mixed_precision = config.get('training', {}).get('use_mixed_precision', False)
    if use_mixed_precision:
        cbs.append(MixedPrecision())
        print("Added MixedPrecision callback.")

    # 8. Create Learner
    try:
        learner = Learner(
            dls=dls,
            model=model,
            loss_func=loss_func,
            opt_func=opt_func,
            splitter=splitter,
            cbs=cbs,
            path=output_dir,
            train_bn=False # Typically False for LLM fine-tuning
        )
    except Exception as e:
        print(f"Error creating Stage 2 Learner: {e}")
        if wandb.run is not None:
            wandb.finish(exit_code=1)
            print("Finished W&B run due to error during Learner creation.")
        raise RuntimeError("Failed to create the Stage 2 Learner object.") from e

    print("--- Stage 2 Learner Setup Complete ---")
    return learner

# %% ../nbs/32_training_stage2.ipynb 9
def train_stage2(config_path: str | Path):
    """Loads config, sets up Stage 2 learner, runs training, and saves weights.
    
    Saves the projector weights and LoRA adapter weights (if used) separately.
    
    Args:
        config_path: Path to the YAML configuration file.
    """
    print(f"--- Starting Stage 2 Training --- ")
    print(f"Loading configuration from: {config_path}")
    config = load_config(config_path)
    output_dir = Path(config['paths']['output_dir'])
    models_dir = output_dir / 'models'
    models_dir.mkdir(parents=True, exist_ok=True)
    
    learner = None # Initialize learner to None for finally block
    try:
        # --- Get Learner (handles loading Stage 1 weights, LoRA setup, etc.) --- 
        learner = get_stage2_learner(config)
        
        # --- Start Training --- 
        epochs = config.get('training', {}).get('num_epochs_stage2', 3)
        lr = config.get('training', {}).get('learning_rate_stage2', 2e-5)
        print(f"Starting training for {epochs} epochs with max_lr={lr}...")
        
        learner.fit_one_cycle(epochs, lr_max=lr)
        
        print("Training finished.")
        
        # --- Save final trained weights --- 
        
        # 1. Save Projector Weights
        projector_save_path = models_dir / (Path(config['paths']['stage2_model_weights']).stem + "_projector_final.pth")
        print(f"Saving final projector weights to: {projector_save_path}")
        if hasattr(learner.model, 'projector') and learner.model.projector is not None:
             torch.save(learner.model.projector.state_dict(), projector_save_path)
             print("Projector weights saved.")
        else:
             print("Warning: Cannot save projector weights, model has no projector.")
             
        # 2. Save LoRA Adapters (if LoRA was used)
        # Check config AND if peft is available AND if model was actually wrapped
        use_lora_config = config.get('model', {}).get('peft', {}).get('use_lora', False)
        lora_applied = _peft_available and use_lora_config and isinstance(learner.model.language_model, PeftModel)

        if lora_applied:
            lora_save_dir = models_dir / (Path(config['paths']['stage2_model_weights']).stem + "_lora_adapters")
            print(f"Saving LoRA adapters to: {lora_save_dir}")
            try:
                # Ensure the directory exists
                lora_save_dir.mkdir(parents=True, exist_ok=True)
                # save_pretrained handles saving adapter_config.json and adapter_model.bin (or .safetensors)
                learner.model.language_model.save_pretrained(str(lora_save_dir)) 
                print("LoRA adapters saved successfully.")
            except Exception as e:
                 print(f"Error saving LoRA adapters: {e}")
                 import traceback
                 traceback.print_exc()
        elif not use_lora_config:
             print("LoRA was not enabled in config. Only projector weights saved explicitly.")
             # Note: The full model state might have been saved by SaveModelCallback if it was enabled.
        else: # LoRA enabled in config but PEFT not available or not applied
             print("LoRA configured but not applied (e.g., peft library issue?). Cannot save adapters.")
             

    except Exception as e:
        print(f"An error occurred during Stage 2 training: {e}")
        import traceback
        traceback.print_exc()
        # Potentially re-raise or handle cleanup
        raise e
    finally:
        # Clean up memory
        if learner is not None and hasattr(learner, 'model') and learner.model is not None:
            if hasattr(learner.model, 'vision_tower') and learner.model.vision_tower is not None:
                learner.model.vision_tower.to('cpu')
            if hasattr(learner.model, 'language_model') and learner.model.language_model is not None:
                learner.model.language_model.to('cpu')
            if hasattr(learner.model, 'projector') and learner.model.projector is not None:
                learner.model.projector.to('cpu')
            del learner.model
            learner.destroy() # Release learner resources
            del learner
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print("Cleaned up learner and model memory.")
            
        # Ensure W&B run is finished if it was started
        if wandb.run is not None:
            try:
                if wandb.run.id: # Check if run is still active
                    wandb.finish()
                    print("Finished W&B run.")
            except Exception as e:
                 print(f"Error finishing W&B run: {e}")
            
    print(f"--- Stage 2 Training Complete --- ")

# %% ../nbs/32_training_stage2.ipynb 11
# Command-line execution block for Stage 2
if __name__ == "__main__" and "get_ipython" not in locals():
    parser = argparse.ArgumentParser(description="Run LLaVA Stage 2 Training")
    # Assume script is run from project root
    parser.add_argument("--config", type=str, default="configs/config.yaml", 
                        help="Path to the configuration YAML file (relative to project root or absolute).")
    args = parser.parse_args()
    
    # Resolve config path relative to project root (defined earlier in the script's import section)
    config_file_path = project_root / args.config
    
    if not config_file_path.is_file():
        print(f"Error: Config file not found at {config_file_path}")
        sys.exit(1)

    try:
        train_stage2(config_path=config_file_path) 
    except NotImplementedError as e:
         print(f"Exiting: {e}") 
         sys.exit(0) 
    except Exception as e:
        print(f"Stage 2 training setup or execution failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)